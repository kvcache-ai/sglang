# kt-kernel SFT MoE 算子接口文档

## 1. 概述

kt-kernel 提供 SFT（Supervised Fine-Tuning）MoE 算子，支持带 LoRA 适配器的 MoE 专家计算。本文档描述其 Python 接口和使用方法。

## 2. 类层次结构

```
KTMoEWrapper (工厂类)
    │
    ├── mode="inference"
    │       │
    │       └── BaseMoEWrapper
    │               │
    │               ├── AMXMoEWrapper
    │               ├── NativeMoEWrapper
    │               ├── LlamafileMoEWrapper
    │               └── GeneralMoEWrapper
    │
    └── mode="sft"
            │
            └── BaseSFTMoEWrapper
                    │
                    └── AMXSFTMoEWrapper
```

## 3. KTMoEWrapper 工厂类

### 3.1 位置

`/home/lpl/ktransformers-llama/kt-kernel/python/experts.py`

### 3.2 构造参数

```python
KTMoEWrapper(
    # 基础参数
    layer_idx: int,                    # 层索引
    num_experts: int,                  # 专家总数
    num_experts_per_tok: int,          # 每 token 选择的专家数 (top-k)
    hidden_size: int,                  # 隐藏层维度
    moe_intermediate_size: int,        # MoE 中间层维度
    num_gpu_experts: int,              # GPU 上的专家数（SFT 通常为 0）
    cpuinfer_threads: int,             # CPU 推理线程数
    threadpool_count: int,             # NUMA 子池数量
    weight_path: str,                  # 权重文件路径
    chunked_prefill_size: int,         # 分块 prefill 大小

    # 模式选择
    mode: str = "inference",           # "inference" 或 "sft"
    method: str = "AMXINT4",           # 量化方法

    # SFT 专用参数（mode="sft" 时使用）
    lora_rank: int = 16,               # LoRA 秩
    lora_alpha: float = 32.0,          # LoRA alpha
    max_cache_depth: int = 1,          # 最大缓存深度（推理用 1）

    # Inference 专用参数（mode="inference" 时使用）
    cpu_save: bool = False,
    max_deferred_experts_per_token: Optional[int] = None,

    # K-Group 量化参数（SFT K-Group 方法）
    group_size: int = 128,
    zero_point: bool = True,
)
```

### 3.3 支持的方法

#### Inference 方法

| 方法名 | 描述 |
|--------|------|
| AMXINT4 | AMX INT4 量化 |
| AMXINT8 | AMX INT8 量化 |
| RAWINT4 | 原生 INT4 |
| FP8 | FP8 量化 |
| LLAMAFILE | GGUF 格式 |
| MOE_INT4 | 通用 INT4 |
| MOE_INT8 | 通用 INT8 |

#### SFT 方法

| 方法名 | 描述 |
|--------|------|
| AMXBF16_SFT | BF16 全精度 |
| AMXINT8_SFT | INT8 量化 |
| AMXINT4_SFT | INT4 量化 |
| AMXINT4_1_SFT | INT4 变体 |
| AMXINT4_KGroup_SFT | K-Group INT4 |
| AMXINT4_1KGroup_SFT | K-Group INT4 变体 |

## 4. BaseSFTMoEWrapper 基类

### 4.1 位置

`/home/lpl/ktransformers-llama/kt-kernel/python/experts_sft.py`

### 4.2 核心方法

#### load_weights()

```python
def load_weights(self, physical_to_logical_map_cpu: torch.Tensor) -> None:
    """
    加载基础权重。

    Args:
        physical_to_logical_map_cpu: 物理到逻辑专家 ID 映射
    """
```

#### init_lora_weights()

```python
def init_lora_weights(
    self,
    gate_lora_a: torch.Tensor,  # [num_experts, lora_rank, hidden_size]
    gate_lora_b: torch.Tensor,  # [num_experts, intermediate_size, lora_rank]
    up_lora_a: torch.Tensor,    # [num_experts, lora_rank, hidden_size]
    up_lora_b: torch.Tensor,    # [num_experts, intermediate_size, lora_rank]
    down_lora_a: torch.Tensor,  # [num_experts, lora_rank, intermediate_size]
    down_lora_b: torch.Tensor,  # [num_experts, hidden_size, lora_rank]
) -> None:
    """
    初始化 LoRA 权重。

    LoRA 输出公式:
        lora_output = (input @ A.T @ B.T) * (lora_alpha / lora_rank)
        output = base_output + lora_output

    注意: 所有张量必须是 contiguous 的，用于 C++ 直接访问。
    """
```

#### forward_sft()

```python
def forward_sft(
    self,
    hidden_states: torch.Tensor,  # [qlen, hidden_size]
    expert_ids: torch.Tensor,     # [qlen, num_experts_per_tok]
    weights: torch.Tensor,        # [qlen, num_experts_per_tok]
    save_for_backward: bool = True,
) -> torch.Tensor:
    """
    SFT 前向传播。

    Args:
        hidden_states: 输入隐藏状态
        expert_ids: 每个 token 选择的专家 ID
        weights: 每个专家的路由权重
        save_for_backward: 是否保存激活用于反向传播

    Returns:
        输出隐藏状态 [qlen, hidden_size]

    注意:
        - 推理场景设置 save_for_backward=False 以节省内存
        - 与 inference 模式的 forward() 不同，这是同步执行
    """
```

#### backward() (训练用)

```python
def backward(
    self,
    grad_output: torch.Tensor,  # [qlen, hidden_size]
) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
    """
    反向传播计算梯度。

    Args:
        grad_output: 上游梯度

    Returns:
        grad_input: 输入梯度 [qlen, hidden_size]
        grad_loras: LoRA 梯度字典:
            - grad_gate_lora_a: [num_experts, lora_rank, hidden_size]
            - grad_gate_lora_b: [num_experts, intermediate_size, lora_rank]
            - ... (共 6 个)
    """
```

#### update_lora_weights()

```python
def update_lora_weights(self) -> None:
    """
    将更新后的 LoRA 权重同步到 C++ 后端。

    典型使用场景（训练）:
        output = wrapper.forward_sft(input, expert_ids, weights)
        grad_input, grad_loras = wrapper.backward(grad_output)
        optimizer.step()  # 更新 LoRA 权重
        wrapper.update_lora_weights()  # 同步到 C++
    """
```

### 4.3 禁用的方法

SFT 模式禁用了以下 inference 方法:

```python
def forward(self, *args, **kwargs):
    raise RuntimeError("forward() is not available in SFT mode. Use forward_sft() instead.")

def submit_forward(self, *args, **kwargs):
    raise RuntimeError("submit_forward() is not available in SFT mode.")

def sync_forward(self, *args, **kwargs):
    raise RuntimeError("sync_forward() is not available in SFT mode.")
```

## 5. AMXSFTMoEWrapper 实现

### 5.1 位置

`/home/lpl/ktransformers-llama/kt-kernel/python/utils/amx_sft.py`

### 5.2 额外方法

#### load_weights_from_tensors()

```python
def load_weights_from_tensors(
    self,
    gate_proj: torch.Tensor,   # [num_experts, intermediate_size, hidden_size]
    up_proj: torch.Tensor,     # [num_experts, intermediate_size, hidden_size]
    down_proj: torch.Tensor,   # [num_experts, hidden_size, intermediate_size]
    physical_to_logical_map_cpu: torch.Tensor,
) -> None:
    """
    从 BF16/FP16 张量加载权重（支持在线量化）。
    """
```

## 6. KExpertsSFTBuffer 缓存管理

### 6.1 位置

`/home/lpl/ktransformers-llama/kt-kernel/python/experts_sft.py`

### 6.2 Buffer 内容

| 类型 | Buffer 名 | Shape |
|------|-----------|-------|
| 前向 | input_cpu | [qlen, hidden_size] |
| 前向 | expert_ids_cpu | [qlen, num_experts_per_tok] |
| 前向 | weights_cpu | [qlen, num_experts_per_tok] |
| 前向 | output_cpu | [qlen, hidden_size] |
| 反向 | grad_output_cpu | [qlen, hidden_size] |
| 反向 | grad_input_cpu | [qlen, hidden_size] |
| LoRA 梯度 | grad_gate_lora_a | [num_experts, lora_rank, hidden_size] |
| LoRA 梯度 | grad_gate_lora_b | [num_experts, intermediate_size, lora_rank] |
| ... | ... | ... |

### 6.3 缓存管理

```python
# 获取或创建 buffer（自动缓存）
buffer = KExpertsSFTBuffer.get_buffer(
    qlen=qlen,
    hidden_size=hidden_size,
    moe_intermediate_size=intermediate_size,
    num_experts=num_experts,
    num_experts_per_tok=num_experts_per_tok,
    lora_rank=lora_rank,
    dtype=torch.bfloat16,
)

# 清除缓存
KExpertsSFTBuffer.clear_cache()
```

## 7. 使用示例

### 7.1 推理场景

```python
from kt_kernel import KTMoEWrapper

# 1. 创建 SFT Wrapper
wrapper = KTMoEWrapper(
    layer_idx=0,
    num_experts=64,
    num_experts_per_tok=8,
    hidden_size=7168,
    moe_intermediate_size=2048,
    num_gpu_experts=0,
    cpuinfer_threads=60,
    threadpool_count=4,
    weight_path="/path/to/weights",
    chunked_prefill_size=25600,
    mode="sft",
    method="AMXBF16_SFT",
    lora_rank=16,
    lora_alpha=32.0,
    max_cache_depth=1,
)

# 2. 加载基础权重
wrapper.load_weights(physical_to_logical_map)

# 3. 初始化 LoRA 权重
wrapper.init_lora_weights(
    gate_lora_a, gate_lora_b,
    up_lora_a, up_lora_b,
    down_lora_a, down_lora_b,
)

# 4. 推理
output = wrapper.forward_sft(
    hidden_states,
    expert_ids,
    weights,
    save_for_backward=False,  # 推理不需要梯度
)
```

### 7.2 训练场景（参考）

```python
# 1-3 同上

# 4. 前向传播（保存激活）
output = wrapper.forward_sft(
    hidden_states, expert_ids, weights,
    save_for_backward=True,
)

# 5. 反向传播
grad_input, grad_loras = wrapper.backward(grad_output)

# 6. 更新权重
optimizer.step()
wrapper.update_lora_weights()
```

## 8. LoRA 权重 Shape 规范

| 投影 | LoRA 类型 | Shape | 说明 |
|------|-----------|-------|------|
| gate | A | [E, R, H] | E=专家数, R=秩, H=隐藏维度 |
| gate | B | [E, I, R] | I=中间维度 |
| up | A | [E, R, H] | |
| up | B | [E, I, R] | |
| down | A | [E, R, I] | 注意: A 的输入是中间维度 |
| down | B | [E, H, R] | 注意: B 的输出是隐藏维度 |

以 DeepSeek-V2 为例 (H=7168, I=2048, E=64, R=16):

| 权重名 | Shape |
|--------|-------|
| gate_lora_a | [64, 16, 7168] |
| gate_lora_b | [64, 2048, 16] |
| up_lora_a | [64, 16, 7168] |
| up_lora_b | [64, 2048, 16] |
| down_lora_a | [64, 16, 2048] |
| down_lora_b | [64, 7168, 16] |

## 9. C++ 接口参考

### 9.1 MOESFTConfig

```cpp
// /home/lpl/ktransformers-llama/kt-kernel/operators/common.hpp
struct MOESFTConfig : public GeneralMOEConfig {
    int lora_rank = 16;
    float lora_alpha = 32.0f;
    float lora_scaling() const { return lora_alpha / lora_rank; }

    // LoRA 权重指针（零拷贝指向 Python tensor）
    void* gate_lora_a = nullptr;
    void* gate_lora_b = nullptr;
    void* up_lora_a = nullptr;
    void* up_lora_b = nullptr;
    void* down_lora_a = nullptr;
    void* down_lora_b = nullptr;

    int max_cache_depth = 1;
};
```

## 10. SFT 权重加载接口

### 10.1 BF16SafeTensorLoader 类

**文件**: `kt-kernel/python/utils/loader.py`

**继承**: `SafeTensorLoader`

#### 构造函数

```python
def __init__(self, file_path: str):
    """
    初始化 BF16 权重加载器。

    Args:
        file_path: HuggingFace 模型目录路径或 safetensors 文件路径
    """
```

#### load_experts()

```python
def load_experts(self, base_key: str, device: str = "cpu") -> dict:
    """
    加载指定层的 BF16 专家权重。

    Args:
        base_key: 层的基础 key（如 "model.layers.0"）
        device: 加载到的设备（默认 "cpu"）

    Returns:
        dict: {
            "gate": List[torch.Tensor],  # 每个专家的 gate 权重
            "up": List[torch.Tensor],    # 每个专家的 up 权重
            "down": List[torch.Tensor],  # 每个专家的 down 权重
            "gate_scale": None,          # BF16 无需 scale
            "up_scale": None,
            "down_scale": None,
        }
    """
```

### 10.2 AMXSFTMoEWrapper 扩展接口

#### load_weights()（更新）

```python
def load_weights(self, physical_to_logical_map_cpu: torch.Tensor) -> None:
    """
    加载基础权重。

    支持两种模式：
    1. 张量模式：先调用 load_weights_from_tensors()，再调用本方法
    2. 文件模式：自动从 weight_path 加载（需确保 weight_path 已设置）

    文件模式根据 method 选择加载器：
    - AMXBF16_SFT: BF16SafeTensorLoader，base_key="model.layers.{L}"
    - AMXINT8_SFT/AMXINT4_SFT: SafeTensorLoader，base_key="blk.{L}"

    Args:
        physical_to_logical_map_cpu: 物理到逻辑专家 ID 映射
    """
```

#### _load_base_weights_from_file()（新增）

```python
def _load_base_weights_from_file(self) -> None:
    """
    从文件加载基础 MoE 权重。

    加载策略：
    - AMXBF16_SFT: 使用 BF16SafeTensorLoader，从 HuggingFace 格式加载
    - AMXINT8_SFT/AMXINT4_SFT: 使用 SafeTensorLoader，从预量化格式加载

    加载后设置：
    - self.gate_proj: [num_experts, intermediate_size, hidden_size]
    - self.up_proj: [num_experts, intermediate_size, hidden_size]
    - self.down_proj: [num_experts, hidden_size, intermediate_size]
    - self.gate_scale, self.up_scale, self.down_scale（仅 INT 方法）

    Raises:
        RuntimeError: 如果 weight_path 未设置
    """
```

### 10.3 KTConfig 新增字段

| 字段 | 类型 | 默认值 | 说明 |
|------|------|--------|------|
| `model_path` | `Optional[str]` | `None` | HuggingFace 模型路径，用于 AMXBF16_SFT |

### 10.4 权重路径选择接口

**位置**: `KTEPWrapperMethod.create_weights()`

```python
# 接口契约：根据 sft_method 选择权重路径
def get_sft_weight_path(kt_config: KTConfig) -> str:
    """
    获取 SFT 模式的权重路径。

    Args:
        kt_config: KT 配置对象

    Returns:
        str: 权重路径
            - AMXBF16_SFT: kt_config.model_path
            - 其他: kt_config.weight_path
    """
    if kt_config.sft_method == "AMXBF16_SFT":
        return kt_config.model_path
    else:
        return kt_config.weight_path
```

### 10.5 支持的 MoE 格式

| 格式名 | 模型 | Expert Key 格式 |
|--------|------|-----------------|
| `deepseek` | DeepSeek-V2/V3 | `{base}.mlp.experts.{E}.{gate,up,down}_proj.weight` |
| `mixtral` | Mixtral/MiniMax | `{base}.block_sparse_moe.experts.{E}.{w1,w3,w2}.weight` |

### 10.6 错误处理

| 错误类型 | 触发条件 | 处理方式 |
|----------|----------|----------|
| `RuntimeError` | `weight_path` 未设置 | 提示调用 `load_weights_from_tensors()` |
| `ValueError` | 未找到专家权重 | 抛出详细错误信息 |
| `KeyError` | 找不到指定 tensor | 抛出 key 名称 |
