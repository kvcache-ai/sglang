# MoE Expert LoRA 功能架构设计

## 1. 整体架构

### 1.1 数据流图

```
┌─────────────────────────────────────────────────────────────────┐
│                    LoRA Adapter (PEFT 格式)                     │
│  Kllama2: 含 MoE experts LoRA  |  Kllama: 仅 shared/attention   │
└─────────────────────────────────────────────────────────────────┘
                              │
                    ┌─────────┴─────────┐
                    │                   │
            ┌───────▼───────┐   ┌───────▼───────┐
            │ convert_moe_  │   │  LoRAAdapter  │
            │ lora.py       │   │  (现有路径)    │
            │ - 预转换工具   │   └───────┬───────┘
            └───────┬───────┘           │
                    │           ┌───────┴───────┐
    ┌───────────────▼───────┐   │               │
    │  kt-kernel 格式文件    │   │               │
    │  layer_{L}:           │   │               │
    │   - gate_lora_a/b     │   │               │
    │   - up_lora_a/b       │   │               │
    │   - down_lora_a/b     │   │               │
    └───────────┬───────────┘   │               │
                │               │               │
    ┌───────────▼───────────────▼───┐   ┌───────▼───────┐
    │     MoELoRAManager            │   │ Attention/    │
    │     (新增)                    │   │ Shared LoRA   │
    │     - 存储堆叠后的权重         │   │ (GPU 现有路径)│
    │     - 提供 per-layer 获取接口  │   └───────────────┘
    └───────────┬───────────────────┘
                │
    ┌───────────▼───────────────────┐
    │     KTEPWrapperMethod         │
    │     (修改 kt_ep_wrapper.py)   │
    │     - 检测是否有 MoE LoRA      │
    │     - 选择 mode="sft"         │
    └───────────┬───────────────────┘
                │
    ┌───────────▼───────────────────┐
    │     KTMoEWrapper              │
    │     mode="sft"                │
    │     method="AMXBF16_SFT"      │
    │     - init_lora_weights()     │
    │     - forward_sft()           │
    └───────────┬───────────────────┘
                │
    ┌───────────▼───────────────────┐
    │     AMXSFTMoEWrapper          │
    │     (kt-kernel C++ 后端)      │
    └───────────────────────────────┘
```

### 1.2 模块职责

| 模块 | 职责 |
|------|------|
| convert_moe_lora.py | 预处理：PEFT → kt-kernel 格式转换 |
| KTConfig | 配置：存储 MoE LoRA 相关配置 |
| KTEPWrapperMethod | 集成：创建 SFT wrapper 并管理生命周期 |
| KTMoEWrapper | 工厂：根据 mode 创建合适的 wrapper |
| AMXSFTMoEWrapper | 实现：执行带 LoRA 的 MoE 计算 |

## 2. 组件设计

### 2.1 转换工具 (convert_moe_lora.py)

**位置**: `sglang/scripts/convert_moe_lora.py`

**接口**:
```python
def convert_peft_to_kt_format(
    input_path: str,          # PEFT adapter_model.safetensors
    output_path: str,         # 输出 .pt 文件
    lora_alpha: float = 32.0, # LoRA alpha（从 adapter_config.json 读取）
) -> dict:
    """
    转换 PEFT 格式的 MoE LoRA adapter 为 kt-kernel 格式。

    Returns:
        转换统计信息
    """
```

**处理流程**:
```
1. 读取 safetensors 文件
2. 扫描所有 keys，匹配 MoE expert LoRA 模式
3. 按 (layer_idx, proj_type, lora_type) 分组
4. 对每组，按 expert_id 排序并堆叠
5. 验证 shape 一致性
6. 保存为 .pt 文件
```

### 2.2 KTConfig 扩展

**位置**: `sglang/srt/layers/moe/kt_ep_wrapper.py`

**新增字段**:
```python
@dataclass
class KTConfig:
    # ... 现有字段 ...

    # MoE LoRA 配置
    moe_lora_enabled: bool = False
    moe_lora_path: Optional[str] = None
    lora_rank: int = 16
    lora_alpha: float = 32.0
    sft_method: str = "AMXBF16_SFT"
```

### 2.3 KTEPWrapperMethod 修改

**位置**: `sglang/srt/layers/moe/kt_ep_wrapper.py`

**关键变更**:

1. **create_weights()**: 根据配置选择 inference 或 sft 模式
2. **process_weights_after_loading()**: 加载并初始化 LoRA 权重
3. **submit()/sync()**: SFT 模式使用同步执行

**模式选择逻辑**:
```python
if self.kt_config.moe_lora_enabled:
    self.wrapper = KTMoEWrapper(
        ...,
        mode="sft",
        method=self.kt_config.sft_method,
        lora_rank=self.kt_config.lora_rank,
        lora_alpha=self.kt_config.lora_alpha,
    )
else:
    self.wrapper = KTMoEWrapper(
        ...,
        mode="inference",
        method=self.kt_config.method,
    )
```

### 2.4 执行模式对比

| 方面 | Inference 模式 | SFT 模式 |
|------|----------------|----------|
| 前向接口 | submit_forward() + sync_forward() | forward_sft() |
| 执行方式 | 异步（CPU-GPU 并行） | 同步 |
| LoRA 支持 | 无 | 有 |
| Wrapper 类 | AMXMoEWrapper | AMXSFTMoEWrapper |

## 3. 接口设计

### 3.1 转换工具 CLI

```bash
python scripts/convert_moe_lora.py \
    --input /path/to/adapter_model.safetensors \
    --config /path/to/adapter_config.json \
    --output /path/to/moe_lora.pt \
    [--verbose]
```

### 3.2 ServerArgs 新增参数

```python
# server_args.py
class ServerArgs:
    # ... 现有参数 ...

    # MoE LoRA 参数
    kt_moe_lora_path: Optional[str] = None
    kt_moe_lora_rank: int = 16
    kt_moe_lora_alpha: float = 32.0
    kt_moe_sft_method: str = "AMXBF16_SFT"
```

### 3.3 KTConfig 创建函数更新

```python
def create_kt_config_from_server_args(
    server_args: "ServerArgs", layer_idx: int
) -> Optional[KTConfig]:
    if server_args.kt_weight_path is None:
        return None

    return KTConfig(
        # ... 现有字段 ...
        moe_lora_enabled=server_args.kt_moe_lora_path is not None,
        moe_lora_path=server_args.kt_moe_lora_path,
        lora_rank=server_args.kt_moe_lora_rank,
        lora_alpha=server_args.kt_moe_lora_alpha,
        sft_method=server_args.kt_moe_sft_method,
    )
```

## 4. 状态管理

### 4.1 生命周期

```
1. 启动时
   ServerArgs 解析
       ↓
   KTConfig 创建（每层）
       ↓
   KTEPWrapperMethod.create_weights()
       ↓
   KTMoEWrapper 创建（inference 或 sft 模式）

2. 权重加载后
   KTEPWrapperMethod.process_weights_after_loading()
       ↓
   加载 moe_lora.pt
       ↓
   wrapper.init_lora_weights()

3. 推理时
   apply() → submit() → gpu_compute() → sync()
   (SFT 模式：submit 跳过，sync 中执行 forward_sft)
```

### 4.2 权重存储

| 权重类型 | 存储位置 | 生命周期 |
|----------|----------|----------|
| Base MoE weights | KTMoEWrapper 内部 | 整个推理过程 |
| LoRA weights | AMXSFTMoEWrapper 属性 | 整个推理过程 |
| 计算 buffer | KExpertsSFTBuffer | 按需分配，缓存复用 |

## 5. 错误处理设计

### 5.1 转换阶段

```python
# convert_moe_lora.py
class ConversionError(Exception):
    pass

def convert_peft_to_kt_format(...):
    # 1. 检查输入文件
    if not os.path.exists(input_path):
        raise FileNotFoundError(f"Input file not found: {input_path}")

    # 2. 检查 MoE keys 存在
    moe_keys = find_moe_keys(all_keys)
    if not moe_keys:
        logger.warning("No MoE expert LoRA keys found")
        return {"status": "skipped", "reason": "no_moe_keys"}

    # 3. 验证 shape 一致性
    for layer_idx in range(num_layers):
        validate_layer_shapes(weights[layer_idx], expected_shapes)
```

### 5.2 运行时

```python
# kt_ep_wrapper.py
def process_weights_after_loading(self, layer):
    if self.kt_config.moe_lora_enabled:
        if not os.path.exists(self.kt_config.moe_lora_path):
            raise RuntimeError(
                f"MoE LoRA file not found: {self.kt_config.moe_lora_path}"
            )

        lora_weights = torch.load(self.kt_config.moe_lora_path)
        layer_key = f"layer_{self.kt_config.layer_idx}"

        if layer_key not in lora_weights:
            raise RuntimeError(
                f"Layer {self.kt_config.layer_idx} not found in MoE LoRA file"
            )
```

## 6. 扩展性考虑

### 6.1 未来扩展点

1. **动态 LoRA 切换**: 当前静态加载，未来可支持运行时切换
2. **多 adapter 支持**: 当前单 adapter，未来可支持多 adapter 合并
3. **异步 SFT**: kt-kernel 未来可能提供异步 forward_sft

### 6.2 配置灵活性

```python
# 未来可扩展的配置项
@dataclass
class KTConfig:
    # ... 现有字段 ...

    # 预留扩展
    moe_lora_dtype: str = "bfloat16"  # 权重精度
    moe_lora_layer_filter: Optional[List[int]] = None  # 指定层
```

## 7. SFT 权重加载架构设计

### 7.1 整体架构

```
┌───────────────────────────────────────────────────────────────────────┐
│                         KTEPWrapperMethod                             │
│                         create_weights()                              │
└───────────────────────────────┬───────────────────────────────────────┘
                                │
                     ┌──────────┴──────────┐
                     │  moe_lora_enabled?  │
                     └──────────┬──────────┘
                                │
              ┌─────────────────┼─────────────────┐
              │ Yes             │                 │ No
              ▼                 │                 ▼
    ┌─────────────────┐         │       ┌─────────────────┐
    │ SFT Mode        │         │       │ Inference Mode  │
    │ sft_method?     │         │       │ AMXMoEWrapper   │
    └────────┬────────┘         │       └─────────────────┘
             │                  │
    ┌────────┴────────┐         │
    │                 │         │
AMXBF16_SFT    AMXINT8/INT4_SFT │
    │                 │         │
    ▼                 ▼         │
┌─────────┐   ┌─────────┐       │
│model_   │   │weight_  │       │
│path     │   │path     │       │
└────┬────┘   └────┬────┘       │
     │             │            │
     └──────┬──────┘            │
            │                   │
            ▼                   │
┌───────────────────────┐       │
│ AMXSFTMoEWrapper      │       │
│ weight_path set       │       │
└───────────┬───────────┘       │
            │                   │
            ▼                   │
┌───────────────────────┐       │
│ load_weights()        │       │
│ _load_base_weights    │       │
│ _from_file()          │       │
└───────────────────────┘
```

### 7.2 组件设计

#### 7.2.1 KTConfig 扩展

**文件**: `sglang/srt/layers/moe/kt_ep_wrapper.py`

```python
@dataclass
class KTConfig:
    # ... 现有字段 ...

    # MoE LoRA 配置
    moe_lora_enabled: bool = False
    moe_lora_path: Optional[str] = None
    lora_rank: int = 16
    lora_alpha: float = 32.0
    sft_method: str = "AMXBF16_SFT"

    # 新增：HuggingFace 模型路径（用于 AMXBF16_SFT）
    model_path: Optional[str] = None
```

#### 7.2.2 权重路径选择模块

**位置**: `KTEPWrapperMethod.create_weights()`

```python
# SFT 模式权重路径选择逻辑
if self.kt_config.moe_lora_enabled:
    if self.kt_config.sft_method == "AMXBF16_SFT":
        # BF16 模式：使用 HuggingFace 模型路径
        sft_weight_path = self.kt_config.model_path
    else:
        # INT8/INT4 模式：使用 KT 预量化权重路径
        sft_weight_path = self.kt_config.weight_path

    self.wrapper = KTMoEWrapper(
        ...,
        weight_path=sft_weight_path,
        mode="sft",
        method=self.kt_config.sft_method,
    )
```

#### 7.2.3 BF16SafeTensorLoader

**文件**: `kt-kernel/python/utils/loader.py`

```python
class BF16SafeTensorLoader(SafeTensorLoader):
    """Loader for native BF16 expert weights."""

    MOE_FORMATS = {
        "deepseek": ("{base}.mlp.experts", "gate_proj", "up_proj", "down_proj"),
        "mixtral": ("{base}.block_sparse_moe.experts", "w1", "w3", "w2"),
    }

    def __init__(self, file_path: str):
        super().__init__(file_path)
        self._detected_format = None
        self._detect_format()

    def _detect_format(self):
        """Auto-detect MoE naming format."""
        # 检测 deepseek 或 mixtral 格式
        ...

    def load_experts(self, base_key: str, device: str = "cpu"):
        """Load BF16 expert weights."""
        # 返回 {gate, up, down, gate_scale=None, ...}
        ...
```

#### 7.2.4 AMXSFTMoEWrapper 权重加载

**文件**: `kt-kernel/python/utils/amx_sft.py`

```python
class AMXSFTMoEWrapper(BaseSFTMoEWrapper):

    def load_weights(self, physical_to_logical_map_cpu: torch.Tensor) -> None:
        """Load base weights for this layer."""
        if self._weights_loaded:
            return

        # 如果基础权重未设置，从文件自动加载
        if self.gate_proj is None or self.up_proj is None or self.down_proj is None:
            self._load_base_weights_from_file()

        # 继续原有逻辑：创建 MOESFTConfig，初始化 C++ 后端
        ...

    def _load_base_weights_from_file(self) -> None:
        """Load base MoE weights from file."""
        if self.method == "AMXBF16_SFT":
            loader = BF16SafeTensorLoader(self.weight_path)
            base_key = f"model.layers.{self.layer_idx}"
        else:
            loader = SafeTensorLoader(self.weight_path)
            base_key = f"blk.{self.layer_idx}"

        experts_data = loader.load_experts(base_key, device="cpu")

        # 堆叠权重
        if self.method == "AMXBF16_SFT":
            self.gate_proj = torch.stack(experts_data["gate"], dim=0)
            self.up_proj = torch.stack(experts_data["up"], dim=0)
            self.down_proj = torch.stack(experts_data["down"], dim=0)
        else:
            # INT8/INT4: 处理 NUMA 分片格式
            ...
```

### 7.3 数据流设计

#### 7.3.1 权重加载数据流

```
启动时:
┌──────────────────┐
│ ServerArgs       │
│ - model_path     │
│ - kt_weight_path │
│ - kt_moe_lora_*  │
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ create_kt_config │
│ _from_server_args│
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ KTConfig         │
│ - model_path ────┼──────────────────┐
│ - weight_path    │                  │
│ - sft_method     │                  │
└────────┬─────────┘                  │
         │                            │
         ▼                            │
┌──────────────────┐                  │
│ KTEPWrapperMethod│                  │
│ create_weights() │                  │
└────────┬─────────┘                  │
         │                            │
         ▼                            │
┌──────────────────┐                  │
│ 选择 weight_path │◄─────────────────┘
│ AMXBF16_SFT:     │
│   model_path     │
│ AMXINT8/INT4:    │
│   weight_path    │
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ KTMoEWrapper     │
│ (SFT mode)       │
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ AMXSFTMoEWrapper │
│ load_weights()   │
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ _load_base_      │
│ weights_from_file│
└────────┬─────────┘
         │
    ┌────┴────┐
    │         │
    ▼         ▼
BF16Loader  SafeTensorLoader
    │         │
    └────┬────┘
         │
         ▼
┌──────────────────┐
│ gate_proj        │
│ up_proj          │
│ down_proj        │
│ (+ scales if INT)│
└──────────────────┘
```

### 7.4 接口契约

#### 7.4.1 Loader 返回格式

所有 Loader 的 `load_experts()` 方法必须返回统一格式：

```python
{
    "gate": List[Tensor] | List[List[ndarray]],  # BF16: List[Tensor], INT: nested list
    "up": List[Tensor] | List[List[ndarray]],
    "down": List[Tensor] | List[List[ndarray]],
    "gate_scale": None | List[List[ndarray]],    # BF16: None, INT: nested list
    "up_scale": None | List[List[ndarray]],
    "down_scale": None | List[List[ndarray]],
}
```

#### 7.4.2 权重 Shape 规范

| 权重 | Shape | 说明 |
|------|-------|------|
| gate_proj | [E, I, H] | E=专家数, I=中间维度, H=隐藏维度 |
| up_proj | [E, I, H] | |
| down_proj | [E, H, I] | 注意维度顺序 |
| *_scale | [E, ...] | INT 量化专用 |
