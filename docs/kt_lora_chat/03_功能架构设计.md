# MoE Expert LoRA 功能架构设计

## 1. 整体架构

### 1.1 数据流图

```
┌─────────────────────────────────────────────────────────────────┐
│                    LoRA Adapter (PEFT 格式)                     │
│  Kllama2: 含 MoE experts LoRA  |  Kllama: 仅 shared/attention   │
└─────────────────────────────────────────────────────────────────┘
                              │
                    ┌─────────┴─────────┐
                    │                   │
            ┌───────▼───────┐   ┌───────▼───────┐
            │ convert_moe_  │   │  LoRAAdapter  │
            │ lora.py       │   │  (现有路径)    │
            │ - 预转换工具   │   └───────┬───────┘
            └───────┬───────┘           │
                    │           ┌───────┴───────┐
    ┌───────────────▼───────┐   │               │
    │  kt-kernel 格式文件    │   │               │
    │  layer_{L}:           │   │               │
    │   - gate_lora_a/b     │   │               │
    │   - up_lora_a/b       │   │               │
    │   - down_lora_a/b     │   │               │
    └───────────┬───────────┘   │               │
                │               │               │
    ┌───────────▼───────────────▼───┐   ┌───────▼───────┐
    │     MoELoRAManager            │   │ Attention/    │
    │     (新增)                    │   │ Shared LoRA   │
    │     - 存储堆叠后的权重         │   │ (GPU 现有路径)│
    │     - 提供 per-layer 获取接口  │   └───────────────┘
    └───────────┬───────────────────┘
                │
    ┌───────────▼───────────────────┐
    │     KTEPWrapperMethod         │
    │     (修改 kt_ep_wrapper.py)   │
    │     - 检测是否有 MoE LoRA      │
    │     - 选择 mode="sft"         │
    └───────────┬───────────────────┘
                │
    ┌───────────▼───────────────────┐
    │     KTMoEWrapper              │
    │     mode="sft"                │
    │     method="AMXBF16_SFT"      │
    │     - init_lora_weights()     │
    │     - forward_sft()           │
    └───────────┬───────────────────┘
                │
    ┌───────────▼───────────────────┐
    │     AMXSFTMoEWrapper          │
    │     (kt-kernel C++ 后端)      │
    └───────────────────────────────┘
```

### 1.2 模块职责

| 模块 | 职责 |
|------|------|
| convert_moe_lora.py | 预处理：PEFT → kt-kernel 格式转换 |
| KTConfig | 配置：存储 MoE LoRA 相关配置 |
| KTEPWrapperMethod | 集成：创建 SFT wrapper 并管理生命周期 |
| KTMoEWrapper | 工厂：根据 mode 创建合适的 wrapper |
| AMXSFTMoEWrapper | 实现：执行带 LoRA 的 MoE 计算 |

## 2. 组件设计

### 2.1 转换工具 (convert_moe_lora.py)

**位置**: `sglang/scripts/convert_moe_lora.py`

**接口**:
```python
def convert_peft_to_kt_format(
    input_path: str,          # PEFT adapter_model.safetensors
    output_path: str,         # 输出 .pt 文件
    lora_alpha: float = 32.0, # LoRA alpha（从 adapter_config.json 读取）
) -> dict:
    """
    转换 PEFT 格式的 MoE LoRA adapter 为 kt-kernel 格式。

    Returns:
        转换统计信息
    """
```

**处理流程**:
```
1. 读取 safetensors 文件
2. 扫描所有 keys，匹配 MoE expert LoRA 模式
3. 按 (layer_idx, proj_type, lora_type) 分组
4. 对每组，按 expert_id 排序并堆叠
5. 验证 shape 一致性
6. 保存为 .pt 文件
```

### 2.2 KTConfig 扩展

**位置**: `sglang/srt/layers/moe/kt_ep_wrapper.py`

**新增字段**:
```python
@dataclass
class KTConfig:
    # ... 现有字段 ...

    # MoE LoRA 配置
    moe_lora_enabled: bool = False
    moe_lora_path: Optional[str] = None
    lora_rank: int = 16
    lora_alpha: float = 32.0
    sft_method: str = "AMXBF16_SFT"
```

### 2.3 KTEPWrapperMethod 修改

**位置**: `sglang/srt/layers/moe/kt_ep_wrapper.py`

**关键变更**:

1. **create_weights()**: 根据配置选择 inference 或 sft 模式
2. **process_weights_after_loading()**: 加载并初始化 LoRA 权重
3. **submit()/sync()**: SFT 模式使用同步执行

**模式选择逻辑**:
```python
if self.kt_config.moe_lora_enabled:
    self.wrapper = KTMoEWrapper(
        ...,
        mode="sft",
        method=self.kt_config.sft_method,
        lora_rank=self.kt_config.lora_rank,
        lora_alpha=self.kt_config.lora_alpha,
    )
else:
    self.wrapper = KTMoEWrapper(
        ...,
        mode="inference",
        method=self.kt_config.method,
    )
```

### 2.4 执行模式对比

| 方面 | Inference 模式 | SFT 模式 |
|------|----------------|----------|
| 前向接口 | submit_forward() + sync_forward() | forward_sft() |
| 执行方式 | 异步（CPU-GPU 并行） | 同步 |
| LoRA 支持 | 无 | 有 |
| Wrapper 类 | AMXMoEWrapper | AMXSFTMoEWrapper |

## 3. 接口设计

### 3.1 转换工具 CLI

```bash
python scripts/convert_moe_lora.py \
    --input /path/to/adapter_model.safetensors \
    --config /path/to/adapter_config.json \
    --output /path/to/moe_lora.pt \
    [--verbose]
```

### 3.2 ServerArgs 新增参数

```python
# server_args.py
class ServerArgs:
    # ... 现有参数 ...

    # MoE LoRA 参数
    kt_moe_lora_path: Optional[str] = None
    kt_moe_lora_rank: int = 16
    kt_moe_lora_alpha: float = 32.0
    kt_moe_sft_method: str = "AMXBF16_SFT"
```

### 3.3 KTConfig 创建函数更新

```python
def create_kt_config_from_server_args(
    server_args: "ServerArgs", layer_idx: int
) -> Optional[KTConfig]:
    if server_args.kt_weight_path is None:
        return None

    return KTConfig(
        # ... 现有字段 ...
        moe_lora_enabled=server_args.kt_moe_lora_path is not None,
        moe_lora_path=server_args.kt_moe_lora_path,
        lora_rank=server_args.kt_moe_lora_rank,
        lora_alpha=server_args.kt_moe_lora_alpha,
        sft_method=server_args.kt_moe_sft_method,
    )
```

## 4. 状态管理

### 4.1 生命周期

```
1. 启动时
   ServerArgs 解析
       ↓
   KTConfig 创建（每层）
       ↓
   KTEPWrapperMethod.create_weights()
       ↓
   KTMoEWrapper 创建（inference 或 sft 模式）

2. 权重加载后
   KTEPWrapperMethod.process_weights_after_loading()
       ↓
   加载 moe_lora.pt
       ↓
   wrapper.init_lora_weights()

3. 推理时
   apply() → submit() → gpu_compute() → sync()
   (SFT 模式：submit 跳过，sync 中执行 forward_sft)
```

### 4.2 权重存储

| 权重类型 | 存储位置 | 生命周期 |
|----------|----------|----------|
| Base MoE weights | KTMoEWrapper 内部 | 整个推理过程 |
| LoRA weights | AMXSFTMoEWrapper 属性 | 整个推理过程 |
| 计算 buffer | KExpertsSFTBuffer | 按需分配，缓存复用 |

## 5. 错误处理设计

### 5.1 转换阶段

```python
# convert_moe_lora.py
class ConversionError(Exception):
    pass

def convert_peft_to_kt_format(...):
    # 1. 检查输入文件
    if not os.path.exists(input_path):
        raise FileNotFoundError(f"Input file not found: {input_path}")

    # 2. 检查 MoE keys 存在
    moe_keys = find_moe_keys(all_keys)
    if not moe_keys:
        logger.warning("No MoE expert LoRA keys found")
        return {"status": "skipped", "reason": "no_moe_keys"}

    # 3. 验证 shape 一致性
    for layer_idx in range(num_layers):
        validate_layer_shapes(weights[layer_idx], expected_shapes)
```

### 5.2 运行时

```python
# kt_ep_wrapper.py
def process_weights_after_loading(self, layer):
    if self.kt_config.moe_lora_enabled:
        if not os.path.exists(self.kt_config.moe_lora_path):
            raise RuntimeError(
                f"MoE LoRA file not found: {self.kt_config.moe_lora_path}"
            )

        lora_weights = torch.load(self.kt_config.moe_lora_path)
        layer_key = f"layer_{self.kt_config.layer_idx}"

        if layer_key not in lora_weights:
            raise RuntimeError(
                f"Layer {self.kt_config.layer_idx} not found in MoE LoRA file"
            )
```

## 6. 扩展性考虑

### 6.1 未来扩展点

1. **动态 LoRA 切换**: 当前静态加载，未来可支持运行时切换
2. **多 adapter 支持**: 当前单 adapter，未来可支持多 adapter 合并
3. **异步 SFT**: kt-kernel 未来可能提供异步 forward_sft

### 6.2 配置灵活性

```python
# 未来可扩展的配置项
@dataclass
class KTConfig:
    # ... 现有字段 ...

    # 预留扩展
    moe_lora_dtype: str = "bfloat16"  # 权重精度
    moe_lora_layer_filter: Optional[List[int]] = None  # 指定层
```
