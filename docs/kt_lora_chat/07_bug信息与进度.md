# MoE Expert LoRA Bug ä¿¡æ¯ä¸è¿›åº¦

## Bug åˆ—è¡¨

### Bug #1: `kt_num_gpu_experts` ä¸º None å¯¼è‡´å¯åŠ¨å¤±è´¥

**çŠ¶æ€**: æœªè§£å†³

**å‘ç°æ—¥æœŸ**: 2026-01-15

**é”™è¯¯ä¿¡æ¯**:
```
TypeError: empty() received an invalid combination of arguments - got (NoneType, int, int, dtype=torch.dtype), but expected one of:
 * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)
 * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)
```

**è§¦å‘å‘½ä»¤**:
```bash
python -m sglang.launch_server \
      --model-path /mnt/data3/models/DeepSeek-V2-Lite-Chat \
      --kt-weight-path /mnt/data3/models/DeepSeek-V2-Lite-Chat-CPU-weight-INT8 \
      --kt-moe-lora-path /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/adapter_model_converted.safetensors \
      --kt-moe-lora-rank 8 \
      --kt-moe-lora-alpha 16.0 \
      --kt-moe-sft-method AMXBF16_SFT \
      --lora-paths /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/adapter_model.safetensors
```

**é”™è¯¯è°ƒç”¨æ ˆ**:
```
File "kt_ep_wrapper.py", line 1154, in create_weights
    self.gpu_method.create_weights(
File "unquant.py", line 181, in create_weights
    torch.empty(num_experts, w13_weight_n, w13_weight_k, dtype=params_dtype),
TypeError: empty() received an invalid combination of arguments
```

**æ ¹æœ¬åŸå› åˆ†æ**:

1. **é—®é¢˜ä½ç½®**: `python/sglang/srt/layers/moe/kt_ep_wrapper.py:1154`

2. **é—®é¢˜æµç¨‹**:
   ```
   server_args.py:500
       kt_num_gpu_experts: Optional[int] = None  (é»˜è®¤å€¼ä¸º None)
           â†“
   ç”¨æˆ·æœªæŒ‡å®š --kt-num-gpu-experts å‚æ•°
           â†“
   create_kt_config_from_server_args()
       num_gpu_experts=server_args.kt_num_gpu_experts  (ä¼ å…¥ None)
           â†“
   KTEPWrapperMethod.__init__()
       self.num_gpu_experts = kt_config.num_gpu_experts  (ä¿å­˜ None)
           â†“
   KTEPWrapperMethod.create_weights()
       self.gpu_method.create_weights(num_experts=self.num_gpu_experts)  (ä¼ å…¥ None)
           â†“
   UnquantizedFusedMoEMethod.create_weights()
       torch.empty(num_experts, w13_weight_n, w13_weight_k, ...)  (num_experts=None å¯¼è‡´é”™è¯¯)
   ```

3. **æ ¸å¿ƒé—®é¢˜**: å½“å¯ç”¨ MoE LoRA (SFT æ¨¡å¼) æ—¶ï¼Œä»£ç æ²¡æœ‰å¤„ç† `kt_num_gpu_experts` ä¸º `None` çš„æƒ…å†µã€‚åœ¨ SFT æ¨¡å¼ä¸‹ï¼Œç”¨æˆ·æœŸæœ›æ‰€æœ‰ä¸“å®¶åœ¨ CPU ä¸Šè¿è¡Œï¼ˆå› ä¸º SFT æ¨¡å¼å¤„ç† LoRA è®¡ç®—ï¼‰ï¼Œä½†ä»£ç è¦æ±‚å¿…é¡»æ˜¾å¼æŒ‡å®š GPU ä¸“å®¶æ•°é‡ã€‚

**ä¸´æ—¶è§£å†³æ–¹æ¡ˆ**:

åœ¨å¯åŠ¨å‘½ä»¤ä¸­æ·»åŠ  `--kt-num-gpu-experts 0`:
```bash
python -m sglang.launch_server \
      --model-path /mnt/data3/models/DeepSeek-V2-Lite-Chat \
      --kt-weight-path /mnt/data3/models/DeepSeek-V2-Lite-Chat-CPU-weight-INT8 \
      --kt-moe-lora-path /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/adapter_model_converted.safetensors \
      --kt-moe-lora-rank 8 \
      --kt-moe-lora-alpha 16.0 \
      --kt-moe-sft-method AMXBF16_SFT \
      --kt-num-gpu-experts 0 \
      --lora-paths /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/adapter_model.safetensors
```

**å»ºè®®çš„ä»£ç ä¿®å¤æ–¹æ¡ˆ**:

åœ¨ `kt_ep_wrapper.py` çš„ `KTEPWrapperMethod.__init__()` ä¸­æ·»åŠ è‡ªåŠ¨é»˜è®¤å€¼é€»è¾‘ï¼š

```python
def __init__(
    self,
    gpu_method: FusedMoEMethodBase,
    kt_config: KTConfig,
):
    # ... existing code ...

    # å½“å¯ç”¨ MoE LoRA æ—¶ï¼Œå¦‚æœæœªæŒ‡å®š GPU ä¸“å®¶æ•°ï¼Œé»˜è®¤ä¸º 0ï¼ˆå…¨éƒ¨åœ¨ CPUï¼‰
    if kt_config.moe_lora_enabled and kt_config.num_gpu_experts is None:
        self.num_gpu_experts = 0
    else:
        self.num_gpu_experts = kt_config.num_gpu_experts
```

æˆ–è€…åœ¨ `create_kt_config_from_server_args()` ä¸­æ·»åŠ éªŒè¯ï¼š

```python
def create_kt_config_from_server_args(server_args, layer_idx):
    # ...
    moe_lora_enabled = server_args.kt_moe_lora_path is not None

    # è‡ªåŠ¨è®¾ç½® num_gpu_experts é»˜è®¤å€¼
    num_gpu_experts = server_args.kt_num_gpu_experts
    if num_gpu_experts is None:
        num_gpu_experts = 0  # é»˜è®¤å…¨éƒ¨åœ¨ CPU

    return KTConfig(
        # ...
        num_gpu_experts=num_gpu_experts,
        # ...
    )
```

**ç›¸å…³æ–‡ä»¶**:
- `python/sglang/srt/layers/moe/kt_ep_wrapper.py` (ç¬¬ 1100, 1154 è¡Œ)
- `python/sglang/srt/server_args.py` (ç¬¬ 500 è¡Œ)
- `python/sglang/srt/layers/quantization/unquant.py` (ç¬¬ 181 è¡Œ)

---

### Bug #2: `kt_cpuinfer` ä¸º None å¯¼è‡´ KTMoEWrapper åˆå§‹åŒ–å¤±è´¥

**çŠ¶æ€**: æœªè§£å†³

**å‘ç°æ—¥æœŸ**: 2026-01-15

**é”™è¯¯ä¿¡æ¯**:
```
TypeError: unsupported operand type(s) for //: 'NoneType' and 'int'
```

**è§¦å‘å‘½ä»¤**:
```bash
python -m sglang.launch_server \
      --model-path /mnt/data3/models/DeepSeek-V2-Lite-Chat \
      --kt-weight-path /mnt/data3/models/DeepSeek-V2-Lite-Chat-CPU-weight-INT8 \
      --kt-moe-lora-path /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/adapter_model_converted.safetensors \
      --kt-moe-lora-rank 8 \
      --kt-moe-lora-alpha 16.0 \
      --kt-moe-sft-method AMXBF16_SFT \
      --kt-num-gpu-experts 0 \
      --lora-paths /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/adapter_model.safetensors
```

**é”™è¯¯è°ƒç”¨æ ˆ**:
```
File "kt_ep_wrapper.py", line 1168, in create_weights
    self.wrapper = KTMoEWrapper(
File "kt_kernel/experts.py", line 200, in __new__
    return _create_sft_wrapper(
File "kt_kernel/experts.py", line 353, in _create_sft_wrapper
    return AMXSFTMoEWrapper(
File "kt_kernel/utils/amx_sft.py", line 117, in __init__
    super().__init__(
File "kt_kernel/experts_sft.py", line 229, in __init__
    self.cpu_infer = self._get_cpu_infer(cpuinfer_threads, threadpool_count)
File "kt_kernel/experts_base.py", line 122, in _get_cpu_infer
    subpool_thread_count = [
File "kt_kernel/experts_base.py", line 123, in <listcomp>
    cpuinfer_threads // threadpool_count + ...
TypeError: unsupported operand type(s) for //: 'NoneType' and 'int'
```

**æ ¹æœ¬åŸå› åˆ†æ**:

1. **é—®é¢˜ä½ç½®**: `kt_ep_wrapper.py:1168` â†’ `kt_kernel/experts_base.py:123`

2. **é—®é¢˜æµç¨‹**:
   ```
   server_args.py:498
       kt_cpuinfer: Optional[int] = None  (é»˜è®¤å€¼ä¸º None)
           â†“
   ç”¨æˆ·æœªæŒ‡å®š --kt-cpuinfer å‚æ•°
           â†“
   create_kt_config_from_server_args()
       cpuinfer_threads=server_args.kt_cpuinfer  (ä¼ å…¥ None)
           â†“
   KTMoEWrapper() åˆ›å»ºæ—¶
       cpuinfer_threads=self.kt_config.cpuinfer_threads  (ä¼ å…¥ None)
           â†“
   kt_kernel/experts_base.py:123
       cpuinfer_threads // threadpool_count  (None // 2 å¯¼è‡´ TypeError)
   ```

3. **æ ¸å¿ƒé—®é¢˜**: `kt_cpuinfer` å‚æ•°æ²¡æœ‰é»˜è®¤å€¼ï¼Œå¿…é¡»ç”±ç”¨æˆ·æ˜¾å¼æŒ‡å®šã€‚

**ä¸´æ—¶è§£å†³æ–¹æ¡ˆ**:

åœ¨å¯åŠ¨å‘½ä»¤ä¸­æ·»åŠ  `--kt-cpuinfer` å‚æ•°ï¼ˆå»ºè®®è®¾ç½®ä¸º CPU æ ¸å¿ƒæ•°ï¼Œå¦‚ 60ï¼‰:
```bash
python -m sglang.launch_server \
      --model-path /mnt/data3/models/DeepSeek-V2-Lite-Chat \
      --kt-weight-path /mnt/data3/models/DeepSeek-V2-Lite-Chat-CPU-weight-INT8 \
      --kt-moe-lora-path /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/adapter_model_converted.safetensors \
      --kt-moe-lora-rank 8 \
      --kt-moe-lora-alpha 16.0 \
      --kt-moe-sft-method AMXBF16_SFT \
      --kt-num-gpu-experts 0 \
      --kt-cpuinfer 60 \
      --lora-paths /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/adapter_model.safetensors
```

**å»ºè®®çš„ä»£ç ä¿®å¤æ–¹æ¡ˆ**:

åœ¨ `create_kt_config_from_server_args()` ä¸­ä¸º `kt_cpuinfer` è®¾ç½®åˆç†çš„é»˜è®¤å€¼ï¼š

```python
import os

def create_kt_config_from_server_args(server_args, layer_idx):
    # ...

    # è‡ªåŠ¨è®¾ç½® cpuinfer_threads é»˜è®¤å€¼
    cpuinfer_threads = server_args.kt_cpuinfer
    if cpuinfer_threads is None:
        # é»˜è®¤ä½¿ç”¨ CPU æ ¸å¿ƒæ•°çš„ä¸€åŠæˆ– 60ï¼Œå–è¾ƒå°å€¼
        cpuinfer_threads = min(os.cpu_count() // 2, 60)

    return KTConfig(
        # ...
        cpuinfer_threads=cpuinfer_threads,
        # ...
    )
```

**ç›¸å…³æ–‡ä»¶**:
- `python/sglang/srt/layers/moe/kt_ep_wrapper.py` (ç¬¬ 1168, 1175 è¡Œ)
- `python/sglang/srt/server_args.py` (ç¬¬ 498 è¡Œ)
- `kt_kernel/experts_base.py` (ç¬¬ 122-123 è¡Œ)

---

### Bug #3: SFT æ¨¡å¼ä¸‹ `load_weights()` è°ƒç”¨é¡ºåºé”™è¯¯

**çŠ¶æ€**: å·²è§£å†³ âœ…

**å‘ç°æ—¥æœŸ**: 2026-01-15

**è§£å†³æ—¥æœŸ**: 2026-01-15

**é”™è¯¯ä¿¡æ¯**:
```
RuntimeError: Base weights not set. Call load_weights_from_tensors() first, or ensure gate_proj, up_proj, down_proj are set before calling load_weights().
```

**è§¦å‘å‘½ä»¤**:
```bash
python -m sglang.launch_server \
      --model-path /mnt/data3/models/DeepSeek-V2-Lite-Chat \
      --kt-weight-path /mnt/data3/models/DeepSeek-V2-Lite-Chat-CPU-weight-INT8 \
      --kt-moe-lora-path /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/adapter_model_converted.safetensors \
      --kt-moe-lora-rank 8 \
      --kt-moe-lora-alpha 16.0 \
      --kt-moe-sft-method AMXBF16_SFT \
      --kt-num-gpu-experts 0 \
      --kt-cpuinfer 60 \
      --lora-paths /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/adapter_model.safetensors
```

**é”™è¯¯è°ƒç”¨æ ˆ**:
```
File "kt_ep_wrapper.py", line 1226, in process_weights_after_loading
    self.wrapper.load_weights(physical_to_logical_map_cpu)
File "kt_kernel/utils/amx_sft.py", line 172, in load_weights
    raise RuntimeError(
RuntimeError: Base weights not set. Call load_weights_from_tensors() first...
```

**æ ¹æœ¬åŸå› åˆ†æ**:

1. **é—®é¢˜ä½ç½®**: `kt_ep_wrapper.py:1226` â†’ `kt_kernel/utils/amx_sft.py:172`

2. **é—®é¢˜æè¿°**:
   - **Inference æ¨¡å¼**: `load_weights(physical_to_logical_map)` ç›´æ¥ä»æ–‡ä»¶åŠ è½½æƒé‡
   - **SFT æ¨¡å¼**: éœ€è¦å…ˆè°ƒç”¨ `load_weights_from_tensors(gate_proj, up_proj, down_proj, ...)` è®¾ç½®åŸºç¡€æƒé‡ï¼Œç„¶åå†è°ƒç”¨ `load_weights()`

3. **æ ¸å¿ƒé—®é¢˜**: SFT æ¨¡å¼ (`AMXSFTMoEWrapper`) çš„æƒé‡åŠ è½½ API ä¸ Inference æ¨¡å¼ (`AMXMoEWrapper`) ä¸åŒ

**è§£å†³æ–¹æ¡ˆ**:

**å…³é”®å‘ç°**: `MOESFTConfig` ç»§æ‰¿è‡ª `GeneralMOEConfig`ï¼Œå› æ­¤ SFT æ¨¡å¼ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸ Inference æ¨¡å¼ç›¸åŒçš„æ–‡ä»¶åŠ è½½æœºåˆ¶ã€‚ç»§æ‰¿å…³ç³»è¯¦è§ `kt-kernel/operators/common.hpp`:

```cpp
struct MOESFTConfig : public GeneralMOEConfig {
    int lora_rank = 16;
    float lora_alpha = 32.0f;
    // LoRA weight pointers...
};

struct GeneralMOEConfig {
    std::vector<std::vector<void*>> gate_projs;
    std::vector<std::vector<void*>> gate_scales;
    std::string path;
    bool save = false;
    bool load = false;
    // ...
};
```

**åŠ è½½ç­–ç•¥ï¼ˆæŒ‰ SFT æ–¹æ³•ç±»å‹ï¼‰**:

| SFT Method | æƒé‡æ¥æº | åŠ è½½å™¨ | æ˜¯å¦éœ€è¦ Scale |
|------------|----------|--------|----------------|
| AMXBF16_SFT | HuggingFace æ¨¡å‹è·¯å¾„ (`--model-path`) | `BF16SafeTensorLoader` | å¦ |
| AMXINT8_SFT | KT é‡åŒ–æƒé‡è·¯å¾„ (`--kt-weight-path`) | `SafeTensorLoader` | æ˜¯ |
| AMXINT4_SFT | KT é‡åŒ–æƒé‡è·¯å¾„ (`--kt-weight-path`) | `SafeTensorLoader` | æ˜¯ |

**å®ç°ä¿®æ”¹**:

1. **æ–°å¢ `BF16SafeTensorLoader`** (`kt-kernel/python/utils/loader.py`):
   - ä¸“é—¨ç”¨äºåŠ è½½ HuggingFace æ ¼å¼çš„ BF16 æƒé‡
   - è‡ªåŠ¨æ£€æµ‹ DeepSeek/Mixtral æ ¼å¼
   - è¿”å› `gate_scale: None, up_scale: None, down_scale: None`

2. **ä¿®æ”¹ `AMXSFTMoEWrapper.load_weights()`** (`kt-kernel/python/utils/amx_sft.py`):
   - æ”¯æŒä»æ–‡ä»¶åŠ è½½ï¼Œç±»ä¼¼äº Inference æ¨¡å¼
   - æ ¹æ® `method` é€‰æ‹©æ­£ç¡®çš„ Loader

3. **ä¿®æ”¹ `kt_ep_wrapper.py`**:
   - SFT æ¨¡å¼ä¸‹è®¾ç½®æ­£ç¡®çš„æƒé‡è·¯å¾„ï¼š
     - AMXBF16_SFT: ä½¿ç”¨ `model_path` (HuggingFace æ¨¡å‹è·¯å¾„)
     - AMXINT8/INT4_SFT: ä½¿ç”¨ `kt_weight_path`

**ä»£ç ä¿®å¤** (`kt_ep_wrapper.py`):

```python
def process_weights_after_loading(self, layer: torch.nn.Module) -> None:
    # ...
    if self.tp_rank == 0 and self.wrapper is not None:
        # SFT æ¨¡å¼å’Œ Inference æ¨¡å¼éƒ½ä½¿ç”¨ç›¸åŒçš„ load_weights() æ¥å£
        # AMXSFTMoEWrapper.load_weights() å·²ä¿®æ”¹ä¸ºæ”¯æŒæ–‡ä»¶åŠ è½½
        self.wrapper.load_weights(physical_to_logical_map_cpu)

        # åŠ è½½ MoE LoRA æƒé‡
        if self.kt_config.moe_lora_enabled:
            self._load_moe_lora_weights()
```

**ä»£ç ä¿®å¤** (`kt-kernel/python/utils/amx_sft.py`):

```python
def load_weights(self, physical_to_logical_map_cpu: torch.Tensor) -> None:
    """Load base weights for this layer."""
    if self._weights_loaded:
        return

    # å¦‚æœåŸºç¡€æƒé‡æœªè®¾ç½®ï¼Œä»æ–‡ä»¶åŠ è½½
    if self.gate_proj is None or self.up_proj is None or self.down_proj is None:
        self._load_base_weights_from_file()

    # ... ç»§ç»­åŸæœ‰é€»è¾‘
```

**ç›¸å…³æ–‡ä»¶**:
- `kt-kernel/python/utils/loader.py` (æ–°å¢ `BF16SafeTensorLoader`)
- `kt-kernel/python/utils/amx_sft.py` (ä¿®æ”¹ `load_weights()`)
- `python/sglang/srt/layers/moe/kt_ep_wrapper.py` (è®¾ç½®æ­£ç¡®çš„æƒé‡è·¯å¾„)

---

### Bug #4: `safe_open` å¯¹è±¡æ²¡æœ‰ `close()` æ–¹æ³•

**çŠ¶æ€**: å·²è§£å†³ âœ…

**å‘ç°æ—¥æœŸ**: 2026-01-16

**è§£å†³æ—¥æœŸ**: 2026-01-16

**é”™è¯¯ä¿¡æ¯**:
```
File "kt_kernel/utils/loader.py", line 168, in close_all_handles
    handle.close()
    ^^^^^^^^^^^^
AttributeError: 'builtins.safe_open' object has no attribute 'close'
```

**è§¦å‘å‘½ä»¤**:
```bash
# ä»»ä½•ä½¿ç”¨ SFT æ¨¡å¼åŠ è½½æƒé‡çš„å‘½ä»¤
python -m sglang.launch_server \
    --model-path /mnt/data3/models/DeepSeek-V2-Lite-Chat \
    --kt-weight-path /mnt/data3/models/DeepSeek-V2-Lite-Chat-CPU-weight-INT8 \
    --kt-moe-lora-path /path/to/moe_lora.pt \
    --kt-moe-sft-method AMXBF16_SFT \
    --kt-num-gpu-experts 0 \
    --kt-cpuinfer 60
```

**æ ¹æœ¬åŸå› åˆ†æ**:

1. **é—®é¢˜ä½ç½®**: `kt-kernel/python/utils/loader.py:168`

2. **é—®é¢˜æè¿°**:
   - `safetensors.safe_open` è¿”å›çš„å¯¹è±¡æ²¡æœ‰ `close()` æ–¹æ³•
   - è¯¥å¯¹è±¡è¢«è®¾è®¡ä¸ºä½¿ç”¨ Python çš„ context managerï¼ˆ`with` è¯­å¥ï¼‰æ¥ç®¡ç†èµ„æº
   - ä»£ç å°è¯•è°ƒç”¨ `handle.close()` ä¼šå¯¼è‡´ `AttributeError`

3. **è§¦å‘æµç¨‹**:
   ```
   AMXSFTMoEWrapper._load_base_weights_from_file()
       â†“
   BF16SafeTensorLoader.load_experts()
       â†“
   loader.close_all_handles()
       â†“
   handle.close()  â† safe_open æ²¡æœ‰æ­¤æ–¹æ³•ï¼ŒæŠ¥é”™
   ```

**è§£å†³æ–¹æ¡ˆ**:

ä¿®æ”¹ `close_all_handles()` æ–¹æ³•ï¼Œä¸è°ƒç”¨ `close()`ï¼Œåªæ¸…é™¤å¼•ç”¨è®©åƒåœ¾å›æ”¶å¤„ç†ï¼š

```python
# kt-kernel/python/utils/loader.py
def close_all_handles(self):
    """Close all file handles and clear the handle map.

    Note: safetensors.safe_open doesn't have a close() method,
    so we just clear the references and let garbage collection handle cleanup.
    """
    # safetensors.safe_open doesn't have close(), just clear references
    self.file_handle_map.clear()
```

**ç›¸å…³æ–‡ä»¶**:
- `kt-kernel/python/utils/loader.py` (ç¬¬ 166-173 è¡Œ)

---

## è¿›åº¦è·Ÿè¸ª

| æ—¥æœŸ | äº‹é¡¹ | çŠ¶æ€ |
|------|------|------|
| 2026-01-15 | å‘ç° Bug #1: `kt_num_gpu_experts` ä¸º None é—®é¢˜ | ä¸´æ—¶æ–¹æ¡ˆå¯ç”¨ï¼Œå¾…ä»£ç ä¿®å¤ |
| 2026-01-15 | å‘ç° Bug #2: `kt_cpuinfer` ä¸º None é—®é¢˜ | ä¸´æ—¶æ–¹æ¡ˆå¯ç”¨ï¼Œå¾…ä»£ç ä¿®å¤ |
| 2026-01-15 | å‘ç° Bug #3: SFT æ¨¡å¼æƒé‡åŠ è½½ API ä¸å…¼å®¹ | âœ… å·²è§£å†³ |
| 2026-01-15 | æ–°å¢ `BF16SafeTensorLoader` åˆ° kt-kernel | âœ… å®Œæˆ |
| 2026-01-15 | è®¾è®¡ SFT æƒé‡åŠ è½½ç­–ç•¥ (BF16/INT8/INT4) | âœ… å®Œæˆ |
| 2026-01-16 | å‘ç°å¹¶ä¿®å¤ Bug #4: `safe_open.close()` ä¸å­˜åœ¨ | âœ… å·²è§£å†³ |

---

## å®Œæ•´ä¸´æ—¶è§£å†³æ–¹æ¡ˆ

éœ€è¦æ·»åŠ ä»¥ä¸‹å‚æ•°æ‰èƒ½æ­£å¸¸å¯åŠ¨ MoE LoRA æ¨ç†ï¼š
- `--kt-num-gpu-experts 0` (æ‰€æœ‰ä¸“å®¶åœ¨ CPU ä¸Šè¿è¡Œ)
- `--kt-cpuinfer 60` (CPU æ¨ç†çº¿ç¨‹æ•°ï¼Œæ ¹æ®å®é™… CPU æ ¸å¿ƒæ•°è°ƒæ•´)

**å®Œæ•´å¯åŠ¨å‘½ä»¤**:
```bash
python -m sglang.launch_server \
      --model-path /mnt/data3/models/DeepSeek-V2-Lite-Chat \
      --kt-weight-path /mnt/data3/models/DeepSeek-V2-Lite-Chat-CPU-weight-INT8 \
      --kt-moe-lora-path /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/adapter_model_converted.safetensors \
      --kt-moe-lora-rank 8 \
      --kt-moe-lora-alpha 16.0 \
      --kt-moe-sft-method AMXBF16_SFT \
      --kt-num-gpu-experts 0 \
      --kt-cpuinfer 60 \
      --lora-paths /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/adapter_model.safetensors
```

---

### Bug #5: "header too large" Error - è½¬æ¢è„šæœ¬è¾“å‡ºæ ¼å¼é”™è¯¯

**çŠ¶æ€**: å·²è§£å†³ âœ…

**å‘ç°æ—¥æœŸ**: 2026-01-16

**è§£å†³æ—¥æœŸ**: 2026-01-16

**é”™è¯¯ä¿¡æ¯**:
```
RuntimeError: Failed to load LoRA adapter /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/: Error while deserializing header: header too large
```

**è§¦å‘å‘½ä»¤**:
```bash
# ç”¨æˆ·ä½¿ç”¨äº† .safetensors æ‰©å±•åä½œä¸ºè½¬æ¢è„šæœ¬è¾“å‡º
python scripts/convert_moe_lora.py \
    --input .../adapter_model.safetensors \
    --output .../adapter_model_converted.safetensors  # é”™è¯¯ï¼
```

**æ ¹æœ¬åŸå› åˆ†æ**:

1. **é—®é¢˜ä½ç½®**: `convert_moe_lora.py` è¾“å‡ºæ–‡ä»¶ + sglang LoRA åŠ è½½å™¨

2. **é—®é¢˜æè¿°**:
   - `convert_moe_lora.py` ä½¿ç”¨ `torch.save()` ä¿å­˜ä¸º **PyTorch `.pt` æ ¼å¼**
   - ä½†ç”¨æˆ·æŒ‡å®šäº† `.safetensors` æ‰©å±•åä½œä¸ºè¾“å‡º
   - sglang çš„ LoRA åŠ è½½å™¨ä½¿ç”¨ `glob(*.safetensors)` æ‰¾åˆ°è¿™ä¸ªæ–‡ä»¶
   - å°è¯•ç”¨ safetensors æ ¼å¼è§£æ PyTorch æ–‡ä»¶ï¼Œå¯¼è‡´ "header too large" é”™è¯¯

3. **é—®é¢˜æµç¨‹**:
   ```
   convert_moe_lora.py
       torch.save(result, "output.safetensors")  # å®é™…æ˜¯ PyTorch æ ¼å¼
           â†“
   sglang LoRA åŠ è½½
       glob("*.safetensors")  # æ‰¾åˆ°è¿™ä¸ªæ–‡ä»¶
           â†“
   safetensors_weights_iterator()
       safe_open("output.safetensors")  # å°è¯•ç”¨ safetensors æ ¼å¼è§£æ
           â†“
   "Error: header too large"  # PyTorch æ ¼å¼æ— æ³•è¢« safetensors è§£æ
   ```

**è§£å†³æ–¹æ¡ˆ**:

1. **åˆ é™¤é”™è¯¯æ–‡ä»¶**:
```bash
rm /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/adapter_model_converted.safetensors
```

2. **ä½¿ç”¨æ­£ç¡®çš„æ‰©å±•åå’Œè·¯å¾„é‡æ–°è½¬æ¢**:
```bash
python scripts/convert_moe_lora.py \
    --input /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/adapter_model.safetensors \
    --config /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/adapter_config.json \
    --output /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/moe_lora.pt
```

**æ³¨æ„äº‹é¡¹**:
- è¾“å‡ºæ–‡ä»¶å¿…é¡»ä½¿ç”¨ `.pt` æ‰©å±•å
- å»ºè®®å°†è¾“å‡ºæ–‡ä»¶æ”¾åœ¨ checkpoint ç›®å½•**å¤–éƒ¨**ï¼Œé¿å…è¢« sglang çš„ `glob(*.safetensors)` è¯¯è¯†åˆ«

**ç›¸å…³æ–‡ä»¶**:
- `scripts/convert_moe_lora.py` (ç¬¬ 207 è¡Œ: `torch.save()`)
- `python/sglang/srt/model_loader/loader.py` (ç¬¬ 434 è¡Œ: `glob.glob()`)
- `python/sglang/srt/lora/lora.py` (ç¬¬ 90 è¡Œ: `_get_weights_iterator`)

---

### Bug #6: SFT æ¨¡å¼ (MoE LoRA) CUDA Graph è®¾å¤‡ä¸åŒ¹é…

**çŠ¶æ€**: âœ… å·²è§£å†³

**å‘ç°æ—¥æœŸ**: 2026-01-16

**è§£å†³æ—¥æœŸ**: 2026-01-16

**é”™è¯¯ä¿¡æ¯**:
```
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
```

**è§¦å‘å‘½ä»¤**:
```bash
# åªæœ‰ä½¿ç”¨ --kt-moe-lora-path å‚æ•°æ—¶æ‰ä¼šè§¦å‘
CUDA_VISIBLE_DEVICES=3 python -m sglang.launch_server \
    --model-path /mnt/data3/models/DeepSeek-V2-Lite-Chat \
    --kt-weight-path /mnt/data3/models/DeepSeek-V2-Lite-Chat-CPU-weight-INT8 \
    --kt-moe-lora-path /path/to/moe_lora.pt \  # å¯ç”¨ SFT æ¨¡å¼
    --kt-moe-sft-method AMXBF16_SFT \
    --kt-num-gpu-experts 0 \
    --kt-cpuinfer 60 \
    --lora-paths /path/to/checkpoint-133
```

**é‡è¦æ¾„æ¸…**:

**ä¹‹å‰çš„åˆ†ææœ‰è¯¯ï¼** kt-kernel åŸå…ˆçš„æ¨ç†æ¨¡å¼ï¼ˆä¸å« MoE LoRAï¼‰æ˜¯**å¯ä»¥å…¼å®¹ CUDA graph** çš„ã€‚

é—®é¢˜**åªå‘ç”Ÿ**åœ¨å¯ç”¨ `--kt-moe-lora-path` å‚æ•°æ—¶ï¼ˆSFT æ¨¡å¼ï¼‰ï¼Œå› ä¸ºï¼š
- æ¨ç†æ¨¡å¼çš„ `sync_forward()` è¿”å› **GPU tensor**ï¼ˆåœ¨å†…éƒ¨å®Œæˆ CPUâ†’GPU æ‹·è´ï¼‰
- SFT æ¨¡å¼çš„ `forward_sft()` è¿”å› **CPU tensor**

**æ ¹æœ¬åŸå› åˆ†æ**:

1. **é—®é¢˜ä½ç½®**: `kt_ep_wrapper.py:1476`
   ```python
   output = output + cpu_output
   # output: cuda:0 (GPU experts è®¡ç®—ç»“æœ)
   # cpu_output: cpu (SFT æ¨¡å¼ä¸‹ forward_sft è¿”å› CPU tensor)
   ```

2. **æ ¸å¿ƒå·®å¼‚**:
   ```python
   # kt_ep_wrapper.py ä¸­ sync() æ–¹æ³•
   def sync(self, x, dispatch_output=None):
       if self.kt_config.moe_lora_enabled:  # --kt-moe-lora-path è®¾ç½®æ—¶
           return self.wrapper.forward_sft(...)  # è¿”å› CPU tensor âŒ
       else:
           return self.wrapper.sync_forward(...)  # è¿”å› GPU tensor âœ“
   ```

3. **forward_sft è¿”å› CPU tensor**:
   ```python
   # amx_sft.py
   def forward_sft(...):
       return buffer.output_cpu.clone()  # CPU tensor!
   ```

**è¯¦ç»†åˆ†æ**: è§ `/home/lpl/sglang-debug/sglang/docs/kt_lora_chat/08_cuda_graphåˆ†æ.md`

**è§£å†³æ–¹æ¡ˆ**:

**å·²å®ç°ä»¥ä¸‹ä¸¤ä¸ªä¿®å¤**ï¼š

1. **ä¿®å¤è®¾å¤‡ä¸åŒ¹é…** (`kt_ep_wrapper.py`):
   ```python
   # apply() æ–¹æ³•ä¸­
   cpu_output = self.sync(x, dispatch_output)
   if cpu_output.device != output.device:
       cpu_output = cpu_output.to(output.device, non_blocking=True)
   output = output + cpu_output
   ```

2. **è‡ªåŠ¨ç¦ç”¨ CUDA Graph** (`server_args.py`):
   ```python
   def _handle_kt_moe_lora(self):
       """Disable CUDA graph when kt-kernel MoE LoRA is enabled."""
       if self.kt_moe_lora_path is not None:
           if not self.disable_cuda_graph:
               logger.warning(
                   "CUDA graph is disabled because kt-kernel MoE LoRA is enabled. "
                   "SFT mode requires synchronous CPU computation."
               )
               self.disable_cuda_graph = True
   ```

**ç›¸å…³æ–‡ä»¶**:
- `python/sglang/srt/layers/moe/kt_ep_wrapper.py` (ç¬¬ 1476 è¡Œ)
- `python/sglang/srt/server_args.py` (æ–°å¢ `_handle_kt_moe_lora()` æ–¹æ³•)

---

## è¿›åº¦è·Ÿè¸ª

| æ—¥æœŸ | äº‹é¡¹ | çŠ¶æ€ |
|------|------|------|
| 2026-01-15 | å‘ç° Bug #1: `kt_num_gpu_experts` ä¸º None é—®é¢˜ | ä¸´æ—¶æ–¹æ¡ˆå¯ç”¨ï¼Œå¾…ä»£ç ä¿®å¤ |
| 2026-01-15 | å‘ç° Bug #2: `kt_cpuinfer` ä¸º None é—®é¢˜ | ä¸´æ—¶æ–¹æ¡ˆå¯ç”¨ï¼Œå¾…ä»£ç ä¿®å¤ |
| 2026-01-15 | å‘ç° Bug #3: SFT æ¨¡å¼æƒé‡åŠ è½½ API ä¸å…¼å®¹ | âœ… å·²è§£å†³ |
| 2026-01-15 | æ–°å¢ `BF16SafeTensorLoader` åˆ° kt-kernel | âœ… å®Œæˆ |
| 2026-01-15 | è®¾è®¡ SFT æƒé‡åŠ è½½ç­–ç•¥ (BF16/INT8/INT4) | âœ… å®Œæˆ |
| 2026-01-16 | å‘ç°å¹¶ä¿®å¤ Bug #4: `safe_open.close()` ä¸å­˜åœ¨ | âœ… å·²è§£å†³ |
| 2026-01-16 | å‘ç° Bug #5: è½¬æ¢è„šæœ¬è¾“å‡ºæ ¼å¼é”™è¯¯ | âœ… å·²è§£å†³ |
| 2026-01-16 | å‘ç°å¹¶ä¿®å¤ Bug #6: SFT æ¨¡å¼ CUDA Graph è®¾å¤‡ä¸åŒ¹é… | âœ… å·²è§£å†³ |

---

## å®Œæ•´å¯åŠ¨å‘½ä»¤

éœ€è¦æ·»åŠ ä»¥ä¸‹å‚æ•°æ‰èƒ½æ­£å¸¸å¯åŠ¨ MoE LoRA æ¨ç†ï¼š
- `--kt-num-gpu-experts 0` (æ‰€æœ‰ä¸“å®¶åœ¨ CPU ä¸Šè¿è¡Œ)
- `--kt-cpuinfer 60` (CPU æ¨ç†çº¿ç¨‹æ•°ï¼Œæ ¹æ®å®é™… CPU æ ¸å¿ƒæ•°è°ƒæ•´)
- ä¸éœ€è¦æ‰‹åŠ¨æ·»åŠ  `--disable-cuda-graph`ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ç¦ç”¨

**å®Œæ•´å¯åŠ¨å‘½ä»¤**:
```bash
CUDA_VISIBLE_DEVICES=3 python -m sglang.launch_server \
    --model-path /mnt/data3/models/DeepSeek-V2-Lite-Chat \
    --kt-weight-path /mnt/data3/models/DeepSeek-V2-Lite-Chat-CPU-weight-INT8 \
    --kt-moe-lora-path /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/moe_lora.pt \
    --kt-moe-lora-rank 8 \
    --kt-moe-lora-alpha 16.0 \
    --kt-moe-sft-method AMXBF16_SFT \
    --kt-num-gpu-experts 0 \
    --kt-cpuinfer 60 \
    --lora-paths /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133
```

---

### Bug #7: æ–°ç‰ˆæœ¬ MoE LoRA è¾“å‡ºä¹±ç 

**çŠ¶æ€**: âœ… å·²è§£å†³

**å‘ç°æ—¥æœŸ**: 2026-01-16

**è§£å†³æ—¥æœŸ**: 2026-01-16

**é—®é¢˜æè¿°**:

- **æ—§ç‰ˆæœ¬**ï¼ˆåªæœ‰ Attention + Shared Experts LoRAï¼‰: è¾“å‡ºæ­£å¸¸
- **æ–°ç‰ˆæœ¬**ï¼ˆåŠ ä¸Š Routed Experts LoRAï¼‰: è¾“å‡ºä¹±ç ï¼ˆå¦‚ "balenabalenabalena..."ï¼‰

**æ ¹æœ¬åŸå›  1**: `topk_ids` è¢«åŸåœ°ä¿®æ”¹

`mask_cpu_expert_ids` å‡½æ•°åœ¨ `kt_ep_wrapper.py:1062` **åŸåœ°ä¿®æ”¹**äº† `topk_ids`ï¼š

```python
def mask_cpu_expert_ids(topk_ids: torch.Tensor, num_gpu_experts: int) -> torch.Tensor:
    topk_ids[topk_ids >= num_gpu_experts] = -1  # â† åŸåœ°ä¿®æ”¹ï¼
    return topk_ids
```

**ä¿®å¤**: æ·»åŠ  `.clone()` é¿å…åŸåœ°ä¿®æ”¹ âœ… å·²ä¿®å¤

**æ ¹æœ¬åŸå›  2**: `hidden_states` è¢« GPU è®¡ç®—ä¿®æ”¹

åœ¨ `apply()` æ–¹æ³•ä¸­ï¼Œ`x = dispatch_output.hidden_states` åªæ˜¯è·å–å¼•ç”¨ï¼Œè€Œ `masked_dispatch_output.hidden_states` å’Œ `x` æ˜¯åŒä¸€ä¸ªå¼•ç”¨ã€‚GPU æ–¹æ³• `gpu_method.apply()` å¯èƒ½åŸåœ°ä¿®æ”¹äº†è¿™ä¸ª tensorï¼ˆç”¨ä½œè¾“å‡ºç¼“å†²åŒºï¼‰ï¼Œå¯¼è‡´ `sync()` æ¥æ”¶åˆ°çš„ `x` å·²ç»æ˜¯é›¶ã€‚

**ä¿®å¤**: åœ¨ SFT æ¨¡å¼ä¸‹ï¼ŒGPU è®¡ç®—å‰ä¿å­˜ `hidden_states` çš„å‰¯æœ¬ âœ… å·²ä¿®å¤

```python
if self.kt_config.moe_lora_enabled:
    x_for_cpu = x.clone()
else:
    x_for_cpu = x
```

**éªŒè¯ç»“æœ**:
- `x.abs().mean(): 0.255859` âœ“ (éé›¶è¾“å…¥)
- `topk_ids.min(): 0, topk_ids.max(): 60` âœ“ (æ­£ç¡®çš„ä¸“å®¶ ID)
- è¾“å‡ºä¸å†ä¹±ç  âœ“

**ç›¸å…³æ–‡ä»¶**:
- `python/sglang/srt/layers/moe/kt_ep_wrapper.py:1062` (topk_ids ä¿®å¤)
- `python/sglang/srt/layers/moe/kt_ep_wrapper.py:1464-1467` (hidden_states ä¿®å¤)

---

### Bug #8: MoE LoRA è¾“å‡ºä¸åŸºåº§æ¨¡å‹å®Œå…¨ä¸€æ ·

**çŠ¶æ€**: ğŸ” è°ƒè¯•ä¸­ï¼ˆè¿›å…¥ C++ å±‚è°ƒè¯•é˜¶æ®µï¼‰

**å‘ç°æ—¥æœŸ**: 2026-01-16

**é—®é¢˜æè¿°**:

ä½¿ç”¨ `demo.py` æµ‹è¯•æ—¶ï¼Œ`DeepSeek-V2-Lite-Chat:lora0` å’Œ `DeepSeek-V2-Lite-Chat` çš„è¾“å‡º**é€å­—ç›¸åŒ**ã€‚è¿™è¡¨æ˜ MoE LoRA è™½ç„¶æ­£ç¡®åŠ è½½ï¼Œä½†æ²¡æœ‰å®é™…ç”Ÿæ•ˆã€‚

---

#### è°ƒè¯•é˜¶æ®µ 1: Python å±‚éªŒè¯ âœ… å®Œæˆ

**éªŒè¯é¡¹ç›®**:

| æ£€æŸ¥é¡¹ | çŠ¶æ€ | è¯´æ˜ |
|--------|------|------|
| LoRA æƒé‡å½¢çŠ¶ | âœ… | `[64, 8, 2048]` ç­‰ï¼Œä¸é¢„æœŸå®Œå…¨åŒ¹é… |
| LoRA æƒé‡å€¼ | âœ… | éé›¶ï¼Œstdâ‰ˆ0.004-0.005ï¼Œæ­£å¸¸åˆ†å¸ƒ |
| `init_lora_weights()` è°ƒç”¨ | âœ… | æ—¥å¿—ç¡®è®¤è¢«è°ƒç”¨ |
| `_weights_loaded` | âœ… | True |
| `moe is not None` | âœ… | True |
| `update_lora_weights()` è°ƒç”¨ | âœ… | æ—¥å¿—ç¡®è®¤è¢«è°ƒç”¨ |

**æ—¥å¿—è¯æ®**:
```
[DEBUG init_lora_weights] layer=1, _weights_loaded=True, moe=True
[MoE LoRA] Layer 1: update_lora_weights() called
```

**ç»“è®º**: Python å±‚å®Œå…¨æ­£ç¡®ï¼Œé—®é¢˜åœ¨ C++ åç«¯ã€‚

---

#### è°ƒè¯•é˜¶æ®µ 2: C++ å±‚åˆ†æ ğŸ” å½“å‰é˜¶æ®µ

**é‡è¦è¯´æ˜**: å½“å‰ä½¿ç”¨çš„æ˜¯**é TP æ¨¡å¼**ï¼Œä»£ç è·¯å¾„å¦‚ä¸‹ï¼š

```
Python: AMXSFTMoEWrapper (amx_sft.py)
  â†’ self.moe = AMXBF16_SFT_MOE(config)  # ç»‘å®šåˆ° C++ AMX_SFT_MOE_TP

Python: update_lora_weights()
  â†’ self.moe.update_lora_weights_task(gate_lora_a.data_ptr(), ...)
  â†’ C++: AMX_SFT_MOE_TP::update_lora_weights()
    â†’ è®¾ç½® gate_lora_a_, gate_lora_b_, etc. æŒ‡é’ˆ
    â†’ è®¾ç½® lora_weights_prepared_ = false

Python: forward_sft()
  â†’ C++: AMX_SFT_MOE_TP::forward_sft()
    â†’ æ£€æŸ¥ if (gate_lora_a_ != nullptr && gate_lora_b_ != nullptr)
    â†’ è°ƒç”¨ compute_lora_gate_up_amx() æˆ– compute_lora_gate_up()
      â†’ prepare_lora_weights() (è½¬æ¢ä¸º BufferB æ ¼å¼)
      â†’ æ‰§è¡Œ LoRA è®¡ç®—
```

**å…³é”® C++ ä»£ç ** (`kt-kernel/operators/amx/sft_moe.hpp`):

```cpp
// Line 523-535: forward_sft() ä¸­çš„ LoRA åˆ†æ”¯
// Step 5.5: Gate + Up LoRA
if (gate_lora_a_ != nullptr && gate_lora_b_ != nullptr) {
    if constexpr (supports_standard_mat_mul_v<T>) {
        compute_lora_gate_up_amx(qlen, activated_expert);  // AMX-optimized path
    } else {
        compute_lora_gate_up(qlen, activated_expert);  // For-loop fallback
    }
}

// Line 688-702: update_lora_weights() è®¾ç½®æŒ‡é’ˆ
void update_lora_weights(void* gate_lora_a, void* gate_lora_b, ...) {
    gate_lora_a_ = (ggml_bf16_t*)gate_lora_a;
    gate_lora_b_ = (ggml_bf16_t*)gate_lora_b;
    // ...
    lora_weights_prepared_ = false;  // éœ€è¦é‡æ–°è½¬æ¢ä¸º BufferB
}
```

**å¯èƒ½é—®é¢˜ç‚¹**:
1. C++ `update_lora_weights()` æ²¡æœ‰æ­£ç¡®æ¥æ”¶åˆ° Python ä¼ æ¥çš„æŒ‡é’ˆ
2. `forward_sft()` ä¸­ `gate_lora_a_` æŒ‡é’ˆä¸º nullptrï¼ˆæœªè¢«è®¾ç½®ï¼‰
3. `prepare_lora_weights()` è½¬æ¢å¤±è´¥
4. LoRA è®¡ç®—é€»è¾‘æœ‰ bug

---

#### ä¸‹ä¸€æ­¥è°ƒè¯•æ–¹æ¡ˆ

**Step 1**: åœ¨ C++ å±‚æ·»åŠ è°ƒè¯•æ‰“å°

å·²åœ¨ä»¥ä¸‹ä½ç½®æ·»åŠ è°ƒè¯•ä»£ç ï¼š

1. `sft_moe.hpp:690-692` - `update_lora_weights()`:
```cpp
printf("[C++ AMX_SFT_MOE_TP::update_lora_weights] tp_part=%d, gate_lora_a=%p, gate_lora_b=%p\n",
       tp_part_idx, gate_lora_a, gate_lora_b);
```

2. `sft_moe.hpp:525-527` - `forward_sft()`:
```cpp
if (tp_part_idx == 0 && qlen > 0) {
    printf("[C++ forward_sft] tp_part=%d, qlen=%d, gate_lora_a_=%p, gate_lora_b_=%p\n",
           tp_part_idx, qlen, (void*)gate_lora_a_, (void*)gate_lora_b_);
}
```

**Step 2**: é‡æ–°ç¼–è¯‘ kt-kernel

```bash
cd /home/lpl/ktransformers-sglang/kt-kernel
pip install -e . --no-build-isolation
```

**Step 3**: é‡å¯æœåŠ¡ï¼Œè§‚å¯Ÿæ—¥å¿—

æœŸæœ›çœ‹åˆ°:
- `[C++ AMX_SFT_MOE_TP::update_lora_weights]` - ç¡®è®¤ C++ å±‚æ”¶åˆ° LoRA æŒ‡é’ˆ
- `[C++ forward_sft]` - ç¡®è®¤ forward æ—¶ LoRA æŒ‡é’ˆçŠ¶æ€

**é¢„æœŸç»“æœåˆ†æ**:
- å¦‚æœ `gate_lora_a` ä¸º `0x0`ï¼ˆnullptrï¼‰ï¼šé—®é¢˜åœ¨ Pythonâ†’C++ è°ƒç”¨é“¾
- å¦‚æœ `gate_lora_a` éç©ºä½† LoRA ä»æ— æ•ˆï¼šé—®é¢˜åœ¨ C++ è®¡ç®—é€»è¾‘

---

**ç›¸å…³æ–‡ä»¶**:
- `kt-kernel/operators/amx/sft_moe.hpp` (C++ LoRA å®ç°ï¼Œé TP æ¨¡å¼)
- `kt-kernel/python/utils/amx_sft.py` (Python wrapper)
- `python/sglang/srt/layers/moe/kt_ep_wrapper.py` (sglang é›†æˆ)

**æ³¨æ„**: ä¿®æ”¹ C++ `.hpp` æ–‡ä»¶åå¿…é¡»é‡æ–°ç¼–è¯‘ kt-kernel æ‰èƒ½ç”Ÿæ•ˆ

---

## è¿›åº¦è·Ÿè¸ª

| æ—¥æœŸ | äº‹é¡¹ | çŠ¶æ€ |
|------|------|------|
| 2026-01-15 | å‘ç° Bug #1: `kt_num_gpu_experts` ä¸º None é—®é¢˜ | ä¸´æ—¶æ–¹æ¡ˆå¯ç”¨ï¼Œå¾…ä»£ç ä¿®å¤ |
| 2026-01-15 | å‘ç° Bug #2: `kt_cpuinfer` ä¸º None é—®é¢˜ | ä¸´æ—¶æ–¹æ¡ˆå¯ç”¨ï¼Œå¾…ä»£ç ä¿®å¤ |
| 2026-01-15 | å‘ç° Bug #3: SFT æ¨¡å¼æƒé‡åŠ è½½ API ä¸å…¼å®¹ | âœ… å·²è§£å†³ |
| 2026-01-15 | æ–°å¢ `BF16SafeTensorLoader` åˆ° kt-kernel | âœ… å®Œæˆ |
| 2026-01-15 | è®¾è®¡ SFT æƒé‡åŠ è½½ç­–ç•¥ (BF16/INT8/INT4) | âœ… å®Œæˆ |
| 2026-01-16 | å‘ç°å¹¶ä¿®å¤ Bug #4: `safe_open.close()` ä¸å­˜åœ¨ | âœ… å·²è§£å†³ |
| 2026-01-16 | å‘ç° Bug #5: è½¬æ¢è„šæœ¬è¾“å‡ºæ ¼å¼é”™è¯¯ | âœ… å·²è§£å†³ |
| 2026-01-16 | å‘ç°å¹¶ä¿®å¤ Bug #6: SFT æ¨¡å¼ CUDA Graph è®¾å¤‡ä¸åŒ¹é… | âœ… å·²è§£å†³ |
| 2026-01-16 | Bug #7: æ–°ç‰ˆæœ¬è¾“å‡ºä¹±ç  (ä¸¤å¤„åŸåœ°ä¿®æ”¹é—®é¢˜) | âœ… å·²è§£å†³ |
| 2026-01-16 | Bug #8: MoE LoRA è¾“å‡ºä¸åŸºåº§æ¨¡å‹ä¸€æ · | ğŸ” è°ƒè¯•ä¸­ |
| 2026-01-16 | åˆ é™¤ kt_ep_wrapper.py è°ƒè¯•æ‰“å°ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰ | âœ… å®Œæˆ |
| 2026-01-16 | æ·»åŠ  amx_sft.py ç²¾ç®€è°ƒè¯•è¾“å‡º | âœ… å®Œæˆ |

---

## å®Œæ•´å¯åŠ¨å‘½ä»¤

éœ€è¦æ·»åŠ ä»¥ä¸‹å‚æ•°æ‰èƒ½æ­£å¸¸å¯åŠ¨ MoE LoRA æ¨ç†ï¼š
- `--kt-num-gpu-experts 0` (æ‰€æœ‰ä¸“å®¶åœ¨ CPU ä¸Šè¿è¡Œ)
- `--kt-cpuinfer 60` (CPU æ¨ç†çº¿ç¨‹æ•°ï¼Œæ ¹æ®å®é™… CPU æ ¸å¿ƒæ•°è°ƒæ•´)
- ä¸éœ€è¦æ‰‹åŠ¨æ·»åŠ  `--disable-cuda-graph`ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ç¦ç”¨

**å®Œæ•´å¯åŠ¨å‘½ä»¤**:
```bash
CUDA_VISIBLE_DEVICES=3 python -m sglang.launch_server \
    --model-path /mnt/data3/models/DeepSeek-V2-Lite-Chat \
    --kt-weight-path /mnt/data3/models/DeepSeek-V2-Lite-Chat-CPU-weight-INT8 \
    --kt-moe-lora-path /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/moe_lora.pt \
    --kt-moe-lora-rank 8 \
    --kt-moe-lora-alpha 16.0 \
    --kt-moe-sft-method AMXBF16_SFT \
    --kt-num-gpu-experts 0 \
    --kt-cpuinfer 60 \
    --lora-paths /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133
```

---

## å¤‡æ³¨

- è½¬æ¢è„šæœ¬ `convert_moe_lora.py` è¾“å‡ºå¿…é¡»ä½¿ç”¨ `.pt` æ‰©å±•å
- å½“å‰æµ‹è¯•ç¯å¢ƒï¼šDeepSeek-V2-Lite-Chat æ¨¡å‹
- LoRA é…ç½®ï¼šrank=8, alpha=16.0
- **é‡è¦æ¾„æ¸…**ï¼škt-kernel æ ‡å‡†æ¨ç†æ¨¡å¼å¯ä»¥å…¼å®¹ CUDA graphï¼Œåªæœ‰ SFT æ¨¡å¼ (MoE LoRA) éœ€è¦ç¦ç”¨
- å½“ä½¿ç”¨ `--kt-moe-lora-path` æ—¶ï¼ŒCUDA graph ä¼šè‡ªåŠ¨ç¦ç”¨
