# MoE Expert LoRA 功能需求规格

## 1. 概述

在 sglang 中添加对 MoE routed experts LoRA 的支持，使用户能够在 CPU 上运行带有 LoRA 适配器的 MoE 专家计算。

## 2. 功能范围

### 2.1 支持范围

| 功能项 | 状态 | 说明 |
|--------|------|------|
| CPU routed experts LoRA | **新增** | 通过 kt-kernel SFT MoE 实现 |
| GPU shared experts LoRA | 已支持 | 现有 sglang LoRA 路径 |
| GPU attention LoRA | 已支持 | 现有 sglang LoRA 路径 |
| 静态加载 | **新增** | 启动时加载 adapter |
| 动态切换 | 不支持 | 不在本次范围 |
| 训练/微调 | 不支持 | 仅推理 |

### 2.2 不在范围内

- GPU routed experts LoRA（复杂度高，暂不实现）
- 多 adapter 动态切换
- 在线训练/梯度计算

## 3. 用户场景

### 3.1 典型使用场景

1. 用户有预训练的 DeepSeek-V2/V3 模型
2. 用户有 PEFT 格式的 LoRA adapter（Kllama2 格式，含 MoE expert LoRA）
3. 用户希望在推理时应用 MoE expert LoRA

### 3.2 预期工作流

```
1. 预处理阶段
   用户运行转换脚本：
   $ python scripts/convert_moe_lora.py \
       --input /path/to/adapter_model.safetensors \
       --output /path/to/moe_lora.pt

2. 推理阶段
   sglang 启动时指定 MoE LoRA：
   $ python -m sglang.launch_server \
       --model-path /path/to/deepseek-v2 \
       --kt-weight-path /path/to/weights \
       --kt-moe-lora-path /path/to/moe_lora.pt \
       --lora-paths /path/to/adapter  # 原有 attention/shared LoRA
```

## 4. 详细需求

### 4.1 Adapter 格式支持

#### 4.1.1 输入格式（PEFT）

支持两种 key 命名模式：

**模式 A（original_moe）**:
```
base_model.model.model.layers.{L}.mlp.original_moe.experts.{E}.{gate,up,down}_proj.lora_{A,B}.weight
```

**模式 B（直接 experts）**:
```
base_model.model.model.layers.{L}.mlp.experts.{E}.{gate,up,down}_proj.lora_{A,B}.weight
```

#### 4.1.2 输出格式（kt-kernel）

```python
{
    'metadata': {
        'lora_rank': 16,
        'lora_alpha': 32.0,
        'num_experts': 64,
        'num_layers': 27,
        'hidden_size': 7168,
        'intermediate_size': 2048,
    },
    'layer_0': {
        'gate_lora_a': Tensor[64, 16, 7168],
        'gate_lora_b': Tensor[64, 2048, 16],
        'up_lora_a': Tensor[64, 16, 7168],
        'up_lora_b': Tensor[64, 2048, 16],
        'down_lora_a': Tensor[64, 16, 2048],
        'down_lora_b': Tensor[64, 7168, 16],
    },
    'layer_1': { ... },
    ...
}
```

### 4.2 命令行参数

| 参数 | 类型 | 默认值 | 说明 |
|------|------|--------|------|
| `--kt-moe-lora-path` | str | None | 转换后的 MoE LoRA 文件路径 |
| `--kt-moe-lora-rank` | int | 16 | LoRA rank |
| `--kt-moe-lora-alpha` | float | 32.0 | LoRA alpha |
| `--kt-moe-sft-method` | str | "AMXBF16_SFT" | SFT 量化方法 |

### 4.3 性能需求

#### 4.3.1 加载时间

| 阶段 | 目标 |
|------|------|
| 转换脚本执行 | < 60s |
| 权重加载 | 与现有 KT 加载相当 |
| init_lora_weights | < 1s per layer |

#### 4.3.2 推理延迟

| 场景 | 预期影响 |
|------|----------|
| Decode (bs=1) | +10-20% 相比无 LoRA |
| Prefill | +5-10% 相比无 LoRA |

**注意**: SFT 模式使用同步执行，无法与 GPU 完全并行。

### 4.4 内存需求

| 组件 | 估算大小（DeepSeek-V2, rank=16） |
|------|----------------------------------|
| 单层 LoRA 权重 | ~50MB |
| 全部 27 层 | ~1.35GB |
| 运行时 buffer | ~100MB |

## 5. 兼容性需求

### 5.1 与现有功能兼容

| 功能 | 兼容性 |
|------|--------|
| 现有 attention/shared LoRA | 兼容，可同时使用 |
| KT CPU-GPU 混合推理 | 兼容 |
| Tensor Parallelism | 待验证 |
| CUDA Graph | 不兼容（SFT 模式） |

### 5.2 模型兼容性

| 模型 | 支持状态 |
|------|----------|
| DeepSeek-V2 | 主要目标 |
| DeepSeek-V3 | 预期支持 |
| 其他 MoE 模型 | 需要验证 |

## 6. 错误处理

### 6.1 转换阶段错误

| 错误类型 | 处理方式 |
|----------|----------|
| 无 MoE expert keys | 警告并跳过 |
| 权重 shape 不匹配 | 抛出异常 |
| 文件格式错误 | 抛出异常 |

### 6.2 运行时错误

| 错误类型 | 处理方式 |
|----------|----------|
| MoE LoRA 文件不存在 | 启动失败 |
| lora_rank 不匹配 | 启动失败 |
| kt-kernel 不支持 SFT | 启动失败 |

## 7. 验收标准

### 7.1 功能验收

1. 转换脚本成功转换 Kllama2 adapter
2. sglang 成功加载转换后的 MoE LoRA
3. 推理输出与预期一致

### 7.2 性能验收

1. 加载时间在可接受范围内
2. 推理延迟增加不超过预期

### 7.3 回归验收

1. 不影响现有无 LoRA 推理
2. 不影响现有 attention/shared LoRA 推理
