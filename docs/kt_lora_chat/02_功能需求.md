# MoE Expert LoRA 功能需求规格

## 1. 概述

在 sglang 中添加对 MoE routed experts LoRA 的支持，使用户能够在 CPU 上运行带有 LoRA 适配器的 MoE 专家计算。

## 2. 功能范围

### 2.1 支持范围

| 功能项 | 状态 | 说明 |
|--------|------|------|
| CPU routed experts LoRA | **新增** | 通过 kt-kernel SFT MoE 实现 |
| GPU shared experts LoRA | 已支持 | 现有 sglang LoRA 路径 |
| GPU attention LoRA | 已支持 | 现有 sglang LoRA 路径 |
| 静态加载 | **新增** | 启动时加载 adapter |
| 动态切换 | 不支持 | 不在本次范围 |
| 训练/微调 | 不支持 | 仅推理 |

### 2.2 不在范围内

- GPU routed experts LoRA（复杂度高，暂不实现）
- 多 adapter 动态切换
- 在线训练/梯度计算

## 3. 用户场景

### 3.1 典型使用场景

1. 用户有预训练的 DeepSeek-V2/V3 模型
2. 用户有 PEFT 格式的 LoRA adapter（Kllama2 格式，含 MoE expert LoRA）
3. 用户希望在推理时应用 MoE expert LoRA

### 3.2 预期工作流

```
1. 预处理阶段
   用户运行转换脚本：
   $ python scripts/convert_moe_lora.py \
       --input /path/to/adapter_model.safetensors \
       --output /path/to/moe_lora.pt

2. 推理阶段
   sglang 启动时指定 MoE LoRA：
   $ python -m sglang.launch_server \
       --model-path /path/to/deepseek-v2 \
       --kt-weight-path /path/to/weights \
       --kt-moe-lora-path /path/to/moe_lora.pt \
       --lora-paths /path/to/adapter  # 原有 attention/shared LoRA
```

## 4. 详细需求

### 4.1 Adapter 格式支持

#### 4.1.1 输入格式（PEFT）

支持两种 key 命名模式：

**模式 A（original_moe）**:
```
base_model.model.model.layers.{L}.mlp.original_moe.experts.{E}.{gate,up,down}_proj.lora_{A,B}.weight
```

**模式 B（直接 experts）**:
```
base_model.model.model.layers.{L}.mlp.experts.{E}.{gate,up,down}_proj.lora_{A,B}.weight
```

#### 4.1.2 输出格式（kt-kernel）

```python
{
    'metadata': {
        'lora_rank': 16,
        'lora_alpha': 32.0,
        'num_experts': 64,
        'num_layers': 27,
        'hidden_size': 7168,
        'intermediate_size': 2048,
    },
    'layer_0': {
        'gate_lora_a': Tensor[64, 16, 7168],
        'gate_lora_b': Tensor[64, 2048, 16],
        'up_lora_a': Tensor[64, 16, 7168],
        'up_lora_b': Tensor[64, 2048, 16],
        'down_lora_a': Tensor[64, 16, 2048],
        'down_lora_b': Tensor[64, 7168, 16],
    },
    'layer_1': { ... },
    ...
}
```

### 4.2 命令行参数

| 参数 | 类型 | 默认值 | 说明 |
|------|------|--------|------|
| `--kt-moe-lora-path` | str | None | 转换后的 MoE LoRA 文件路径 |
| `--kt-moe-lora-rank` | int | 16 | LoRA rank |
| `--kt-moe-lora-alpha` | float | 32.0 | LoRA alpha |
| `--kt-moe-sft-method` | str | "AMXBF16_SFT" | SFT 量化方法 |

### 4.3 性能需求

#### 4.3.1 加载时间

| 阶段 | 目标 |
|------|------|
| 转换脚本执行 | < 60s |
| 权重加载 | 与现有 KT 加载相当 |
| init_lora_weights | < 1s per layer |

#### 4.3.2 推理延迟

| 场景 | 预期影响 |
|------|----------|
| Decode (bs=1) | +10-20% 相比无 LoRA |
| Prefill | +5-10% 相比无 LoRA |

**注意**: SFT 模式使用同步执行，无法与 GPU 完全并行。

### 4.4 内存需求

| 组件 | 估算大小（DeepSeek-V2, rank=16） |
|------|----------------------------------|
| 单层 LoRA 权重 | ~50MB |
| 全部 27 层 | ~1.35GB |
| 运行时 buffer | ~100MB |

## 5. 兼容性需求

### 5.1 与现有功能兼容

| 功能 | 兼容性 |
|------|--------|
| 现有 attention/shared LoRA | 兼容，可同时使用 |
| KT CPU-GPU 混合推理 | 兼容 |
| Tensor Parallelism | 待验证 |
| CUDA Graph | 不兼容（SFT 模式） |

### 5.2 模型兼容性

| 模型 | 支持状态 |
|------|----------|
| DeepSeek-V2 | 主要目标 |
| DeepSeek-V3 | 预期支持 |
| 其他 MoE 模型 | 需要验证 |

## 6. 错误处理

### 6.1 转换阶段错误

| 错误类型 | 处理方式 |
|----------|----------|
| 无 MoE expert keys | 警告并跳过 |
| 权重 shape 不匹配 | 抛出异常 |
| 文件格式错误 | 抛出异常 |

### 6.2 运行时错误

| 错误类型 | 处理方式 |
|----------|----------|
| MoE LoRA 文件不存在 | 启动失败 |
| lora_rank 不匹配 | 启动失败 |
| kt-kernel 不支持 SFT | 启动失败 |

## 7. 验收标准

### 7.1 功能验收

1. 转换脚本成功转换 Kllama2 adapter
2. sglang 成功加载转换后的 MoE LoRA
3. 推理输出与预期一致

### 7.2 性能验收

1. 加载时间在可接受范围内
2. 推理延迟增加不超过预期

### 7.3 回归验收

1. 不影响现有无 LoRA 推理
2. 不影响现有 attention/shared LoRA 推理

## 8. SFT 权重加载需求

### 8.1 需求背景

SFT 模式的权重加载与 Inference 模式不同，需要支持从文件自动加载基础 MoE 权重。

### 8.2 功能需求

#### 8.2.1 BF16 权重加载

| 需求项 | 描述 | 优先级 |
|--------|------|--------|
| 自动格式检测 | 支持 DeepSeek 和 Mixtral 命名格式 | P0 |
| HuggingFace 路径加载 | 从 `--model-path` 加载 BF16 权重 | P0 |
| 无 Scale 处理 | BF16 格式不需要 scale 文件 | P0 |

#### 8.2.2 INT8/INT4 权重加载

| 需求项 | 描述 | 优先级 |
|--------|------|--------|
| 预量化权重加载 | 从 `--kt-weight-path` 加载 INT8/INT4 权重 | P0 |
| Scale 文件加载 | 同时加载对应的 scale 文件 | P0 |
| NUMA 分片支持 | 支持 NUMA 分片的权重格式 | P1 |

### 8.3 接口需求

#### 8.3.1 KTConfig 扩展

新增 `model_path` 字段用于存储 HuggingFace 模型路径：

```python
@dataclass
class KTConfig:
    # ... 现有字段 ...
    model_path: Optional[str] = None  # 新增：HuggingFace 模型路径
```

#### 8.3.2 权重路径选择逻辑

```python
# SFT 模式权重路径选择
if sft_method == "AMXBF16_SFT":
    weight_path = model_path      # HuggingFace BF16 权重
else:
    weight_path = kt_weight_path  # KT 预量化权重
```

### 8.4 加载器需求

#### 8.4.1 BF16SafeTensorLoader

| 需求项 | 描述 |
|--------|------|
| 继承 SafeTensorLoader | 复用基础功能 |
| 自动格式检测 | 检测 DeepSeek/Mixtral 格式 |
| 返回格式统一 | 返回 `{gate, up, down, gate_scale=None, ...}` |

#### 8.4.2 返回格式规范

```python
{
    "gate": List[torch.Tensor],      # [num_experts] 个 tensor
    "up": List[torch.Tensor],        # [num_experts] 个 tensor
    "down": List[torch.Tensor],      # [num_experts] 个 tensor
    "gate_scale": None,              # BF16 不需要 scale
    "up_scale": None,
    "down_scale": None,
}
```

### 8.5 非功能需求

#### 8.5.1 性能需求

| 指标 | 目标 |
|------|------|
| 单层权重加载时间 | < 5s |
| 内存峰值增加 | < 2x 权重大小 |

#### 8.5.2 兼容性需求

| 兼容项 | 状态 |
|--------|------|
| 现有 Inference 模式 | 不受影响 |
| 现有 SFT tensor 加载 | 仍然支持 |
| DeepSeek-V2/V3 模型 | 主要支持 |
| Mixtral 模型 | 预期支持 |
