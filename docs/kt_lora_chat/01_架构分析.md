# sglang MoE Expert LoRA 架构分析

## 1. 背景

本文档分析 sglang 和 kt-kernel 的现有架构，为添加 MoE routed experts LoRA 支持提供技术基础。

## 2. 现有 LoRA Adapter 格式分析

### 2.1 Kllama2 Adapter（含 MoE Expert LoRA）

**路径**: `/mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/`

**文件大小**: 305MB

**权重统计**:
| 类型 | 数量 | Key 模式 |
|------|------|----------|
| MoE Expert LoRA | 9984 keys | `layers.{L}.mlp.original_moe.experts.{E}.{gate,up,down}_proj.lora_{A,B}` |
| Shared Expert LoRA | 156 keys | `layers.{L}.mlp.shared_experts.{gate,up,down}_proj.lora_{A,B}` |
| Attention LoRA | 216 keys | `layers.{L}.self_attn.{q,kv_a,kv_b,o}_proj.lora_{A,B}` |

**MoE Expert LoRA Key 示例**:
```
base_model.model.model.layers.1.mlp.original_moe.experts.0.gate_proj.lora_A.weight
base_model.model.model.layers.1.mlp.original_moe.experts.0.gate_proj.lora_B.weight
base_model.model.model.layers.1.mlp.original_moe.experts.0.up_proj.lora_A.weight
base_model.model.model.layers.1.mlp.original_moe.experts.0.up_proj.lora_B.weight
base_model.model.model.layers.1.mlp.original_moe.experts.0.down_proj.lora_A.weight
base_model.model.model.layers.1.mlp.original_moe.experts.0.down_proj.lora_B.weight
... (重复 64 experts × 27 layers)
```

**LoRA 权重 Shape** (lora_rank=16):
- `gate_proj.lora_A`: [16, 7168] (hidden_size=7168)
- `gate_proj.lora_B`: [2048, 16] (intermediate_size=2048)
- `up_proj.lora_A`: [16, 7168]
- `up_proj.lora_B`: [2048, 16]
- `down_proj.lora_A`: [16, 2048]
- `down_proj.lora_B`: [7168, 16]

### 2.2 Kllama Adapter（不含 MoE Expert LoRA）

**路径**: `/mnt/data/lpl/kernel_new_test_adapter/Kllama_deepseekV2_WEST_ALL/checkpoint-133/`

**文件大小**: 27MB

**权重统计**:
| 类型 | 数量 |
|------|------|
| MoE Expert LoRA | 0 keys |
| Shared Expert LoRA | 156 keys |
| Attention LoRA | 216 keys |

### 2.3 格式差异总结

| 对比项 | Kllama2 | Kllama |
|--------|---------|--------|
| 大小 | 305MB | 27MB |
| MoE Expert LoRA | 有 (9984 keys) | 无 |
| Shared Expert LoRA | 有 | 有 |
| Attention LoRA | 有 | 有 |

## 3. sglang 现有 LoRA 架构

### 3.1 核心类

#### LoRAAdapter (`sglang/srt/lora/lora.py`)

负责加载和管理单个 LoRA adapter：

```python
class LoRAAdapter(nn.Module):
    def __init__(self, uid, config, base_hf_config, load_config, lora_backend):
        self.layers: List[LoRALayer] = nn.ModuleList([...])
        self.embedding_layers: Dict[str, torch.Tensor] = {}

    def initialize_weights(self):
        # 加载权重，进行 QKV/GateUp 合并
        for name, loaded_weight in loader._get_weights_iterator(...):
            layer_id = get_layer_id(name)
            if layer_id is not None:
                self.layers[layer_id].weights[name] = loaded_weight.cpu()
```

**关键逻辑**:
1. `normalize_qkv_proj()`: 合并 q/k/v → qkv_proj
2. `normalize_gate_up_proj()`: 合并 gate/up → gate_up_proj
3. **不处理 MoE expert LoRA keys** - 只识别 `self_attn.*` 和 `shared_experts.*`

### 3.2 权重加载流程

```
safetensors 文件
      |
      v
LoRAAdapter.initialize_weights()
      |
      +-- get_layer_id(name) 解析层号
      |
      +-- normalize_qkv_proj() 合并 QKV
      |
      +-- normalize_gate_up_proj() 合并 GateUp
      |
      v
LoRALayer.weights[name] = tensor
```

### 3.3 为什么现有代码忽略 MoE Expert LoRA

1. **FusedMoE 是单一模块**: 没有 per-expert 子模块
2. **权重合并逻辑**: 只处理 attention 和 shared_experts
3. **GPU 推理路径**: 不支持 per-expert LoRA 计算

## 4. kt-kernel SFT MoE 架构

### 4.1 类层次结构

```
KTMoEWrapper (工厂类，experts.py)
    |
    +-- mode="inference" --> BaseMoEWrapper --> AMXMoEWrapper
    |
    +-- mode="sft" --> BaseSFTMoEWrapper --> AMXSFTMoEWrapper
```

### 4.2 BaseSFTMoEWrapper (`experts_sft.py`)

```python
class BaseSFTMoEWrapper(_MoEBase, ABC):
    """SFT MoE 基类，支持 LoRA fine-tuning"""

    def __init__(self, ..., lora_rank=16, lora_alpha=32.0, max_cache_depth=1):
        # LoRA 权重占位符
        self.gate_lora_a: Optional[torch.Tensor] = None
        self.gate_lora_b: Optional[torch.Tensor] = None
        self.up_lora_a: Optional[torch.Tensor] = None
        self.up_lora_b: Optional[torch.Tensor] = None
        self.down_lora_a: Optional[torch.Tensor] = None
        self.down_lora_b: Optional[torch.Tensor] = None

    @abstractmethod
    def init_lora_weights(self, gate_lora_a, gate_lora_b, ...):
        """初始化 LoRA 权重"""
        pass

    @abstractmethod
    def forward_sft(self, hidden_states, expert_ids, weights, save_for_backward=True):
        """SFT 前向传播"""
        pass

    @abstractmethod
    def backward(self, grad_output):
        """反向传播"""
        pass
```

### 4.3 AMXSFTMoEWrapper (`utils/amx_sft.py`)

具体实现：

```python
class AMXSFTMoEWrapper(BaseSFTMoEWrapper):
    """AMX 加速的 SFT MoE 实现"""

    def init_lora_weights(self, gate_lora_a, gate_lora_b, ...):
        # 验证 shape
        expected_shapes = {
            "gate_lora_a": (num_experts, lora_rank, hidden_size),
            "gate_lora_b": (num_experts, intermediate_size, lora_rank),
            ...
        }
        # 存储权重（contiguous for C++ access）
        self.gate_lora_a = gate_lora_a.contiguous()
        ...

    def forward_sft(self, hidden_states, expert_ids, weights, save_for_backward=True):
        # 获取 buffer
        buffer = KExpertsSFTBuffer.get_buffer(...)
        # 拷贝输入
        buffer.input_cpu.copy_(hidden_states)
        buffer.expert_ids_cpu.copy_(expert_ids)
        buffer.weights_cpu.copy_(weights)
        # 执行
        self.cpu_infer.submit(self.moe.forward_sft_task(...))
        self.cpu_infer.sync()
        return buffer.output_cpu.clone()
```

### 4.4 LoRA 权重 Shape 规范

kt-kernel 要求的 LoRA 权重格式：

| 权重名 | Shape | 说明 |
|--------|-------|------|
| gate_lora_a | [expert_num, lora_rank, hidden_size] | gate 投影 A 矩阵 |
| gate_lora_b | [expert_num, intermediate_size, lora_rank] | gate 投影 B 矩阵 |
| up_lora_a | [expert_num, lora_rank, hidden_size] | up 投影 A 矩阵 |
| up_lora_b | [expert_num, intermediate_size, lora_rank] | up 投影 B 矩阵 |
| down_lora_a | [expert_num, lora_rank, intermediate_size] | down 投影 A 矩阵 |
| down_lora_b | [expert_num, hidden_size, lora_rank] | down 投影 B 矩阵 |

## 5. sglang KT Wrapper 架构

### 5.1 KTConfig (`kt_ep_wrapper.py:54-79`)

```python
@dataclass
class KTConfig:
    layer_idx: int
    num_gpu_experts: int
    cpuinfer_threads: int
    threadpool_count: int
    weight_path: str
    chunked_prefill_size: int
    max_deferred_experts_per_token: int
    method: str  # "AMXINT4", "AMXINT8" 等
```

### 5.2 KTEPWrapperMethod

核心 CPU-GPU 混合推理逻辑：

```python
class KTEPWrapperMethod(FusedMoEMethodBase):
    def create_weights(self, layer, num_experts, ...):
        # 1. 为 GPU experts 创建权重
        self.gpu_method.create_weights(...)

        # 2. 创建 KT wrapper (CPU experts)
        self.wrapper = KTMoEWrapper(
            ...,
            method=self.kt_config.method,  # 当前只支持 inference 模式
        )

    def apply(self, layer, dispatch_output):
        # 1. 提交 CPU 计算
        self.submit(layer, dispatch_output)

        # 2. 执行 GPU 计算
        gpu_combine_input = self.gpu_method.apply(...)

        # 3. 同步 CPU 结果并合并
        cpu_output = self.sync(x)
        output = gpu_output + cpu_output
```

### 5.3 异步执行模式

```
submit_forward()  ──>  CPU 开始计算 (非阻塞)
       |
       v
gpu_method.apply() ──>  GPU 计算 (并行)
       |
       v
sync_forward()    ──>  等待 CPU 完成，合并结果
```

## 6. 架构差距分析

### 6.1 当前不支持 MoE Expert LoRA 的原因

| 组件 | 问题 |
|------|------|
| LoRAAdapter | 只解析 attention/shared_experts keys |
| FusedMoE | 单一模块，无 per-expert LoRA 支持 |
| KTEPWrapperMethod | 只使用 inference 模式，不支持 LoRA |

### 6.2 集成方案

```
PEFT Adapter
    |
    v
convert_moe_lora.py (预转换)
    |
    v
kt-kernel 格式 .pt 文件
    |
    v
KTEPWrapperMethod (加载并初始化)
    |
    +-- mode="sft"
    +-- init_lora_weights()
    +-- forward_sft()
```

### 6.3 关键技术决策

1. **预转换 vs 运行时转换**: 选择预转换，避免启动延迟
2. **同步 vs 异步执行**: SFT 模式只支持同步，接受性能损失
3. **权重存储**: 独立 .pt 文件，与原 adapter 分离
