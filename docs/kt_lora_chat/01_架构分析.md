# sglang MoE Expert LoRA 架构分析

## 1. 背景

本文档分析 sglang 和 kt-kernel 的现有架构，为添加 MoE routed experts LoRA 支持提供技术基础。

## 2. 现有 LoRA Adapter 格式分析

### 2.1 Kllama2 Adapter（含 MoE Expert LoRA）

**路径**: `/mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/`

**文件大小**: 305MB

**权重统计**:
| 类型 | 数量 | Key 模式 |
|------|------|----------|
| MoE Expert LoRA | 9984 keys | `layers.{L}.mlp.original_moe.experts.{E}.{gate,up,down}_proj.lora_{A,B}` |
| Shared Expert LoRA | 156 keys | `layers.{L}.mlp.shared_experts.{gate,up,down}_proj.lora_{A,B}` |
| Attention LoRA | 216 keys | `layers.{L}.self_attn.{q,kv_a,kv_b,o}_proj.lora_{A,B}` |

**MoE Expert LoRA Key 示例**:
```
base_model.model.model.layers.1.mlp.original_moe.experts.0.gate_proj.lora_A.weight
base_model.model.model.layers.1.mlp.original_moe.experts.0.gate_proj.lora_B.weight
base_model.model.model.layers.1.mlp.original_moe.experts.0.up_proj.lora_A.weight
base_model.model.model.layers.1.mlp.original_moe.experts.0.up_proj.lora_B.weight
base_model.model.model.layers.1.mlp.original_moe.experts.0.down_proj.lora_A.weight
base_model.model.model.layers.1.mlp.original_moe.experts.0.down_proj.lora_B.weight
... (重复 64 experts × 27 layers)
```

**LoRA 权重 Shape** (lora_rank=16):
- `gate_proj.lora_A`: [16, 7168] (hidden_size=7168)
- `gate_proj.lora_B`: [2048, 16] (intermediate_size=2048)
- `up_proj.lora_A`: [16, 7168]
- `up_proj.lora_B`: [2048, 16]
- `down_proj.lora_A`: [16, 2048]
- `down_proj.lora_B`: [7168, 16]

### 2.2 Kllama Adapter（不含 MoE Expert LoRA）

**路径**: `/mnt/data/lpl/kernel_new_test_adapter/Kllama_deepseekV2_WEST_ALL/checkpoint-133/`

**文件大小**: 27MB

**权重统计**:
| 类型 | 数量 |
|------|------|
| MoE Expert LoRA | 0 keys |
| Shared Expert LoRA | 156 keys |
| Attention LoRA | 216 keys |

### 2.3 格式差异总结

| 对比项 | Kllama2 | Kllama |
|--------|---------|--------|
| 大小 | 305MB | 27MB |
| MoE Expert LoRA | 有 (9984 keys) | 无 |
| Shared Expert LoRA | 有 | 有 |
| Attention LoRA | 有 | 有 |

## 3. sglang 现有 LoRA 架构

### 3.1 核心类

#### LoRAAdapter (`sglang/srt/lora/lora.py`)

负责加载和管理单个 LoRA adapter：

```python
class LoRAAdapter(nn.Module):
    def __init__(self, uid, config, base_hf_config, load_config, lora_backend):
        self.layers: List[LoRALayer] = nn.ModuleList([...])
        self.embedding_layers: Dict[str, torch.Tensor] = {}

    def initialize_weights(self):
        # 加载权重，进行 QKV/GateUp 合并
        for name, loaded_weight in loader._get_weights_iterator(...):
            layer_id = get_layer_id(name)
            if layer_id is not None:
                self.layers[layer_id].weights[name] = loaded_weight.cpu()
```

**关键逻辑**:
1. `normalize_qkv_proj()`: 合并 q/k/v → qkv_proj
2. `normalize_gate_up_proj()`: 合并 gate/up → gate_up_proj
3. **不处理 MoE expert LoRA keys** - 只识别 `self_attn.*` 和 `shared_experts.*`

### 3.2 权重加载流程

```
safetensors 文件
      |
      v
LoRAAdapter.initialize_weights()
      |
      +-- get_layer_id(name) 解析层号
      |
      +-- normalize_qkv_proj() 合并 QKV
      |
      +-- normalize_gate_up_proj() 合并 GateUp
      |
      v
LoRALayer.weights[name] = tensor
```

### 3.3 为什么现有代码忽略 MoE Expert LoRA

1. **FusedMoE 是单一模块**: 没有 per-expert 子模块
2. **权重合并逻辑**: 只处理 attention 和 shared_experts
3. **GPU 推理路径**: 不支持 per-expert LoRA 计算

## 4. kt-kernel SFT MoE 架构

### 4.1 类层次结构

```
KTMoEWrapper (工厂类，experts.py)
    |
    +-- mode="inference" --> BaseMoEWrapper --> AMXMoEWrapper
    |
    +-- mode="sft" --> BaseSFTMoEWrapper --> AMXSFTMoEWrapper
```

### 4.2 BaseSFTMoEWrapper (`experts_sft.py`)

```python
class BaseSFTMoEWrapper(_MoEBase, ABC):
    """SFT MoE 基类，支持 LoRA fine-tuning"""

    def __init__(self, ..., lora_rank=16, lora_alpha=32.0, max_cache_depth=1):
        # LoRA 权重占位符
        self.gate_lora_a: Optional[torch.Tensor] = None
        self.gate_lora_b: Optional[torch.Tensor] = None
        self.up_lora_a: Optional[torch.Tensor] = None
        self.up_lora_b: Optional[torch.Tensor] = None
        self.down_lora_a: Optional[torch.Tensor] = None
        self.down_lora_b: Optional[torch.Tensor] = None

    @abstractmethod
    def init_lora_weights(self, gate_lora_a, gate_lora_b, ...):
        """初始化 LoRA 权重"""
        pass

    @abstractmethod
    def forward_sft(self, hidden_states, expert_ids, weights, save_for_backward=True):
        """SFT 前向传播"""
        pass

    @abstractmethod
    def backward(self, grad_output):
        """反向传播"""
        pass
```

### 4.3 AMXSFTMoEWrapper (`utils/amx_sft.py`)

具体实现：

```python
class AMXSFTMoEWrapper(BaseSFTMoEWrapper):
    """AMX 加速的 SFT MoE 实现"""

    def init_lora_weights(self, gate_lora_a, gate_lora_b, ...):
        # 验证 shape
        expected_shapes = {
            "gate_lora_a": (num_experts, lora_rank, hidden_size),
            "gate_lora_b": (num_experts, intermediate_size, lora_rank),
            ...
        }
        # 存储权重（contiguous for C++ access）
        self.gate_lora_a = gate_lora_a.contiguous()
        ...

    def forward_sft(self, hidden_states, expert_ids, weights, save_for_backward=True):
        # 获取 buffer
        buffer = KExpertsSFTBuffer.get_buffer(...)
        # 拷贝输入
        buffer.input_cpu.copy_(hidden_states)
        buffer.expert_ids_cpu.copy_(expert_ids)
        buffer.weights_cpu.copy_(weights)
        # 执行
        self.cpu_infer.submit(self.moe.forward_sft_task(...))
        self.cpu_infer.sync()
        return buffer.output_cpu.clone()
```

### 4.4 LoRA 权重 Shape 规范

kt-kernel 要求的 LoRA 权重格式：

| 权重名 | Shape | 说明 |
|--------|-------|------|
| gate_lora_a | [expert_num, lora_rank, hidden_size] | gate 投影 A 矩阵 |
| gate_lora_b | [expert_num, intermediate_size, lora_rank] | gate 投影 B 矩阵 |
| up_lora_a | [expert_num, lora_rank, hidden_size] | up 投影 A 矩阵 |
| up_lora_b | [expert_num, intermediate_size, lora_rank] | up 投影 B 矩阵 |
| down_lora_a | [expert_num, lora_rank, intermediate_size] | down 投影 A 矩阵 |
| down_lora_b | [expert_num, hidden_size, lora_rank] | down 投影 B 矩阵 |

## 5. sglang KT Wrapper 架构

### 5.1 KTConfig (`kt_ep_wrapper.py:54-79`)

```python
@dataclass
class KTConfig:
    layer_idx: int
    num_gpu_experts: int
    cpuinfer_threads: int
    threadpool_count: int
    weight_path: str
    chunked_prefill_size: int
    max_deferred_experts_per_token: int
    method: str  # "AMXINT4", "AMXINT8" 等
```

### 5.2 KTEPWrapperMethod

核心 CPU-GPU 混合推理逻辑：

```python
class KTEPWrapperMethod(FusedMoEMethodBase):
    def create_weights(self, layer, num_experts, ...):
        # 1. 为 GPU experts 创建权重
        self.gpu_method.create_weights(...)

        # 2. 创建 KT wrapper (CPU experts)
        self.wrapper = KTMoEWrapper(
            ...,
            method=self.kt_config.method,  # 当前只支持 inference 模式
        )

    def apply(self, layer, dispatch_output):
        # 1. 提交 CPU 计算
        self.submit(layer, dispatch_output)

        # 2. 执行 GPU 计算
        gpu_combine_input = self.gpu_method.apply(...)

        # 3. 同步 CPU 结果并合并
        cpu_output = self.sync(x)
        output = gpu_output + cpu_output
```

### 5.3 异步执行模式

```
submit_forward()  ──>  CPU 开始计算 (非阻塞)
       |
       v
gpu_method.apply() ──>  GPU 计算 (并行)
       |
       v
sync_forward()    ──>  等待 CPU 完成，合并结果
```

## 6. 架构差距分析

### 6.1 当前不支持 MoE Expert LoRA 的原因

| 组件 | 问题 |
|------|------|
| LoRAAdapter | 只解析 attention/shared_experts keys |
| FusedMoE | 单一模块，无 per-expert LoRA 支持 |
| KTEPWrapperMethod | 只使用 inference 模式，不支持 LoRA |

### 6.2 集成方案

```
PEFT Adapter
    |
    v
convert_moe_lora.py (预转换)
    |
    v
kt-kernel 格式 .pt 文件
    |
    v
KTEPWrapperMethod (加载并初始化)
    |
    +-- mode="sft"
    +-- init_lora_weights()
    +-- forward_sft()
```

### 6.3 关键技术决策

1. **预转换 vs 运行时转换**: 选择预转换，避免启动延迟
2. **同步 vs 异步执行**: SFT 模式只支持同步，接受性能损失
3. **权重存储**: 独立 .pt 文件，与原 adapter 分离

## 7. SFT 权重加载架构分析

### 7.1 问题背景

SFT 模式 (`AMXSFTMoEWrapper`) 的权重加载 API 与 Inference 模式 (`AMXMoEWrapper`) 不同：

| 模式 | 加载方法 | 说明 |
|------|----------|------|
| Inference | `load_weights(map)` | 直接从 `kt_weight_path` 文件加载 |
| SFT（原有） | `load_weights_from_tensors()` | 需要先传入 BF16 权重张量 |

### 7.2 关键发现：MOESFTConfig 继承关系

`MOESFTConfig` 继承自 `GeneralMOEConfig`，因此 SFT 模式也可以使用与 Inference 模式相同的文件加载机制：

```cpp
// kt-kernel/operators/common.hpp
struct MOESFTConfig : public GeneralMOEConfig {
    int lora_rank = 16;
    float lora_alpha = 32.0f;
    // LoRA weight pointers...
};

struct GeneralMOEConfig {
    std::vector<std::vector<void*>> gate_projs;
    std::vector<std::vector<void*>> gate_scales;
    std::string path;
    bool save = false;
    bool load = false;
    // ...
};
```

### 7.3 权重加载策略分析

根据 SFT 方法类型，选择不同的权重加载策略：

| SFT Method | 权重来源 | 加载器类 | 是否需要 Scale |
|------------|----------|----------|----------------|
| `AMXBF16_SFT` | HuggingFace 模型路径 (`--model-path`) | `BF16SafeTensorLoader` | 否 |
| `AMXINT8_SFT` | KT 量化权重路径 (`--kt-weight-path`) | `SafeTensorLoader` | 是 |
| `AMXINT4_SFT` | KT 量化权重路径 (`--kt-weight-path`) | `SafeTensorLoader` | 是 |

### 7.4 BF16SafeTensorLoader 设计

新增 `BF16SafeTensorLoader` 类用于加载 HuggingFace 格式的 BF16 权重：

```python
# kt-kernel/python/utils/loader.py
class BF16SafeTensorLoader(SafeTensorLoader):
    """Loader for native BF16 expert weights (no quantization, no scales).

    Supported formats:
    - DeepSeek style: {base}.mlp.experts.{id}.{gate,up,down}_proj.weight
    - Mixtral/MiniMax style: {base}.block_sparse_moe.experts.{id}.{w1,w3,w2}.weight
    """

    MOE_FORMATS = {
        "deepseek": ("{base}.mlp.experts", "gate_proj", "up_proj", "down_proj"),
        "mixtral": ("{base}.block_sparse_moe.experts", "w1", "w3", "w2"),
    }

    def load_experts(self, base_key: str, device: str = "cpu"):
        """Load BF16 expert weights (no scales needed).

        Returns:
            Dictionary with keys: gate, up, down, gate_scale (None), up_scale (None), down_scale (None)
        """
```

### 7.5 权重文件命名规范

不同加载器期望的 key 命名格式：

| 加载器 | Base Key 格式 | Expert Key 格式 |
|--------|---------------|-----------------|
| `BF16SafeTensorLoader` | `model.layers.{L}` | `model.layers.{L}.mlp.experts.{E}.gate_proj.weight` |
| `SafeTensorLoader` | `blk.{L}` | `blk.{L}.ffn_gate_exps.{E}.numa.{N}.weight` |

### 7.6 数据流图

```
                     ┌─────────────────────────┐
                     │  KTEPWrapperMethod      │
                     │  create_weights()       │
                     └───────────┬─────────────┘
                                 │
                   ┌─────────────┴─────────────┐
                   │                           │
           AMXBF16_SFT               AMXINT8/INT4_SFT
                   │                           │
         ┌─────────▼─────────┐       ┌─────────▼─────────┐
         │ model_path        │       │ weight_path       │
         │ (HuggingFace)     │       │ (KT pre-quant)    │
         └─────────┬─────────┘       └─────────┬─────────┘
                   │                           │
                   ▼                           ▼
       ┌───────────────────┐       ┌───────────────────┐
       │ BF16SafeTensor    │       │ SafeTensor        │
       │ Loader            │       │ Loader            │
       └─────────┬─────────┘       └─────────┬─────────┘
                 │                           │
                 └───────────┬───────────────┘
                             │
                             ▼
                 ┌───────────────────────┐
                 │ AMXSFTMoEWrapper      │
                 │ load_weights()        │
                 │ _load_base_weights    │
                 │ _from_file()          │
                 └───────────────────────┘
```
