# MoE Expert LoRA 功能具体实现

## 1. 转换脚本实现

### 1.1 文件位置

`/home/lpl/sglang-debug/sglang/scripts/convert_moe_lora.py`

### 1.2 完整实现

```python
#!/usr/bin/env python3
"""
将 PEFT 格式的 MoE LoRA adapter 转换为 kt-kernel 友好格式。

Usage:
    python scripts/convert_moe_lora.py \
        --input /path/to/adapter_model.safetensors \
        --config /path/to/adapter_config.json \
        --output /path/to/moe_lora.pt
"""

import argparse
import json
import logging
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import torch
from safetensors import safe_open

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# MoE expert LoRA key pattern
# Matches: layers.{L}.mlp.original_moe.experts.{E}.{proj}.lora_{type}.weight
# Or:      layers.{L}.mlp.experts.{E}.{proj}.lora_{type}.weight
MOE_PATTERN = re.compile(
    r".*layers\.(\d+)\.mlp\.(original_moe\.)?experts\.(\d+)\."
    r"(gate|up|down)_proj\.lora_(A|B)\.weight"
)


def parse_moe_key(key: str) -> Optional[Tuple[int, int, str, str]]:
    """
    解析 MoE expert LoRA key。

    Returns:
        (layer_idx, expert_id, proj_type, lora_type) or None
    """
    match = MOE_PATTERN.match(key)
    if match:
        layer_idx = int(match.group(1))
        expert_id = int(match.group(3))
        proj_type = match.group(4)  # gate, up, down
        lora_type = match.group(5).lower()  # a, b
        return (layer_idx, expert_id, proj_type, lora_type)
    return None


def load_adapter_config(config_path: str) -> dict:
    """加载 adapter_config.json"""
    with open(config_path, "r") as f:
        return json.load(f)


def convert_peft_to_kt_format(
    input_path: str,
    output_path: str,
    lora_alpha: float = 32.0,
    verbose: bool = False,
) -> dict:
    """
    转换 PEFT 格式的 MoE LoRA adapter 为 kt-kernel 格式。

    Args:
        input_path: PEFT adapter_model.safetensors 路径
        output_path: 输出 .pt 文件路径
        lora_alpha: LoRA alpha 值
        verbose: 是否输出详细信息

    Returns:
        转换统计信息
    """
    logger.info(f"Loading adapter from: {input_path}")

    # 1. 读取 safetensors 文件
    with safe_open(input_path, framework="pt") as f:
        all_keys = list(f.keys())

    # 2. 扫描 MoE expert LoRA keys
    moe_weights: Dict[int, Dict[int, Dict[str, Dict[str, torch.Tensor]]]] = {}
    # Structure: moe_weights[layer_idx][expert_id][proj_type][lora_type] = tensor

    moe_key_count = 0
    lora_rank = None

    with safe_open(input_path, framework="pt") as f:
        for key in all_keys:
            parsed = parse_moe_key(key)
            if parsed is None:
                continue

            layer_idx, expert_id, proj_type, lora_type = parsed
            tensor = f.get_tensor(key)

            # 初始化嵌套字典
            if layer_idx not in moe_weights:
                moe_weights[layer_idx] = {}
            if expert_id not in moe_weights[layer_idx]:
                moe_weights[layer_idx][expert_id] = {}
            if proj_type not in moe_weights[layer_idx][expert_id]:
                moe_weights[layer_idx][expert_id][proj_type] = {}

            moe_weights[layer_idx][expert_id][proj_type][lora_type] = tensor
            moe_key_count += 1

            # 推断 lora_rank
            if lora_type == "a" and lora_rank is None:
                lora_rank = tensor.shape[0]

            if verbose:
                logger.debug(f"  {key}: {tensor.shape}")

    logger.info(f"Found {moe_key_count} MoE expert LoRA keys")

    if moe_key_count == 0:
        logger.warning("No MoE expert LoRA keys found. Nothing to convert.")
        return {"status": "skipped", "reason": "no_moe_keys"}

    # 3. 堆叠权重
    result = {
        "metadata": {
            "lora_rank": lora_rank,
            "lora_alpha": lora_alpha,
        }
    }

    layer_indices = sorted(moe_weights.keys())
    num_layers = len(layer_indices)
    result["metadata"]["num_layers"] = num_layers

    for layer_idx in layer_indices:
        layer_data = moe_weights[layer_idx]
        expert_ids = sorted(layer_data.keys())
        num_experts = len(expert_ids)

        if layer_idx == layer_indices[0]:
            result["metadata"]["num_experts"] = num_experts
            # 推断 hidden_size 和 intermediate_size
            sample_expert = layer_data[expert_ids[0]]
            if "gate" in sample_expert and "a" in sample_expert["gate"]:
                result["metadata"]["hidden_size"] = sample_expert["gate"]["a"].shape[1]
            if "gate" in sample_expert and "b" in sample_expert["gate"]:
                result["metadata"]["intermediate_size"] = sample_expert["gate"]["b"].shape[0]

        layer_weights = {}

        for proj_type in ["gate", "up", "down"]:
            for lora_type in ["a", "b"]:
                weight_name = f"{proj_type}_lora_{lora_type}"
                tensors = []

                for expert_id in expert_ids:
                    if (
                        expert_id in layer_data
                        and proj_type in layer_data[expert_id]
                        and lora_type in layer_data[expert_id][proj_type]
                    ):
                        tensors.append(layer_data[expert_id][proj_type][lora_type])
                    else:
                        raise ValueError(
                            f"Missing weight: layer={layer_idx}, expert={expert_id}, "
                            f"proj={proj_type}, lora={lora_type}"
                        )

                # 堆叠为 [num_experts, ...]
                stacked = torch.stack(tensors, dim=0)
                layer_weights[weight_name] = stacked

                if verbose and layer_idx == layer_indices[0]:
                    logger.info(f"  {weight_name}: {stacked.shape}")

        result[f"layer_{layer_idx}"] = layer_weights

    # 4. 保存
    logger.info(f"Saving to: {output_path}")
    torch.save(result, output_path)

    # 5. 输出统计
    stats = {
        "status": "success",
        "num_layers": num_layers,
        "num_experts": result["metadata"]["num_experts"],
        "lora_rank": lora_rank,
        "lora_alpha": lora_alpha,
        "total_keys": moe_key_count,
    }
    logger.info(f"Conversion complete: {stats}")

    return stats


def main():
    parser = argparse.ArgumentParser(
        description="Convert PEFT MoE LoRA adapter to kt-kernel format"
    )
    parser.add_argument(
        "--input",
        type=str,
        required=True,
        help="Path to adapter_model.safetensors",
    )
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="Path to adapter_config.json (optional, for lora_alpha)",
    )
    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="Output path for .pt file",
    )
    parser.add_argument(
        "--lora-alpha",
        type=float,
        default=32.0,
        help="LoRA alpha value (default: 32.0)",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Enable verbose output",
    )

    args = parser.parse_args()

    # 从 config 读取 lora_alpha
    lora_alpha = args.lora_alpha
    if args.config:
        config = load_adapter_config(args.config)
        lora_alpha = config.get("lora_alpha", lora_alpha)
        logger.info(f"Using lora_alpha from config: {lora_alpha}")

    convert_peft_to_kt_format(
        input_path=args.input,
        output_path=args.output,
        lora_alpha=lora_alpha,
        verbose=args.verbose,
    )


if __name__ == "__main__":
    main()
```

## 2. kt_ep_wrapper.py 修改

### 2.1 KTConfig 扩展

**位置**: 第 54-79 行

```python
@dataclass
class KTConfig:
    """Configuration for KTransformers heterogeneous computing CPU part."""

    layer_idx: int
    num_gpu_experts: int
    cpuinfer_threads: int
    threadpool_count: int
    weight_path: str
    chunked_prefill_size: int
    max_deferred_experts_per_token: int
    method: str
    num_layers: Optional[int] = None
    gpu_prefill_token_threshold: Optional[int] = None

    # === 新增：MoE LoRA 配置 ===
    moe_lora_enabled: bool = False
    moe_lora_path: Optional[str] = None
    lora_rank: int = 16
    lora_alpha: float = 32.0
    sft_method: str = "AMXBF16_SFT"
```

### 2.2 KTEPWrapperMethod.create_weights() 修改

**位置**: 第 1141-1155 行

```python
def create_weights(
    self,
    layer: torch.nn.Module,
    num_experts: int,
    hidden_size: int,
    intermediate_size_per_partition: int,
    params_dtype: torch.dtype,
    **extra_weight_attrs,
):
    # ... 现有代码 ...

    # 2. Initialize KT wrapper for CPU experts
    if self.tp_rank == 0:
        if self.kt_config.moe_lora_enabled:
            # SFT 模式（带 LoRA）
            self.wrapper = KTMoEWrapper(
                layer_idx=self.kt_config.layer_idx,
                num_experts=num_experts,
                num_experts_per_tok=num_experts_per_tok,
                hidden_size=hidden_size,
                moe_intermediate_size=intermediate_size_full,
                num_gpu_experts=self.num_gpu_experts,
                cpuinfer_threads=self.kt_config.cpuinfer_threads,
                threadpool_count=self.kt_config.threadpool_count,
                weight_path=self.kt_config.weight_path,
                chunked_prefill_size=self.kt_config.chunked_prefill_size,
                # SFT 特有参数
                mode="sft",
                method=self.kt_config.sft_method,
                lora_rank=self.kt_config.lora_rank,
                lora_alpha=self.kt_config.lora_alpha,
                max_cache_depth=1,  # 推理只需要 1
            )
        else:
            # Inference 模式（无 LoRA）
            self.wrapper = KTMoEWrapper(
                layer_idx=self.kt_config.layer_idx,
                num_experts=num_experts,
                num_experts_per_tok=num_experts_per_tok,
                hidden_size=hidden_size,
                moe_intermediate_size=intermediate_size_full,
                num_gpu_experts=self.num_gpu_experts,
                cpuinfer_threads=self.kt_config.cpuinfer_threads,
                threadpool_count=self.kt_config.threadpool_count,
                weight_path=self.kt_config.weight_path,
                chunked_prefill_size=self.kt_config.chunked_prefill_size,
                method=self.kt_config.method,
                max_deferred_experts_per_token=layer_max_deferred,
            )
```

### 2.3 process_weights_after_loading() 修改

**位置**: 第 1157-1181 行

```python
def process_weights_after_loading(self, layer: torch.nn.Module) -> None:
    # 1. Process GPU weights
    if hasattr(self.gpu_method, "process_weights_after_loading"):
        self.gpu_method.process_weights_after_loading(layer)

    # 2. Load CPU weights using KT wrapper
    if self.tp_rank == 0 and self.wrapper is not None:
        torch.cuda.synchronize()

        from sglang.srt.eplb.expert_location_dispatch import (
            get_global_expert_location_metadata,
        )

        physical_to_logical_map_cpu = (
            get_global_expert_location_metadata()
            .physical_to_logical_map_cpu[self.kt_config.layer_idx]
            .contiguous()
        )
        self.wrapper.load_weights(physical_to_logical_map_cpu)

        # === 新增：加载 MoE LoRA 权重 ===
        if self.kt_config.moe_lora_enabled:
            self._load_moe_lora_weights()

def _load_moe_lora_weights(self):
    """加载并初始化 MoE LoRA 权重"""
    import os

    lora_path = self.kt_config.moe_lora_path
    layer_idx = self.kt_config.layer_idx

    if not os.path.exists(lora_path):
        raise RuntimeError(f"MoE LoRA file not found: {lora_path}")

    logger.info(f"Loading MoE LoRA weights for layer {layer_idx} from {lora_path}")

    lora_weights = torch.load(lora_path, map_location="cpu")
    layer_key = f"layer_{layer_idx}"

    if layer_key not in lora_weights:
        raise RuntimeError(
            f"Layer {layer_idx} not found in MoE LoRA file. "
            f"Available layers: {[k for k in lora_weights.keys() if k.startswith('layer_')]}"
        )

    layer_lora = lora_weights[layer_key]

    # 初始化 LoRA 权重
    self.wrapper.init_lora_weights(
        gate_lora_a=layer_lora["gate_lora_a"],
        gate_lora_b=layer_lora["gate_lora_b"],
        up_lora_a=layer_lora["up_lora_a"],
        up_lora_b=layer_lora["up_lora_b"],
        down_lora_a=layer_lora["down_lora_a"],
        down_lora_b=layer_lora["down_lora_b"],
    )

    logger.info(f"MoE LoRA weights initialized for layer {layer_idx}")
```

### 2.4 submit() 和 sync() 修改

**位置**: 第 1210-1257 行

```python
def submit(
    self,
    layer: torch.nn.Module,
    dispatch_output: "StandardDispatchOutput",
) -> None:
    """Submit CPU expert computation asynchronously (non-blocking)."""
    if self.tp_rank != 0 or self.wrapper is None:
        return

    # SFT 模式不支持异步，跳过 submit
    if self.kt_config.moe_lora_enabled:
        # 保存 dispatch_output 供 sync() 使用
        self._cached_dispatch_output = dispatch_output
        return

    # 原有异步提交逻辑
    x = dispatch_output.hidden_states
    topk_output = dispatch_output.topk_output
    topk_weights, topk_ids, _ = topk_output

    self.wrapper.submit_forward(
        x, topk_ids, topk_weights, torch.cuda.current_stream(x.device).cuda_stream
    )

def sync(self, x: torch.Tensor) -> torch.Tensor:
    """Synchronize and retrieve CPU expert computation results."""
    if self.tp_rank != 0 or self.wrapper is None:
        return torch.zeros_like(x)

    # SFT 模式：同步执行 forward_sft
    if self.kt_config.moe_lora_enabled:
        dispatch_output = self._cached_dispatch_output
        output = self.wrapper.forward_sft(
            dispatch_output.hidden_states,
            dispatch_output.topk_output.topk_ids,
            dispatch_output.topk_output.topk_weights,
            save_for_backward=False,  # 推理不需要梯度
        )
        self._cached_dispatch_output = None
        return output

    # 原有异步同步逻辑
    return self.wrapper.sync_forward(
        x, torch.cuda.current_stream(x.device).cuda_stream
    )
```

## 3. create_kt_config_from_server_args() 修改

**位置**: 第 987-1017 行

```python
def create_kt_config_from_server_args(
    server_args: "ServerArgs", layer_idx: int
) -> Optional[KTConfig]:
    """Create KTConfig from ServerArgs if KT is configured."""
    if server_args.kt_weight_path is None:
        return None

    hf_config = server_args.get_hf_config()
    num_layers = getattr(hf_config, "num_hidden_layers", None)

    # 检测是否启用 MoE LoRA
    moe_lora_enabled = (
        hasattr(server_args, "kt_moe_lora_path")
        and server_args.kt_moe_lora_path is not None
    )

    return KTConfig(
        layer_idx=layer_idx,
        num_gpu_experts=server_args.kt_num_gpu_experts,
        cpuinfer_threads=server_args.kt_cpuinfer,
        threadpool_count=server_args.kt_threadpool_count,
        weight_path=server_args.kt_weight_path,
        chunked_prefill_size=server_args.chunked_prefill_size,
        method=server_args.kt_method,
        max_deferred_experts_per_token=server_args.kt_max_deferred_experts_per_token,
        num_layers=num_layers,
        gpu_prefill_token_threshold=server_args.kt_gpu_prefill_token_threshold,
        # MoE LoRA 配置
        moe_lora_enabled=moe_lora_enabled,
        moe_lora_path=getattr(server_args, "kt_moe_lora_path", None),
        lora_rank=getattr(server_args, "kt_moe_lora_rank", 16),
        lora_alpha=getattr(server_args, "kt_moe_lora_alpha", 32.0),
        sft_method=getattr(server_args, "kt_moe_sft_method", "AMXBF16_SFT"),
    )
```

## 4. ServerArgs 修改（可选）

**位置**: `sglang/srt/server_args.py`

```python
@dataclass
class ServerArgs:
    # ... 现有参数 ...

    # === 新增：MoE LoRA 参数 ===
    kt_moe_lora_path: Optional[str] = None
    kt_moe_lora_rank: int = 16
    kt_moe_lora_alpha: float = 32.0
    kt_moe_sft_method: str = "AMXBF16_SFT"

    # 在 add_cli_args() 中添加
    @staticmethod
    def add_cli_args(parser: argparse.ArgumentParser):
        # ... 现有参数 ...

        parser.add_argument(
            "--kt-moe-lora-path",
            type=str,
            default=None,
            help="Path to converted MoE LoRA weights (.pt file)",
        )
        parser.add_argument(
            "--kt-moe-lora-rank",
            type=int,
            default=16,
            help="LoRA rank for MoE experts",
        )
        parser.add_argument(
            "--kt-moe-lora-alpha",
            type=float,
            default=32.0,
            help="LoRA alpha for MoE experts",
        )
        parser.add_argument(
            "--kt-moe-sft-method",
            type=str,
            default="AMXBF16_SFT",
            choices=["AMXBF16_SFT", "AMXINT8_SFT", "AMXINT4_SFT"],
            help="SFT quantization method",
        )
```

## 5. 实现检查清单

- [x] 创建 `scripts/convert_moe_lora.py`
- [x] 修改 `kt_ep_wrapper.py`:
  - [x] 扩展 `KTConfig`（含 `model_path` 字段）
  - [x] 修改 `create_weights()`（权重路径选择逻辑）
  - [x] 修改 `process_weights_after_loading()`
  - [x] 添加 `_load_moe_lora_weights()`
  - [x] 修改 `submit()` 和 `sync()`
  - [x] 修改 `create_kt_config_from_server_args()`
- [x] 修改 `server_args.py`
- [x] 实现 SFT 权重加载:
  - [x] 新增 `BF16SafeTensorLoader` 类
  - [x] 修改 `AMXSFTMoEWrapper.load_weights()`
  - [x] 添加 `_load_base_weights_from_file()` 方法

## 6. SFT 权重加载实现

### 6.1 BF16SafeTensorLoader 实现

**文件**: `kt-kernel/python/utils/loader.py`

```python
class BF16SafeTensorLoader(SafeTensorLoader):
    """Loader for native BF16 expert weights (no quantization, no scales).

    This loader handles HuggingFace format expert weights in BF16/FP16 precision.
    Unlike SafeTensorLoader which loads pre-quantized weights with scales,
    this loader returns raw weights without any scale tensors.

    Supported formats:
    - DeepSeek style: {base}.mlp.experts.{id}.{gate,up,down}_proj.weight
    - Mixtral/MiniMax style: {base}.block_sparse_moe.experts.{id}.{w1,w3,w2}.weight
    """

    MOE_FORMATS = {
        "deepseek": ("{base}.mlp.experts", "gate_proj", "up_proj", "down_proj"),
        "mixtral": ("{base}.block_sparse_moe.experts", "w1", "w3", "w2"),
    }

    def __init__(self, file_path: str):
        """Initialize loader with path to HuggingFace model directory or file."""
        super().__init__(file_path)
        self._detected_format = None
        self._detect_format()

    def _detect_format(self):
        """Auto-detect MoE naming format from available keys."""
        all_keys = self._get_all_keys()

        for format_name, (pattern, _, _, _) in self.MOE_FORMATS.items():
            # Check if any key matches this format pattern
            test_pattern = pattern.replace("{base}", "model.layers.0")
            if any(test_pattern in key for key in all_keys):
                self._detected_format = format_name
                return

        # Default to deepseek format
        self._detected_format = "deepseek"

    def _get_all_keys(self) -> list:
        """Get all keys from all safetensor files."""
        keys = []
        for handle in self._handles.values():
            keys.extend(handle.keys())
        return keys

    def load_experts(self, base_key: str, device: str = "cpu"):
        """Load BF16 expert weights for a layer (no scales needed).

        Args:
            base_key: Base key for the layer (e.g., "model.layers.0")
            device: Device to load tensors to (default: "cpu")

        Returns:
            Dictionary with keys:
            - gate: List of gate projection tensors per expert
            - up: List of up projection tensors per expert
            - down: List of down projection tensors per expert
            - gate_scale: None (BF16 doesn't need scales)
            - up_scale: None
            - down_scale: None
        """
        pattern, gate_name, up_name, down_name = self.MOE_FORMATS[self._detected_format]
        expert_base = pattern.format(base=base_key)

        gate_weights = []
        up_weights = []
        down_weights = []

        # Discover number of experts by scanning keys
        all_keys = self._get_all_keys()
        expert_ids = set()
        for key in all_keys:
            if expert_base in key:
                # Extract expert ID from key
                match = re.search(rf"{re.escape(expert_base)}\.(\d+)\.", key)
                if match:
                    expert_ids.add(int(match.group(1)))

        if not expert_ids:
            raise ValueError(f"No experts found for base key: {base_key}")

        # Load weights for each expert in order
        for expert_id in sorted(expert_ids):
            gate_key = f"{expert_base}.{expert_id}.{gate_name}.weight"
            up_key = f"{expert_base}.{expert_id}.{up_name}.weight"
            down_key = f"{expert_base}.{expert_id}.{down_name}.weight"

            gate_weights.append(self._load_tensor(gate_key, device))
            up_weights.append(self._load_tensor(up_key, device))
            down_weights.append(self._load_tensor(down_key, device))

        return {
            "gate": gate_weights,
            "up": up_weights,
            "down": down_weights,
            "gate_scale": None,
            "up_scale": None,
            "down_scale": None,
        }

    def _load_tensor(self, key: str, device: str) -> torch.Tensor:
        """Load a single tensor by key."""
        for handle in self._handles.values():
            if key in handle.keys():
                return handle.get_tensor(key).to(device)
        raise KeyError(f"Tensor not found: {key}")
```

### 6.2 KTConfig 扩展（含 model_path）

**文件**: `sglang/srt/layers/moe/kt_ep_wrapper.py`

```python
@dataclass
class KTConfig:
    """Configuration for KTransformers heterogeneous computing CPU part."""

    layer_idx: int
    num_gpu_experts: int
    cpuinfer_threads: int
    threadpool_count: int
    weight_path: str
    chunked_prefill_size: int
    max_deferred_experts_per_token: int
    method: str
    num_layers: Optional[int] = None
    gpu_prefill_token_threshold: Optional[int] = None

    # MoE LoRA 配置
    moe_lora_enabled: bool = False
    moe_lora_path: Optional[str] = None
    lora_rank: int = 16
    lora_alpha: float = 32.0
    sft_method: str = "AMXBF16_SFT"

    # 新增：HuggingFace 模型路径（用于 AMXBF16_SFT）
    model_path: Optional[str] = None
```

### 6.3 create_weights() 权重路径选择

**文件**: `sglang/srt/layers/moe/kt_ep_wrapper.py`

```python
# KTEPWrapperMethod.create_weights() 中
if self.tp_rank == 0:
    if self.kt_config.moe_lora_enabled:
        # SFT mode with MoE LoRA support
        # Determine weight path based on SFT method:
        # - AMXBF16_SFT: use model_path (HuggingFace BF16 weights)
        # - AMXINT8_SFT/AMXINT4_SFT: use weight_path (pre-quantized weights)
        if self.kt_config.sft_method == "AMXBF16_SFT":
            sft_weight_path = self.kt_config.model_path
        else:
            sft_weight_path = self.kt_config.weight_path

        self.wrapper = KTMoEWrapper(
            layer_idx=self.kt_config.layer_idx,
            # ... 其他参数 ...
            weight_path=sft_weight_path,
            mode="sft",
            method=self.kt_config.sft_method,
            lora_rank=self.kt_config.lora_rank,
            lora_alpha=self.kt_config.lora_alpha,
            max_cache_depth=1,
        )
```

### 6.4 AMXSFTMoEWrapper.load_weights() 修改

**文件**: `kt-kernel/python/utils/amx_sft.py`

```python
def load_weights(self, physical_to_logical_map_cpu: torch.Tensor) -> None:
    """
    Load base weights for this layer.

    Supports two loading modes:
    1. From tensors: Call load_weights_from_tensors() first, then load_weights()
    2. From files: Automatically load from weight_path if base weights not set
       - AMXBF16_SFT: Uses BF16SafeTensorLoader (HuggingFace format)
       - AMXINT8_SFT/AMXINT4_SFT: Uses SafeTensorLoader (pre-quantized format)

    Args:
        physical_to_logical_map_cpu: Mapping from physical to logical expert IDs
    """
    if self._weights_loaded:
        return

    # If base weights not set, try to load from file
    if self.gate_proj is None or self.up_proj is None or self.down_proj is None:
        self._load_base_weights_from_file()

    # 继续原有逻辑：创建 MOESFTConfig，初始化 C++ 后端
    config = MOESFTConfig()
    # ...
```

### 6.5 _load_base_weights_from_file() 实现

**文件**: `kt-kernel/python/utils/amx_sft.py`

```python
def _load_base_weights_from_file(self) -> None:
    """
    Load base MoE weights from file based on the SFT method.

    Loading strategy:
    - AMXBF16_SFT: Use BF16SafeTensorLoader (HuggingFace format, no scales)
    - AMXINT8_SFT/AMXINT4_SFT: Use SafeTensorLoader (pre-quantized format with scales)
    """
    if not hasattr(self, "weight_path") or self.weight_path is None:
        raise RuntimeError(
            "weight_path not set. Cannot load weights from file. "
            "Either set weight_path or call load_weights_from_tensors() instead."
        )

    print(f"[AMXSFTMoEWrapper] Loading base weights for layer {self.layer_idx} "
          f"from {self.weight_path} using method {self.method}")

    # Determine loader and base key format based on method
    if self.method == "AMXBF16_SFT":
        # BF16 mode: Load from HuggingFace model path
        loader = BF16SafeTensorLoader(self.weight_path)
        base_key = f"model.layers.{self.layer_idx}"
    else:
        # INT8/INT4 mode: Load from pre-quantized path
        loader = SafeTensorLoader(self.weight_path)
        base_key = f"blk.{self.layer_idx}"

    # Load expert weights
    experts_data = loader.load_experts(base_key, device="cpu")

    # Stack expert weights: [num_experts, ...]
    if self.method == "AMXBF16_SFT":
        # BF16SafeTensorLoader returns list of tensors
        self.gate_proj = torch.stack(experts_data["gate"], dim=0).contiguous()
        self.up_proj = torch.stack(experts_data["up"], dim=0).contiguous()
        self.down_proj = torch.stack(experts_data["down"], dim=0).contiguous()
    else:
        # SafeTensorLoader returns nested lists [numa_id][expert_id] -> numpy array
        import numpy as np
        numa_id = 0
        self.gate_proj = torch.from_numpy(
            np.stack(experts_data["gate"][numa_id], axis=0)
        ).contiguous()
        self.up_proj = torch.from_numpy(
            np.stack(experts_data["up"][numa_id], axis=0)
        ).contiguous()
        self.down_proj = torch.from_numpy(
            np.stack(experts_data["down"][numa_id], axis=0)
        ).contiguous()

        # Also store scales for INT8/INT4 methods
        self.gate_scale = torch.from_numpy(
            np.stack(experts_data["gate_scale"][numa_id], axis=0)
        ).contiguous()
        self.up_scale = torch.from_numpy(
            np.stack(experts_data["up_scale"][numa_id], axis=0)
        ).contiguous()
        self.down_scale = torch.from_numpy(
            np.stack(experts_data["down_scale"][numa_id], axis=0)
        ).contiguous()

    # Close loader handles
    loader.close_all_handles()

    print(f"[AMXSFTMoEWrapper] Loaded weights: gate_proj={self.gate_proj.shape}, "
          f"up_proj={self.up_proj.shape}, down_proj={self.down_proj.shape}")
```

### 6.6 create_kt_config_from_server_args() 更新

**文件**: `sglang/srt/layers/moe/kt_ep_wrapper.py`

```python
def create_kt_config_from_server_args(
    server_args: "ServerArgs", layer_idx: int
) -> Optional[KTConfig]:
    """Create KTConfig from ServerArgs if KT is configured."""
    if server_args.kt_weight_path is None:
        return None

    # ...

    return KTConfig(
        layer_idx=layer_idx,
        # ... 现有字段 ...
        # MoE LoRA configuration
        moe_lora_enabled=moe_lora_enabled,
        moe_lora_path=moe_lora_path,
        lora_rank=lora_rank,
        lora_alpha=lora_alpha,
        sft_method=sft_method,
        # 新增：传递 model_path
        model_path=server_args.model_path,
    )
```

## 7. 权重加载流程图

```
服务启动
    │
    ▼
┌─────────────────────────────────┐
│ ServerArgs 解析                 │
│ - model_path: /path/to/hf_model │
│ - kt_weight_path: /path/to/kt   │
│ - kt_moe_lora_path: /path/to/pt │
│ - kt_moe_sft_method: AMXBF16_SFT│
└─────────────┬───────────────────┘
              │
              ▼
┌─────────────────────────────────┐
│ create_kt_config_from_server_args│
│ - model_path 传入 KTConfig      │
└─────────────┬───────────────────┘
              │
              ▼
┌─────────────────────────────────┐
│ KTEPWrapperMethod.create_weights│
│ if sft_method == "AMXBF16_SFT": │
│     weight_path = model_path    │
│ else:                           │
│     weight_path = kt_weight_path│
└─────────────┬───────────────────┘
              │
              ▼
┌─────────────────────────────────┐
│ KTMoEWrapper(                   │
│     mode="sft",                 │
│     weight_path=sft_weight_path │
│ )                               │
└─────────────┬───────────────────┘
              │
              ▼
┌─────────────────────────────────┐
│ process_weights_after_loading() │
│ wrapper.load_weights(map)       │
└─────────────┬───────────────────┘
              │
              ▼
┌─────────────────────────────────┐
│ AMXSFTMoEWrapper.load_weights() │
│ if gate_proj is None:           │
│     _load_base_weights_from_file│
└─────────────┬───────────────────┘
              │
    ┌─────────┴─────────┐
    │                   │
AMXBF16_SFT      AMXINT8/INT4_SFT
    │                   │
    ▼                   ▼
┌────────────┐   ┌────────────┐
│BF16Safe    │   │SafeTensor  │
│TensorLoader│   │Loader      │
│base_key:   │   │base_key:   │
│model.layers│   │blk.{L}     │
└─────┬──────┘   └─────┬──────┘
      │                │
      └────────┬───────┘
               │
               ▼
┌─────────────────────────────────┐
│ gate_proj, up_proj, down_proj   │
│ (+ scales for INT methods)      │
└─────────────────────────────────┘
```
