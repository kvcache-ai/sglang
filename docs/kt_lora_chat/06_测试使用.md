# MoE Expert LoRA 测试与使用指南

## 1. 快速开始

### 1.1 前置条件

- sglang 已安装并配置
- kt-kernel 已编译并支持 SFT MoE
- PEFT 格式的 LoRA adapter（Kllama2 格式）

### 1.2 三步使用流程

```bash
# Step 1: 转换 adapter
python scripts/convert_moe_lora.py \
    --input /path/to/adapter_model.safetensors \
    --config /path/to/adapter_config.json \
    --output /path/to/moe_lora.pt

# Step 2: 验证转换结果
python -c "
import torch
d = torch.load('/path/to/moe_lora.pt')
print('Metadata:', d['metadata'])
print('Layers:', [k for k in d.keys() if k.startswith('layer_')])
"

# Step 3: 启动 sglang 服务
python -m sglang.launch_server \
    --model-path /path/to/deepseek-v2 \
    --kt-weight-path /path/to/kt-weights \
    --kt-moe-lora-path /path/to/moe_lora.pt \
    --lora-paths /path/to/adapter  # 原有 attention/shared LoRA
```

## 2. 转换工具使用

### 2.1 基本用法

```bash
python scripts/convert_moe_lora.py \
    --input /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/adapter_model.safetensors \
    --config /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/adapter_config.json \
    --output /tmp/moe_lora.pt \
    --verbose
```

### 2.2 参数说明

| 参数 | 必需 | 说明 |
|------|------|------|
| --input | 是 | PEFT adapter_model.safetensors 路径 |
| --config | 否 | adapter_config.json 路径（用于读取 lora_alpha） |
| --output | 是 | 输出 .pt 文件路径 |
| --lora-alpha | 否 | 手动指定 lora_alpha（默认 32.0） |
| --verbose | 否 | 显示详细输出 |

### 2.3 预期输出

```
INFO:__main__:Loading adapter from: /path/to/adapter_model.safetensors
INFO:__main__:Found 9984 MoE expert LoRA keys
INFO:__main__:Saving to: /tmp/moe_lora.pt
INFO:__main__:Conversion complete: {
    'status': 'success',
    'num_layers': 27,
    'num_experts': 64,
    'lora_rank': 16,
    'lora_alpha': 32.0,
    'total_keys': 9984
}
```

## 3. sglang 服务启动

### 3.1 完整启动命令

```bash
python -m sglang.launch_server \
    --model-path /path/to/deepseek-v2 \
    --trust-remote-code \
    --tp 4 \
    --port 30000 \
    \
    # KT 基础配置
    --kt-weight-path /path/to/kt-weights \
    --kt-cpuinfer 60 \
    --kt-threadpool-count 4 \
    --kt-num-gpu-experts 0 \
    \
    # MoE LoRA 配置
    --kt-moe-lora-path /path/to/moe_lora.pt \
    --kt-moe-lora-rank 16 \
    --kt-moe-lora-alpha 32.0 \
    --kt-moe-sft-method AMXBF16_SFT \
    \
    # 原有 LoRA 配置（attention/shared）
    --lora-paths /path/to/adapter
```

### 3.2 参数说明

| 参数 | 默认值 | 说明 |
|------|--------|------|
| --kt-moe-lora-path | None | 转换后的 MoE LoRA 文件 |
| --kt-moe-lora-rank | 16 | LoRA 秩 |
| --kt-moe-lora-alpha | 32.0 | LoRA alpha |
| --kt-moe-sft-method | AMXBF16_SFT | 量化方法 |

### 3.3 支持的 SFT 方法

| 方法 | 说明 | 推荐场景 |
|------|------|----------|
| AMXBF16_SFT | BF16 全精度 | 精度优先 |
| AMXINT8_SFT | INT8 量化 | 平衡 |
| AMXINT4_SFT | INT4 量化 | 性能优先 |

## 4. 测试方案

### 4.1 单元测试：转换脚本

```python
# test_convert_moe_lora.py
import pytest
import torch
from scripts.convert_moe_lora import convert_peft_to_kt_format, parse_moe_key


def test_parse_moe_key():
    """测试 key 解析"""
    key = "base_model.model.model.layers.1.mlp.original_moe.experts.5.gate_proj.lora_A.weight"
    result = parse_moe_key(key)
    assert result == (1, 5, "gate", "a")


def test_parse_moe_key_alternative():
    """测试替代命名模式"""
    key = "base_model.model.model.layers.1.mlp.experts.5.gate_proj.lora_B.weight"
    result = parse_moe_key(key)
    assert result == (1, 5, "gate", "b")


def test_convert_kllama2():
    """测试转换 Kllama2 adapter"""
    result = convert_peft_to_kt_format(
        input_path="/mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/adapter_model.safetensors",
        output_path="/tmp/test_moe_lora.pt",
        lora_alpha=32.0,
    )
    assert result["status"] == "success"
    assert result["num_experts"] == 64
    assert result["lora_rank"] == 16


def test_convert_kllama_skip():
    """测试 Kllama adapter（无 MoE）应跳过"""
    result = convert_peft_to_kt_format(
        input_path="/mnt/data/lpl/kernel_new_test_adapter/Kllama_deepseekV2_WEST_ALL/checkpoint-133/adapter_model.safetensors",
        output_path="/tmp/test_moe_lora.pt",
        lora_alpha=32.0,
    )
    assert result["status"] == "skipped"


def test_output_shapes():
    """测试输出权重 shape"""
    convert_peft_to_kt_format(
        input_path="...",
        output_path="/tmp/test_moe_lora.pt",
    )
    d = torch.load("/tmp/test_moe_lora.pt")

    layer = d["layer_1"]
    assert layer["gate_lora_a"].shape == (64, 16, 7168)
    assert layer["gate_lora_b"].shape == (64, 2048, 16)
    assert layer["down_lora_a"].shape == (64, 16, 2048)
    assert layer["down_lora_b"].shape == (64, 7168, 16)
```

### 4.2 集成测试：kt-kernel SFT

```python
# test_kt_sft_integration.py
import torch
from kt_kernel import KTMoEWrapper


def test_sft_wrapper_creation():
    """测试 SFT wrapper 创建"""
    wrapper = KTMoEWrapper(
        layer_idx=0,
        num_experts=64,
        num_experts_per_tok=8,
        hidden_size=7168,
        moe_intermediate_size=2048,
        num_gpu_experts=0,
        cpuinfer_threads=60,
        threadpool_count=4,
        weight_path="/path/to/weights",
        chunked_prefill_size=25600,
        mode="sft",
        method="AMXBF16_SFT",
        lora_rank=16,
        lora_alpha=32.0,
    )
    assert wrapper is not None


def test_lora_init_shapes():
    """测试 LoRA 权重初始化"""
    wrapper = KTMoEWrapper(...)  # 同上

    # 创建测试权重
    gate_lora_a = torch.randn(64, 16, 7168, dtype=torch.bfloat16)
    gate_lora_b = torch.randn(64, 2048, 16, dtype=torch.bfloat16)
    # ... 其他权重

    wrapper.init_lora_weights(
        gate_lora_a, gate_lora_b,
        up_lora_a, up_lora_b,
        down_lora_a, down_lora_b,
    )


def test_forward_sft():
    """测试 forward_sft 输出 shape"""
    wrapper = KTMoEWrapper(...)
    wrapper.load_weights(...)
    wrapper.init_lora_weights(...)

    hidden_states = torch.randn(128, 7168, dtype=torch.bfloat16)
    expert_ids = torch.randint(0, 64, (128, 8), dtype=torch.int64)
    weights = torch.rand(128, 8, dtype=torch.float32)

    output = wrapper.forward_sft(
        hidden_states, expert_ids, weights,
        save_for_backward=False,
    )

    assert output.shape == (128, 7168)
```

### 4.3 端到端测试

```python
# test_e2e_moe_lora.py
import requests


def test_inference_with_moe_lora():
    """测试带 MoE LoRA 的推理"""
    # 前提：sglang 服务已启动

    response = requests.post(
        "http://localhost:30000/generate",
        json={
            "text": "Hello, how are you?",
            "max_new_tokens": 50,
        }
    )

    assert response.status_code == 200
    result = response.json()
    assert "text" in result
    print(f"Generated: {result['text']}")


def test_inference_latency():
    """测试推理延迟"""
    import time

    # Warm up
    for _ in range(3):
        requests.post("http://localhost:30000/generate", json={...})

    # Measure
    latencies = []
    for _ in range(10):
        start = time.time()
        requests.post("http://localhost:30000/generate", json={...})
        latencies.append(time.time() - start)

    avg_latency = sum(latencies) / len(latencies)
    print(f"Average latency: {avg_latency * 1000:.2f} ms")
```

## 5. 故障排除

### 5.1 常见错误

#### 转换时：No MoE expert LoRA keys found

**原因**: 输入 adapter 不包含 MoE expert LoRA

**解决**: 确认使用的是 Kllama2 格式（而非 Kllama）

#### 启动时：MoE LoRA file not found

**原因**: 指定的 moe_lora.pt 路径不存在

**解决**: 检查路径是否正确，确认转换脚本已执行

#### 启动时：Layer X not found in MoE LoRA file

**原因**: 模型层数与 adapter 不匹配

**解决**: 检查 adapter 是否针对目标模型训练

#### 运行时：Shape mismatch

**原因**: LoRA 权重 shape 不匹配

**解决**: 检查 lora_rank、hidden_size、intermediate_size 是否一致

### 5.2 调试命令

```bash
# 检查转换后的文件
python -c "
import torch
d = torch.load('/path/to/moe_lora.pt')
print('Metadata:', d['metadata'])
for k, v in d['layer_0'].items():
    print(f'  {k}: {v.shape}')
"

# 检查 adapter 原始 keys
python -c "
from safetensors import safe_open
with safe_open('/path/to/adapter_model.safetensors', framework='pt') as f:
    keys = [k for k in f.keys() if 'experts' in k]
    print(f'Found {len(keys)} expert keys')
    for k in keys[:10]:
        print(f'  {k}')
"
```

## 6. 性能优化建议

### 6.1 SFT 方法选择

| 场景 | 推荐方法 | 原因 |
|------|----------|------|
| 精度敏感 | AMXBF16_SFT | 无量化损失 |
| 内存受限 | AMXINT4_SFT | 最小内存占用 |
| 平衡 | AMXINT8_SFT | 较好的精度/性能平衡 |

### 6.2 线程配置

```bash
# 推荐配置
--kt-cpuinfer 60 \        # 总 CPU 推理线程
--kt-threadpool-count 4   # NUMA 子池数
```

### 6.3 注意事项

1. **同步执行**: SFT 模式使用同步执行，CPU-GPU 并行度降低
2. **内存占用**: LoRA 权重额外占用约 1.35GB（DeepSeek-V2, rank=16）
3. **预热**: 首次推理可能较慢，建议预热

## 7. 参考资料

- kt-kernel 官方文档: `/home/lpl/ktransformers-llama/kt-kernel/docs/SFT+KTWrapper/`
- kt-kernel 测试用例: `/home/lpl/ktransformers-llama/kt-kernel/examples/test_moe_sft_wrapper.py`
- sglang LoRA 文档: `/home/lpl/sglang-debug/sglang/docs/`
