# MoE Expert LoRA 测试与使用指南

## 1. 快速开始

### 1.1 前置条件

- sglang 已安装并配置
- kt-kernel 已编译并支持 SFT MoE
- PEFT 格式的 LoRA adapter（Kllama2 格式）

### 1.2 三步使用流程

```bash
# Step 1: 转换 adapter
python scripts/convert_moe_lora.py \
    --input /path/to/adapter_model.safetensors \
    --config /path/to/adapter_config.json \
    --output /path/to/moe_lora.pt

# Step 2: 验证转换结果
python -c "
import torch
d = torch.load('/path/to/moe_lora.pt')
print('Metadata:', d['metadata'])
print('Layers:', [k for k in d.keys() if k.startswith('layer_')])
"

# Step 3: 启动 sglang 服务
python -m sglang.launch_server \
    --model-path /path/to/deepseek-v2 \
    --kt-weight-path /path/to/kt-weights \
    --kt-moe-lora-path /path/to/moe_lora.pt \
    --lora-paths /path/to/adapter  # 原有 attention/shared LoRA
```

## 2. 转换工具使用

### 2.1 基本用法

```bash
python scripts/convert_moe_lora.py \
    --input /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/adapter_model.safetensors \
    --config /mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/adapter_config.json \
    --output /tmp/moe_lora.pt \
    --verbose
```

### 2.2 参数说明

| 参数 | 必需 | 说明 |
|------|------|------|
| --input | 是 | PEFT adapter_model.safetensors 路径 |
| --config | 否 | adapter_config.json 路径（用于读取 lora_alpha） |
| --output | 是 | 输出 .pt 文件路径 |
| --lora-alpha | 否 | 手动指定 lora_alpha（默认 32.0） |
| --verbose | 否 | 显示详细输出 |

### 2.3 预期输出

```
INFO:__main__:Loading adapter from: /path/to/adapter_model.safetensors
INFO:__main__:Found 9984 MoE expert LoRA keys
INFO:__main__:Saving to: /tmp/moe_lora.pt
INFO:__main__:Conversion complete: {
    'status': 'success',
    'num_layers': 27,
    'num_experts': 64,
    'lora_rank': 16,
    'lora_alpha': 32.0,
    'total_keys': 9984
}
```

## 3. sglang 服务启动

### 3.1 完整启动命令

```bash
python -m sglang.launch_server \
    --model-path /path/to/deepseek-v2 \
    --trust-remote-code \
    --tp 4 \
    --port 30000 \
    \
    # KT 基础配置
    --kt-weight-path /path/to/kt-weights \
    --kt-cpuinfer 60 \
    --kt-threadpool-count 4 \
    --kt-num-gpu-experts 0 \
    \
    # MoE LoRA 配置
    --kt-moe-lora-path /path/to/moe_lora.pt \
    --kt-moe-lora-rank 16 \
    --kt-moe-lora-alpha 32.0 \
    --kt-moe-sft-method AMXBF16_SFT \
    \
    # 原有 LoRA 配置（attention/shared）
    --lora-paths /path/to/adapter
```

### 3.2 参数说明

| 参数 | 默认值 | 说明 |
|------|--------|------|
| --kt-moe-lora-path | None | 转换后的 MoE LoRA 文件 |
| --kt-moe-lora-rank | 16 | LoRA 秩 |
| --kt-moe-lora-alpha | 32.0 | LoRA alpha |
| --kt-moe-sft-method | AMXBF16_SFT | 量化方法 |

### 3.3 支持的 SFT 方法

| 方法 | 说明 | 推荐场景 |
|------|------|----------|
| AMXBF16_SFT | BF16 全精度 | 精度优先 |
| AMXINT8_SFT | INT8 量化 | 平衡 |
| AMXINT4_SFT | INT4 量化 | 性能优先 |

## 4. 测试方案

### 4.1 单元测试：转换脚本

```python
# test_convert_moe_lora.py
import pytest
import torch
from scripts.convert_moe_lora import convert_peft_to_kt_format, parse_moe_key


def test_parse_moe_key():
    """测试 key 解析"""
    key = "base_model.model.model.layers.1.mlp.original_moe.experts.5.gate_proj.lora_A.weight"
    result = parse_moe_key(key)
    assert result == (1, 5, "gate", "a")


def test_parse_moe_key_alternative():
    """测试替代命名模式"""
    key = "base_model.model.model.layers.1.mlp.experts.5.gate_proj.lora_B.weight"
    result = parse_moe_key(key)
    assert result == (1, 5, "gate", "b")


def test_convert_kllama2():
    """测试转换 Kllama2 adapter"""
    result = convert_peft_to_kt_format(
        input_path="/mnt/data/lpl/kernel_new_test_adapter/Kllama2_deepseekV2_WEST_ALL/checkpoint-133/adapter_model.safetensors",
        output_path="/tmp/test_moe_lora.pt",
        lora_alpha=32.0,
    )
    assert result["status"] == "success"
    assert result["num_experts"] == 64
    assert result["lora_rank"] == 16


def test_convert_kllama_skip():
    """测试 Kllama adapter（无 MoE）应跳过"""
    result = convert_peft_to_kt_format(
        input_path="/mnt/data/lpl/kernel_new_test_adapter/Kllama_deepseekV2_WEST_ALL/checkpoint-133/adapter_model.safetensors",
        output_path="/tmp/test_moe_lora.pt",
        lora_alpha=32.0,
    )
    assert result["status"] == "skipped"


def test_output_shapes():
    """测试输出权重 shape"""
    convert_peft_to_kt_format(
        input_path="...",
        output_path="/tmp/test_moe_lora.pt",
    )
    d = torch.load("/tmp/test_moe_lora.pt")

    layer = d["layer_1"]
    assert layer["gate_lora_a"].shape == (64, 16, 7168)
    assert layer["gate_lora_b"].shape == (64, 2048, 16)
    assert layer["down_lora_a"].shape == (64, 16, 2048)
    assert layer["down_lora_b"].shape == (64, 7168, 16)
```

### 4.2 集成测试：kt-kernel SFT

```python
# test_kt_sft_integration.py
import torch
from kt_kernel import KTMoEWrapper


def test_sft_wrapper_creation():
    """测试 SFT wrapper 创建"""
    wrapper = KTMoEWrapper(
        layer_idx=0,
        num_experts=64,
        num_experts_per_tok=8,
        hidden_size=7168,
        moe_intermediate_size=2048,
        num_gpu_experts=0,
        cpuinfer_threads=60,
        threadpool_count=4,
        weight_path="/path/to/weights",
        chunked_prefill_size=25600,
        mode="sft",
        method="AMXBF16_SFT",
        lora_rank=16,
        lora_alpha=32.0,
    )
    assert wrapper is not None


def test_lora_init_shapes():
    """测试 LoRA 权重初始化"""
    wrapper = KTMoEWrapper(...)  # 同上

    # 创建测试权重
    gate_lora_a = torch.randn(64, 16, 7168, dtype=torch.bfloat16)
    gate_lora_b = torch.randn(64, 2048, 16, dtype=torch.bfloat16)
    # ... 其他权重

    wrapper.init_lora_weights(
        gate_lora_a, gate_lora_b,
        up_lora_a, up_lora_b,
        down_lora_a, down_lora_b,
    )


def test_forward_sft():
    """测试 forward_sft 输出 shape"""
    wrapper = KTMoEWrapper(...)
    wrapper.load_weights(...)
    wrapper.init_lora_weights(...)

    hidden_states = torch.randn(128, 7168, dtype=torch.bfloat16)
    expert_ids = torch.randint(0, 64, (128, 8), dtype=torch.int64)
    weights = torch.rand(128, 8, dtype=torch.float32)

    output = wrapper.forward_sft(
        hidden_states, expert_ids, weights,
        save_for_backward=False,
    )

    assert output.shape == (128, 7168)
```

### 4.3 端到端测试

```python
# test_e2e_moe_lora.py
import requests


def test_inference_with_moe_lora():
    """测试带 MoE LoRA 的推理"""
    # 前提：sglang 服务已启动

    response = requests.post(
        "http://localhost:30000/generate",
        json={
            "text": "Hello, how are you?",
            "max_new_tokens": 50,
        }
    )

    assert response.status_code == 200
    result = response.json()
    assert "text" in result
    print(f"Generated: {result['text']}")


def test_inference_latency():
    """测试推理延迟"""
    import time

    # Warm up
    for _ in range(3):
        requests.post("http://localhost:30000/generate", json={...})

    # Measure
    latencies = []
    for _ in range(10):
        start = time.time()
        requests.post("http://localhost:30000/generate", json={...})
        latencies.append(time.time() - start)

    avg_latency = sum(latencies) / len(latencies)
    print(f"Average latency: {avg_latency * 1000:.2f} ms")
```

## 5. 故障排除

### 5.1 常见错误

#### 转换时：No MoE expert LoRA keys found

**原因**: 输入 adapter 不包含 MoE expert LoRA

**解决**: 确认使用的是 Kllama2 格式（而非 Kllama）

#### 启动时：MoE LoRA file not found

**原因**: 指定的 moe_lora.pt 路径不存在

**解决**: 检查路径是否正确，确认转换脚本已执行

#### 启动时：Layer X not found in MoE LoRA file

**原因**: 模型层数与 adapter 不匹配

**解决**: 检查 adapter 是否针对目标模型训练

#### 运行时：Shape mismatch

**原因**: LoRA 权重 shape 不匹配

**解决**: 检查 lora_rank、hidden_size、intermediate_size 是否一致

### 5.2 调试命令

```bash
# 检查转换后的文件
python -c "
import torch
d = torch.load('/path/to/moe_lora.pt')
print('Metadata:', d['metadata'])
for k, v in d['layer_0'].items():
    print(f'  {k}: {v.shape}')
"

# 检查 adapter 原始 keys
python -c "
from safetensors import safe_open
with safe_open('/path/to/adapter_model.safetensors', framework='pt') as f:
    keys = [k for k in f.keys() if 'experts' in k]
    print(f'Found {len(keys)} expert keys')
    for k in keys[:10]:
        print(f'  {k}')
"
```

## 6. 性能优化建议

### 6.1 SFT 方法选择

| 场景 | 推荐方法 | 原因 |
|------|----------|------|
| 精度敏感 | AMXBF16_SFT | 无量化损失 |
| 内存受限 | AMXINT4_SFT | 最小内存占用 |
| 平衡 | AMXINT8_SFT | 较好的精度/性能平衡 |

### 6.2 线程配置

```bash
# 推荐配置
--kt-cpuinfer 60 \        # 总 CPU 推理线程
--kt-threadpool-count 4   # NUMA 子池数
```

### 6.3 注意事项

1. **同步执行**: SFT 模式使用同步执行，CPU-GPU 并行度降低
2. **内存占用**: LoRA 权重额外占用约 1.35GB（DeepSeek-V2, rank=16）
3. **预热**: 首次推理可能较慢，建议预热

## 7. 参考资料

- kt-kernel 官方文档: `/home/lpl/ktransformers-llama/kt-kernel/docs/SFT+KTWrapper/`
- kt-kernel 测试用例: `/home/lpl/ktransformers-llama/kt-kernel/examples/test_moe_sft_wrapper.py`
- sglang LoRA 文档: `/home/lpl/sglang-debug/sglang/docs/`

## 8. SFT 权重加载测试

### 8.1 BF16SafeTensorLoader 单元测试

```python
# test_bf16_loader.py
import pytest
import torch
from kt_kernel.utils.loader import BF16SafeTensorLoader


def test_loader_init():
    """测试 BF16SafeTensorLoader 初始化"""
    loader = BF16SafeTensorLoader("/path/to/deepseek-v2")
    assert loader is not None
    assert loader._detected_format in ["deepseek", "mixtral"]


def test_load_experts_deepseek():
    """测试 DeepSeek 格式加载"""
    loader = BF16SafeTensorLoader("/path/to/deepseek-v2")

    experts_data = loader.load_experts("model.layers.0", device="cpu")

    # 验证返回格式
    assert "gate" in experts_data
    assert "up" in experts_data
    assert "down" in experts_data
    assert experts_data["gate_scale"] is None
    assert experts_data["up_scale"] is None
    assert experts_data["down_scale"] is None

    # 验证专家数量
    assert len(experts_data["gate"]) == 64  # DeepSeek-V2 有 64 专家

    # 验证 tensor 类型
    assert isinstance(experts_data["gate"][0], torch.Tensor)


def test_load_experts_mixtral():
    """测试 Mixtral 格式加载"""
    loader = BF16SafeTensorLoader("/path/to/mixtral")

    experts_data = loader.load_experts("model.layers.0", device="cpu")

    assert len(experts_data["gate"]) == 8  # Mixtral 有 8 专家


def test_invalid_base_key():
    """测试无效的 base_key"""
    loader = BF16SafeTensorLoader("/path/to/model")

    with pytest.raises(ValueError, match="No experts found"):
        loader.load_experts("invalid.key", device="cpu")
```

### 8.2 AMXSFTMoEWrapper 权重加载测试

```python
# test_amx_sft_weight_loading.py
import torch
from kt_kernel import KTMoEWrapper


def test_bf16_file_loading():
    """测试 AMXBF16_SFT 从文件加载"""
    wrapper = KTMoEWrapper(
        layer_idx=0,
        num_experts=64,
        num_experts_per_tok=8,
        hidden_size=7168,
        moe_intermediate_size=2048,
        num_gpu_experts=0,
        cpuinfer_threads=60,
        threadpool_count=4,
        weight_path="/path/to/deepseek-v2",  # HuggingFace 路径
        chunked_prefill_size=25600,
        mode="sft",
        method="AMXBF16_SFT",
        lora_rank=16,
        lora_alpha=32.0,
    )

    # 创建 dummy 映射
    physical_to_logical_map = torch.arange(64, dtype=torch.int64)

    # 加载权重（应自动从文件加载）
    wrapper.load_weights(physical_to_logical_map)

    # 验证权重已加载
    assert wrapper.gate_proj is not None
    assert wrapper.up_proj is not None
    assert wrapper.down_proj is not None

    # 验证 shape
    assert wrapper.gate_proj.shape == (64, 2048, 7168)
    assert wrapper.up_proj.shape == (64, 2048, 7168)
    assert wrapper.down_proj.shape == (64, 7168, 2048)


def test_int8_file_loading():
    """测试 AMXINT8_SFT 从文件加载"""
    wrapper = KTMoEWrapper(
        layer_idx=0,
        num_experts=64,
        num_experts_per_tok=8,
        hidden_size=7168,
        moe_intermediate_size=2048,
        num_gpu_experts=0,
        cpuinfer_threads=60,
        threadpool_count=4,
        weight_path="/path/to/kt-weights-int8",  # KT 预量化路径
        chunked_prefill_size=25600,
        mode="sft",
        method="AMXINT8_SFT",
        lora_rank=16,
        lora_alpha=32.0,
    )

    physical_to_logical_map = torch.arange(64, dtype=torch.int64)
    wrapper.load_weights(physical_to_logical_map)

    # 验证 scale 已加载
    assert wrapper.gate_scale is not None
    assert wrapper.up_scale is not None
    assert wrapper.down_scale is not None


def test_fallback_to_tensor_loading():
    """测试回退到张量加载模式"""
    wrapper = KTMoEWrapper(
        layer_idx=0,
        # ... 配置 ...
        weight_path=None,  # 未设置权重路径
        mode="sft",
        method="AMXBF16_SFT",
    )

    # 先加载张量
    gate_proj = torch.randn(64, 2048, 7168, dtype=torch.bfloat16)
    up_proj = torch.randn(64, 2048, 7168, dtype=torch.bfloat16)
    down_proj = torch.randn(64, 7168, 2048, dtype=torch.bfloat16)

    physical_to_logical_map = torch.arange(64, dtype=torch.int64)

    wrapper.load_weights_from_tensors(
        gate_proj, up_proj, down_proj,
        physical_to_logical_map
    )

    assert wrapper.gate_proj is not None
```

### 8.3 集成测试：完整 SFT 推理流程

```python
# test_sft_inference_flow.py
import torch
from kt_kernel import KTMoEWrapper


def test_full_sft_inference():
    """测试完整的 SFT 推理流程"""
    # 1. 创建 SFT wrapper
    wrapper = KTMoEWrapper(
        layer_idx=0,
        num_experts=64,
        num_experts_per_tok=8,
        hidden_size=7168,
        moe_intermediate_size=2048,
        num_gpu_experts=0,
        cpuinfer_threads=60,
        threadpool_count=4,
        weight_path="/path/to/deepseek-v2",
        chunked_prefill_size=25600,
        mode="sft",
        method="AMXBF16_SFT",
        lora_rank=16,
        lora_alpha=32.0,
    )

    # 2. 加载基础权重
    physical_to_logical_map = torch.arange(64, dtype=torch.int64)
    wrapper.load_weights(physical_to_logical_map)

    # 3. 加载 LoRA 权重
    gate_lora_a = torch.randn(64, 16, 7168, dtype=torch.bfloat16)
    gate_lora_b = torch.randn(64, 2048, 16, dtype=torch.bfloat16)
    up_lora_a = torch.randn(64, 16, 7168, dtype=torch.bfloat16)
    up_lora_b = torch.randn(64, 2048, 16, dtype=torch.bfloat16)
    down_lora_a = torch.randn(64, 16, 2048, dtype=torch.bfloat16)
    down_lora_b = torch.randn(64, 7168, 16, dtype=torch.bfloat16)

    wrapper.init_lora_weights(
        gate_lora_a, gate_lora_b,
        up_lora_a, up_lora_b,
        down_lora_a, down_lora_b,
    )

    # 4. 执行推理
    hidden_states = torch.randn(128, 7168, dtype=torch.bfloat16)
    expert_ids = torch.randint(0, 64, (128, 8), dtype=torch.int64)
    weights = torch.rand(128, 8, dtype=torch.float32)
    weights = weights / weights.sum(dim=1, keepdim=True)

    output = wrapper.forward_sft(
        hidden_states, expert_ids, weights,
        save_for_backward=False,
    )

    # 5. 验证输出
    assert output.shape == (128, 7168)
    assert output.dtype == torch.bfloat16
```

### 8.4 端到端测试：sglang 服务

```bash
# 测试 AMXBF16_SFT 模式
python -m sglang.launch_server \
    --model-path /mnt/data3/models/DeepSeek-V2-Lite-Chat \
    --kt-weight-path /mnt/data3/models/DeepSeek-V2-Lite-Chat-CPU-weight-INT8 \
    --kt-moe-lora-path /path/to/moe_lora.pt \
    --kt-moe-lora-rank 16 \
    --kt-moe-lora-alpha 32.0 \
    --kt-moe-sft-method AMXBF16_SFT \
    --kt-num-gpu-experts 0 \
    --kt-cpuinfer 60 \
    --port 30000

# 测试请求
curl -X POST http://localhost:30000/generate \
    -H "Content-Type: application/json" \
    -d '{"text": "Hello, how are you?", "max_new_tokens": 50}'
```

### 8.5 调试命令

```bash
# 检查 BF16SafeTensorLoader 格式检测
python -c "
from kt_kernel.utils.loader import BF16SafeTensorLoader

loader = BF16SafeTensorLoader('/path/to/model')
print(f'Detected format: {loader._detected_format}')
print(f'Available keys sample: {loader._get_all_keys()[:5]}')
"

# 检查权重加载
python -c "
from kt_kernel import KTMoEWrapper
import torch

wrapper = KTMoEWrapper(
    layer_idx=0,
    # ... 配置 ...
    weight_path='/path/to/model',
    mode='sft',
    method='AMXBF16_SFT',
)

map_cpu = torch.arange(64, dtype=torch.int64)
wrapper.load_weights(map_cpu)

print(f'gate_proj: {wrapper.gate_proj.shape}')
print(f'up_proj: {wrapper.up_proj.shape}')
print(f'down_proj: {wrapper.down_proj.shape}')
"
```

### 8.6 性能测试

```python
# test_sft_loading_performance.py
import time
import torch
from kt_kernel import KTMoEWrapper


def test_loading_time():
    """测试权重加载时间"""
    times = []

    for layer_idx in range(27):  # DeepSeek-V2 有 27 层
        wrapper = KTMoEWrapper(
            layer_idx=layer_idx,
            # ... 配置 ...
            weight_path="/path/to/model",
            mode="sft",
            method="AMXBF16_SFT",
        )

        map_cpu = torch.arange(64, dtype=torch.int64)

        start = time.perf_counter()
        wrapper.load_weights(map_cpu)
        elapsed = time.perf_counter() - start

        times.append(elapsed)
        print(f"Layer {layer_idx}: {elapsed:.2f}s")

    avg_time = sum(times) / len(times)
    print(f"Average loading time per layer: {avg_time:.2f}s")
    print(f"Total loading time: {sum(times):.2f}s")
```
