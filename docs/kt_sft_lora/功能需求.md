# SGLang MoE LoRA 功能需求

## 1. 背景

### 1.1 当前问题

SGLang 当前的 LoRA 实现仅支持标准 Linear 层，不支持 MoE (Mixture of Experts) 架构中的 routed experts。这导致：

- 使用包含 MoE experts LoRA 的 adapter 时，experts 部分的 LoRA 权重无法被应用
- 模型输出错误/乱码
- 用户无法在 SGLang 中使用针对 MoE 模型全量微调的 LoRA adapter

### 1.2 用户场景

用户使用 LLaMA-Factory 对 DeepSeek-V2-Lite-Chat 进行 LoRA 微调，target_modules 包括：
- Attention: `q_proj`, `o_proj`, `kv_a_proj_with_mqa`, `kv_b_proj`
- MLP: `mlp.gate_proj`, `mlp.up_proj`, `mlp.down_proj` (layer 0 dense MLP)
- Shared Experts: `shared_experts.gate_proj/up_proj/down_proj`
- **Routed Experts**: `mlp.experts.X.gate_proj/up_proj/down_proj` (layer 1-26)

当前 SGLang 支持状态：
| 模块类型 | 权重数量 | 支持状态 |
|---------|---------|---------|
| Attention | 216 | ✅ 支持 |
| Dense MLP (layer 0) | 6 | ✅ 支持 |
| Shared Experts | 156 | ✅ 支持 |
| **Routed Experts** | **9984** | ❌ **不支持** |

## 2. 功能需求

### 2.1 核心功能

**F1: MoE Routed Experts LoRA 支持**
- 支持加载包含 `mlp.experts.X.{gate_proj,up_proj,down_proj}` LoRA 权重的 adapter
- 在推理时正确应用 per-expert LoRA
- 支持动态 expert routing（不同 token 选择不同 experts）

**F2: 与现有 LoRA 系统兼容**
- 复用现有的 LoRAAdapter、LoRAManager、LoRAMemoryPool 架构
- 支持混合使用 attention LoRA + MoE LoRA
- 支持 multi-tenant LoRA（多个不同 adapter 同时服务）

**F3: 分布式支持**
- 支持 Tensor Parallel (TP)
- 支持 Expert Parallel (EP)（如果已启用）

### 2.2 性能需求

**P1: 内存效率**
- MoE LoRA 权重按需加载到 GPU
- 支持内存池管理和 eviction 策略
- 避免不必要的内存复制

**P2: 计算效率**
- 使用 fused Triton kernels
- 避免 Python 循环遍历 experts
- 支持批量处理多个 token 的 LoRA 计算

### 2.3 接口需求

**I1: 用户接口不变**
- 用户仍使用 `--lora-paths` 参数加载 adapter
- 用户仍使用 `model:adapter_name` 格式指定使用的 adapter
- 无需额外配置即可自动检测和使用 MoE LoRA

**I2: Adapter 格式兼容**
- 支持 PEFT 格式的 MoE LoRA adapter
- 支持通过 convert_lora.py 转换后的格式

## 3. 非功能需求

### 3.1 代码质量
- 遵循 SGLang 现有代码风格
- 添加必要的单元测试
- 添加集成测试验证端到端功能

### 3.2 文档
- 更新 LoRA 相关文档说明 MoE 支持
- 记录支持的模型列表

## 4. 约束条件

### 4.1 技术约束
- 必须与 FusedMoE 集成（不能使用未融合的实现）
- 必须支持量化（FP8 等）
- 不能破坏现有 LoRA 功能

### 4.2 范围约束
- 初期支持 DeepSeek-V2/V3 系列模型
- 初期只支持 `gate_proj`, `up_proj`, `down_proj` 三个 expert 子模块的 LoRA
- 不支持 router/gate 网络的 LoRA（属于另一个功能）

## 5. 验收标准

1. 使用用户提供的 DeepSeek-V2-Lite + MoE LoRA adapter 能够正确推理
2. 输出与 LLaMA-Factory 推理结果一致
3. 支持 TP=4 分布式部署
4. 内存占用合理（不超过 2x 相比仅 attention LoRA）
5. 延迟增加可接受（不超过 30% 相比无 LoRA）
