# SGLang MoE LoRA 算子接口设计

## 1. 概述

本文档定义 MoE LoRA 所需的 Triton kernel 接口设计。

### 1.1 设计目标

- **高效**: 单次 kernel 调用处理多 token、多 expert
- **灵活**: 支持不同 expert 有不同 rank 的 LoRA
- **兼容**: 与现有 FusedMoE 计算流程集成

### 1.2 计算模式

```
MoE LoRA 增量计算:
对于每个 token t, 对于每个选中的 expert e:
    delta[t] += routing_weight[t,e] * scaling * (
        down_B[e] @ down_A[e] @ silu(gate_up_B[e] @ gate_up_A[e] @ x[t])
    )
```

## 2. Kernel 1: MoE LoRA A (gate_up projection)

### 2.1 功能描述

计算 `intermediate = gate_up_A[expert_id] @ x` 对于每个 token 选中的 experts。

### 2.2 接口定义

```python
@triton.jit
def moe_lora_a_kernel(
    # 输入
    x_ptr,                    # 输入 hidden_states
    expert_ids_ptr,           # 每个 token 选择的 expert IDs
    routing_weights_ptr,      # 对应的 routing weights
    lora_a_ptr,               # LoRA A 权重缓冲区
    weight_indices_ptr,       # 每个 sequence 使用的 adapter index
    lora_ranks_ptr,           # 每个 adapter 的 rank

    # 输出
    output_ptr,               # 输出 intermediate

    # 维度参数
    num_tokens: tl.constexpr,
    hidden_size: tl.constexpr,
    max_rank: tl.constexpr,
    num_experts: tl.constexpr,
    top_k: tl.constexpr,
    max_loras: tl.constexpr,

    # Stride 参数
    stride_x_t, stride_x_h,
    stride_expert_t, stride_expert_k,
    stride_lora_adapter, stride_lora_expert, stride_lora_r, stride_lora_h,
    stride_out_t, stride_out_k, stride_out_r,

    # Block 参数
    BLOCK_T: tl.constexpr,
    BLOCK_R: tl.constexpr,
    BLOCK_H: tl.constexpr,
):
    """
    MoE LoRA A Kernel

    计算: output[t, k, :] = lora_a[adapter, expert_ids[t,k], :, :] @ x[t, :]

    Grid: (cdiv(num_tokens * top_k, BLOCK_T), cdiv(max_rank, BLOCK_R))
    """
    pass
```

### 2.3 内存布局

```
输入 x:
    Shape: (num_tokens, hidden_size)
    Layout: Row-major, contiguous

expert_ids:
    Shape: (num_tokens, top_k)
    Layout: Row-major
    Value range: [0, num_experts)

lora_a 缓冲区:
    Shape: (max_loras, num_experts, max_rank, hidden_size)
    Layout: Row-major
    Access pattern: lora_a[adapter_id, expert_id, :, :]

输出:
    Shape: (num_tokens, top_k, max_rank)
    Layout: Row-major
```

### 2.4 伪代码

```python
def moe_lora_a_reference(x, expert_ids, lora_a, weight_indices, lora_ranks):
    """
    Reference implementation for testing

    x: (num_tokens, hidden_size)
    expert_ids: (num_tokens, top_k)
    lora_a: (max_loras, num_experts, max_rank, hidden_size)
    weight_indices: (num_seqs,) -> adapter index for each sequence
    """
    num_tokens, top_k = expert_ids.shape
    max_rank = lora_a.shape[2]

    output = torch.zeros(num_tokens, top_k, max_rank, device=x.device)

    for t in range(num_tokens):
        adapter_id = weight_indices[t // seq_len]  # 简化: 假设固定 seq_len
        rank = lora_ranks[adapter_id]

        for k in range(top_k):
            expert_id = expert_ids[t, k]
            # output[t, k, :rank] = lora_a[adapter_id, expert_id, :rank, :] @ x[t, :]
            output[t, k, :rank] = torch.mv(
                lora_a[adapter_id, expert_id, :rank, :],
                x[t, :]
            )

    return output
```

## 3. Kernel 2: MoE LoRA B with Activation (gate_up_proj -> activation)

### 3.1 功能描述

计算 `activated = silu(gate) * up` 其中:
- `gate = gate_up_B[:intermediate] @ intermediate_a`
- `up = gate_up_B[intermediate:] @ intermediate_a`

### 3.2 接口定义

```python
@triton.jit
def moe_lora_gate_up_b_kernel(
    # 输入
    intermediate_a_ptr,       # LoRA A 输出
    expert_ids_ptr,
    lora_b_ptr,               # gate_up LoRA B 权重
    weight_indices_ptr,
    lora_ranks_ptr,

    # 输出
    activated_ptr,            # 激活后的中间结果

    # 维度
    num_tokens: tl.constexpr,
    intermediate_size: tl.constexpr,
    max_rank: tl.constexpr,
    num_experts: tl.constexpr,
    top_k: tl.constexpr,

    # Strides
    ...

    # Blocks
    BLOCK_T: tl.constexpr,
    BLOCK_I: tl.constexpr,
    BLOCK_R: tl.constexpr,
):
    """
    Fused gate_up LoRA B + SiLU activation

    计算:
    gate = lora_b[adapter, expert, :intermediate_size, :] @ intermediate_a
    up = lora_b[adapter, expert, intermediate_size:, :] @ intermediate_a
    activated = silu(gate) * up

    Grid: (cdiv(num_tokens * top_k, BLOCK_T), cdiv(intermediate_size, BLOCK_I))
    """
    pass
```

### 3.3 内存布局

```
intermediate_a (从 Kernel 1 输出):
    Shape: (num_tokens, top_k, max_rank)

gate_up_lora_b 缓冲区:
    Shape: (max_loras, num_experts, intermediate_size * 2, max_rank)
    Layout: 前半部分是 gate_proj, 后半部分是 up_proj

activated 输出:
    Shape: (num_tokens, top_k, intermediate_size)
```

## 4. Kernel 3: MoE LoRA Down + Combine

### 4.1 功能描述

计算 down projection 并与 routing weights 合并:
```
for each expert e selected by token t:
    down_out = down_B[e] @ down_A[e] @ activated[t, e]
    delta[t] += routing_weight[t, e] * scaling * down_out
```

### 4.2 接口定义

```python
@triton.jit
def moe_lora_down_combine_kernel(
    # 输入
    activated_ptr,            # 激活后的中间结果
    expert_ids_ptr,
    routing_weights_ptr,
    down_lora_a_ptr,
    down_lora_b_ptr,
    weight_indices_ptr,
    lora_ranks_ptr,
    scalings_ptr,             # LoRA scaling factors

    # 输出 (in-place add)
    output_ptr,               # 原始 MoE 输出，将累加 LoRA delta

    # 维度
    num_tokens: tl.constexpr,
    hidden_size: tl.constexpr,
    intermediate_size: tl.constexpr,
    max_rank: tl.constexpr,
    num_experts: tl.constexpr,
    top_k: tl.constexpr,

    # Blocks
    BLOCK_T: tl.constexpr,
    BLOCK_H: tl.constexpr,
    BLOCK_R: tl.constexpr,
):
    """
    Fused down LoRA A/B + routing weight combine

    计算:
    for k in range(top_k):
        intermediate = down_A[expert_ids[t,k]] @ activated[t,k]
        delta = down_B[expert_ids[t,k]] @ intermediate
        output[t] += routing_weights[t,k] * scaling * delta

    Grid: (cdiv(num_tokens, BLOCK_T), cdiv(hidden_size, BLOCK_H))
    """
    pass
```

### 4.3 优化考虑

1. **Fused Reduction**: 将 top_k 个 expert 的贡献在 kernel 内累加
2. **In-place Add**: 直接累加到原始 MoE 输出，避免额外内存分配
3. **Scaling Fusion**: 将 LoRA scaling 和 routing weight 的乘法融合

## 5. 简化版 Kernel: All-in-One MoE LoRA

### 5.1 设计考虑

上述三个 kernel 的分离设计允许最大灵活性，但可能增加 kernel launch 开销。
对于简单情况，可以提供 all-in-one kernel。

### 5.2 接口定义

```python
@triton.jit
def moe_lora_fused_kernel(
    # 输入
    x_ptr,                    # (num_tokens, hidden_size)
    expert_ids_ptr,           # (num_tokens, top_k)
    routing_weights_ptr,      # (num_tokens, top_k)

    # LoRA 权重
    gate_up_a_ptr,            # (max_loras, num_experts, rank, hidden_size)
    gate_up_b_ptr,            # (max_loras, num_experts, intermediate*2, rank)
    down_a_ptr,               # (max_loras, num_experts, rank, intermediate)
    down_b_ptr,               # (max_loras, num_experts, hidden_size, rank)

    # Batch info
    weight_indices_ptr,
    lora_ranks_ptr,
    scalings_ptr,

    # 输出
    output_ptr,               # (num_tokens, hidden_size) in-place add

    # 维度
    num_tokens: tl.constexpr,
    hidden_size: tl.constexpr,
    intermediate_size: tl.constexpr,
    max_rank: tl.constexpr,
    num_experts: tl.constexpr,
    top_k: tl.constexpr,

    # Blocks
    BLOCK_T: tl.constexpr,
    BLOCK_H: tl.constexpr,
):
    """
    All-in-one MoE LoRA kernel

    对于每个 token:
    1. 为每个选中的 expert 计算完整 LoRA 路径
    2. 按 routing weight 加权累加

    优点: 减少 kernel launch 开销
    缺点: 寄存器压力大，可能影响 occupancy

    Grid: (cdiv(num_tokens, BLOCK_T), cdiv(hidden_size, BLOCK_H))
    """
    pass
```

## 6. Python Wrapper 接口

### 6.1 高层接口

```python
def run_moe_lora(
    hidden_states: torch.Tensor,      # (num_tokens, hidden_size)
    topk_ids: torch.Tensor,           # (num_tokens, top_k)
    topk_weights: torch.Tensor,       # (num_tokens, top_k)
    moe_lora_info: MoELoRABatchInfo,
    gate_up_a_buffer: torch.Tensor,   # (max_loras, num_experts, rank, hidden)
    gate_up_b_buffer: torch.Tensor,   # (max_loras, num_experts, inter*2, rank)
    down_a_buffer: torch.Tensor,      # (max_loras, num_experts, rank, inter)
    down_b_buffer: torch.Tensor,      # (max_loras, num_experts, hidden, rank)
    output: torch.Tensor,             # (num_tokens, hidden_size) 原始 MoE 输出
) -> torch.Tensor:
    """
    运行 MoE LoRA 计算并将结果累加到 output

    参数:
        hidden_states: 输入 hidden states
        topk_ids: 每个 token 选择的 expert IDs
        topk_weights: 对应的 routing weights
        moe_lora_info: 批处理信息 (weight_indices, ranks, scalings)
        *_buffer: LoRA 权重缓冲区
        output: 原始 MoE 输出 (将被 in-place 修改)

    返回:
        output: 累加 LoRA delta 后的输出
    """
    # 选择 kernel 策略
    if should_use_fused_kernel(hidden_states.shape, topk_ids.shape):
        return _run_moe_lora_fused(...)
    else:
        return _run_moe_lora_staged(...)
```

### 6.2 分阶段实现

```python
def _run_moe_lora_staged(
    hidden_states, topk_ids, topk_weights, moe_lora_info,
    gate_up_a, gate_up_b, down_a, down_b, output
):
    """三阶段 MoE LoRA 实现"""
    num_tokens = hidden_states.shape[0]
    top_k = topk_ids.shape[1]
    max_rank = gate_up_a.shape[2]
    intermediate_size = down_a.shape[3]

    # Stage 1: gate_up LoRA A
    intermediate_a = torch.empty(
        (num_tokens, top_k, max_rank),
        dtype=hidden_states.dtype, device=hidden_states.device
    )
    moe_lora_a_kernel[grid1](
        hidden_states, topk_ids, gate_up_a,
        moe_lora_info.weight_indices, moe_lora_info.lora_ranks,
        intermediate_a,
        ...
    )

    # Stage 2: gate_up LoRA B + activation
    activated = torch.empty(
        (num_tokens, top_k, intermediate_size),
        dtype=hidden_states.dtype, device=hidden_states.device
    )
    moe_lora_gate_up_b_kernel[grid2](
        intermediate_a, topk_ids, gate_up_b,
        moe_lora_info.weight_indices, moe_lora_info.lora_ranks,
        activated,
        ...
    )

    # Stage 3: down LoRA + combine (in-place add to output)
    moe_lora_down_combine_kernel[grid3](
        activated, topk_ids, topk_weights,
        down_a, down_b,
        moe_lora_info.weight_indices, moe_lora_info.lora_ranks,
        moe_lora_info.scalings,
        output,
        ...
    )

    return output
```

## 7. Grid 和 Block 配置

### 7.1 Kernel 1 (LoRA A)

```python
# Grid 配置
BLOCK_T = 16   # Tokens per block
BLOCK_R = 32   # Rank elements per block
BLOCK_H = 128  # Hidden elements per block (reduction)

grid = (
    triton.cdiv(num_tokens * top_k, BLOCK_T),
    triton.cdiv(max_rank, BLOCK_R)
)

# Occupancy: 约 4 blocks per SM (受 shared memory 限制)
```

### 7.2 Kernel 2 (gate_up B + activation)

```python
BLOCK_T = 16
BLOCK_I = 64   # Intermediate elements per block
BLOCK_R = 16   # Rank reduction

grid = (
    triton.cdiv(num_tokens * top_k, BLOCK_T),
    triton.cdiv(intermediate_size, BLOCK_I)
)
```

### 7.3 Kernel 3 (down + combine)

```python
BLOCK_T = 8    # Fewer tokens due to top_k reduction loop
BLOCK_H = 128  # Hidden output elements

grid = (
    triton.cdiv(num_tokens, BLOCK_T),
    triton.cdiv(hidden_size, BLOCK_H)
)
```

## 8. 性能优化策略

### 8.1 内存访问优化

```python
# 1. Coalesced Memory Access
# 确保 hidden_size 维度是 innermost，便于合并访问
x_ptr + t * stride_x_t + h * stride_x_h  # h 应该是连续的

# 2. Shared Memory Tiling
# 对于大 hidden_size，使用 shared memory 做 tiling
shared_x = tl.zeros((BLOCK_T, BLOCK_H), dtype=tl.float32)

# 3. Vectorized Load
# 使用 tl.load 的 mask 和 vectorization
x_block = tl.load(x_ptr + offsets, mask=mask, other=0.0)
```

### 8.2 计算优化

```python
# 1. FMA (Fused Multiply-Add)
# Triton 自动生成 FMA 指令

# 2. Reduction Tree
# 对于 hidden_size reduction，使用 tree reduction
acc = tl.sum(x_block * w_block, axis=1)

# 3. Avoid Bank Conflicts
# Shared memory 布局避免 bank conflicts
# 使用 padding 或 swizzling
```

### 8.3 Expert 选择优化

```python
# 1. Expert Masking
# 只计算有效的 (token, expert) 对
expert_mask = expert_ids != INVALID_EXPERT_ID

# 2. Sparse Computation
# 如果某些 expert 在 batch 中未被选择，跳过其 LoRA 计算
active_experts = tl.unique(expert_ids)  # 需要额外 kernel 预处理

# 3. Expert Grouping
# 按 expert_id 重排序 tokens 以提高局部性
# (类似于现有 LoRA 的 chunked backend)
```

## 9. 测试接口

### 9.1 单元测试

```python
def test_moe_lora_a_kernel():
    """测试 MoE LoRA A kernel 正确性"""
    # 准备测试数据
    num_tokens, hidden_size, max_rank = 128, 2048, 8
    num_experts, top_k = 64, 6

    x = torch.randn(num_tokens, hidden_size, device="cuda")
    expert_ids = torch.randint(0, num_experts, (num_tokens, top_k), device="cuda")
    lora_a = torch.randn(1, num_experts, max_rank, hidden_size, device="cuda")
    weight_indices = torch.zeros(num_tokens, dtype=torch.int32, device="cuda")
    lora_ranks = torch.tensor([max_rank], device="cuda")

    # 运行 kernel
    output = torch.zeros(num_tokens, top_k, max_rank, device="cuda")
    moe_lora_a_kernel[grid](...)

    # 参考实现
    expected = moe_lora_a_reference(x, expert_ids, lora_a, weight_indices, lora_ranks)

    # 验证
    torch.testing.assert_close(output, expected, rtol=1e-3, atol=1e-3)
```

### 9.2 性能基准

```python
def benchmark_moe_lora():
    """性能基准测试"""
    configs = [
        (256, 2048, 8, 64, 6),   # DeepSeek-V2-Lite
        (256, 5120, 8, 256, 8),  # DeepSeek-V3
    ]

    for num_tokens, hidden, rank, num_experts, top_k in configs:
        # 准备数据
        ...

        # Warmup
        for _ in range(10):
            run_moe_lora(...)

        # Benchmark
        torch.cuda.synchronize()
        start = time.time()
        for _ in range(100):
            run_moe_lora(...)
        torch.cuda.synchronize()
        elapsed = time.time() - start

        print(f"Config: {configs}")
        print(f"Time per call: {elapsed / 100 * 1000:.3f} ms")
        print(f"Throughput: {num_tokens * 100 / elapsed:.0f} tokens/s")
```

## 10. 与现有系统集成

### 10.1 Backend 接口扩展

```python
# 在 LoRABackend 中添加
class LoRABackend:
    def run_moe_lora(
        self,
        hidden_states: torch.Tensor,
        topk_ids: torch.Tensor,
        topk_weights: torch.Tensor,
        moe_lora_info: MoELoRABatchInfo,
        output: torch.Tensor,
    ) -> torch.Tensor:
        """运行 MoE LoRA 计算"""
        return run_moe_lora(
            hidden_states, topk_ids, topk_weights, moe_lora_info,
            self.moe_gate_up_a_buffer,
            self.moe_gate_up_b_buffer,
            self.moe_down_a_buffer,
            self.moe_down_b_buffer,
            output,
        )
```

### 10.2 内存管理集成

```python
# 在 LoRAManager 中添加
class LoRAManager:
    def prepare_moe_lora_batch(
        self,
        forward_batch: ForwardBatch,
        layer_id: int,
    ) -> Optional[MoELoRABatchInfo]:
        """为指定层准备 MoE LoRA 批处理信息"""
        if not self.has_moe_lora:
            return None

        return self.moe_memory_pool.prepare_batch(
            forward_batch,
            layer_id,
            self.lora_adapters,
        )
```

## 11. 文件组织

```
sglang/srt/lora/
├── triton_ops/
│   ├── moe_lora_a.py          # Kernel 1
│   ├── moe_lora_gate_up_b.py  # Kernel 2
│   ├── moe_lora_down.py       # Kernel 3
│   └── moe_lora_fused.py      # All-in-one kernel (可选)
├── moe_mem_pool.py            # MoE LoRA 内存池
├── moe_layers.py              # FusedMoEWithLoRA
└── moe_lora.py                # MoELoRALayer, MoELoRABatchInfo
```
