# SGLang MoE LoRA 功能设计

## 1. 设计目标

### 1.1 核心目标

实现对 MoE 架构中 routed experts 的 LoRA 支持，使 SGLang 能够：
1. 加载包含 expert LoRA 权重的 adapter
2. 在推理时正确应用 per-expert LoRA
3. 保持与现有 LoRA 系统的兼容性

### 1.2 设计原则

- **最小侵入**: 尽量复用现有组件，减少对核心代码的修改
- **高性能**: 使用 fused Triton kernels，避免 Python 循环
- **可扩展**: 设计支持未来扩展（如 router LoRA）

## 2. 整体架构

### 2.1 组件关系图

```
┌─────────────────────────────────────────────────────────────────────────┐
│                            LoRA Manager                                  │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                      LoRAAdapter (扩展)                          │   │
│  │  - layers: Dict[layer_id, LoRALayer]                            │   │
│  │  - moe_layers: Dict[layer_id, MoELoRALayer] ★NEW★               │   │
│  └─────────────────────────────────────────────────────────────────┘   │
└────────────────────────────────┬────────────────────────────────────────┘
                                 │
         ┌───────────────────────┼───────────────────────┐
         ▼                       ▼                       ▼
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│LoRAMemoryPool   │    │MoELoRAMemPool   │    │  LoRABackend    │
│ (现有)           │    │     ★NEW★       │    │    (扩展)       │
│ - A_buffer      │    │ - expert_A_buf  │    │ - run_lora_a    │
│ - B_buffer      │    │ - expert_B_buf  │    │ - run_moe_lora  │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                       │                       │
         └───────────────────────┼───────────────────────┘
                                 ▼
                    ┌─────────────────────────┐
                    │   Model Forward Pass    │
                    │  ┌───────────────────┐  │
                    │  │ FusedMoEWithLoRA  │  │
                    │  │      ★NEW★        │  │
                    │  └───────────────────┘  │
                    └─────────────────────────┘
```

### 2.2 新增/修改组件

| 组件 | 类型 | 文件位置 | 功能 |
|-----|------|---------|------|
| `MoELoRALayer` | 新增 | `lora/lora.py` | 存储 per-expert LoRA 权重 |
| `MoELoRAMemoryPool` | 新增 | `lora/moe_mem_pool.py` | Expert LoRA GPU 缓冲区管理 |
| `FusedMoEWithLoRA` | 新增 | `lora/moe_layers.py` | FusedMoE 的 LoRA wrapper |
| `moe_lora_kernels` | 新增 | `lora/triton_ops/moe_lora.py` | Expert-aware LoRA 计算 |
| `LoRAAdapter` | 修改 | `lora/lora.py` | 扩展支持 MoE 权重加载 |
| `LoRAManager` | 修改 | `lora/lora_manager.py` | 集成 MoE LoRA 管理 |

## 3. 数据结构设计

### 3.1 MoELoRALayer

```python
@dataclass
class MoELoRALayer:
    """存储单个 MoE layer 的所有 expert LoRA 权重"""

    # Per-expert gate_up LoRA
    # Shape: (num_experts, rank, hidden_size)
    gate_up_lora_a: Dict[int, torch.Tensor]  # expert_id -> weight
    # Shape: (num_experts, intermediate_size*2, rank)
    gate_up_lora_b: Dict[int, torch.Tensor]

    # Per-expert down LoRA
    # Shape: (num_experts, rank, intermediate_size)
    down_lora_a: Dict[int, torch.Tensor]
    # Shape: (num_experts, hidden_size, rank)
    down_lora_b: Dict[int, torch.Tensor]

    # 元数据
    num_experts: int
    rank: int
    scaling: float  # alpha / rank
```

### 3.2 MoELoRABatchInfo

```python
@dataclass
class MoELoRABatchInfo:
    """MoE LoRA 批处理信息"""

    # 基础批处理信息
    bs: int                           # 批大小
    num_tokens: int                   # 总 token 数

    # Expert 路由信息 (从 TopK 输出获取)
    topk_ids: torch.Tensor            # (num_tokens, top_k) 每 token 选择的 experts
    topk_weights: torch.Tensor        # (num_tokens, top_k) 对应权重

    # LoRA adapter 信息
    weight_indices: torch.Tensor      # (bs,) 每个 sequence 使用的 adapter
    lora_ranks: torch.Tensor          # (max_loras,) 每个 adapter 的 rank
    scalings: torch.Tensor            # (max_loras,) 每个 adapter 的 scaling

    # Expert 到 adapter 的映射
    expert_lora_mask: torch.Tensor    # (num_experts,) 哪些 expert 有 LoRA
```

### 3.3 GPU 缓冲区布局

```python
# MoE LoRA 内存池缓冲区
class MoELoRAMemoryPool:
    # gate_up_proj LoRA A
    # Shape: (max_loras, num_experts, max_rank, hidden_size)
    gate_up_A_buffer: torch.Tensor

    # gate_up_proj LoRA B
    # Shape: (max_loras, num_experts, intermediate_size*2, max_rank)
    gate_up_B_buffer: torch.Tensor

    # down_proj LoRA A
    # Shape: (max_loras, num_experts, max_rank, intermediate_size)
    down_A_buffer: torch.Tensor

    # down_proj LoRA B
    # Shape: (max_loras, num_experts, hidden_size, max_rank)
    down_B_buffer: torch.Tensor
```

## 4. Forward 流程设计

### 4.1 FusedMoEWithLoRA Forward

```python
class FusedMoEWithLoRA(nn.Module):
    def __init__(self, fused_moe: FusedMoE, lora_backend: LoRABackend):
        self.fused_moe = fused_moe
        self.lora_backend = lora_backend
        self.set_lora = False

    def forward(
        self,
        hidden_states: torch.Tensor,
        moe_lora_info: Optional[MoELoRABatchInfo] = None,
    ) -> torch.Tensor:
        """
        Forward with optional MoE LoRA

        流程:
        1. 获取 router 输出 (topk_ids, topk_weights)
        2. 计算原始 MoE 输出
        3. 如果启用 LoRA:
           a. 计算 LoRA 增量 (per-expert)
           b. 按 routing 权重加权
           c. 累加到输出
        """
        # Step 1: Router
        topk_output = self.fused_moe.gate(hidden_states)
        topk_ids = topk_output.topk_ids      # (num_tokens, top_k)
        topk_weights = topk_output.topk_weights

        # Step 2: 原始 MoE 计算
        moe_output = self.fused_moe.forward_moe_core(hidden_states, topk_output)

        # Step 3: LoRA 增量
        if self.set_lora and moe_lora_info is not None:
            lora_delta = self.compute_moe_lora(
                hidden_states,
                topk_ids,
                topk_weights,
                moe_lora_info
            )
            moe_output = moe_output + lora_delta

        return moe_output

    def compute_moe_lora(
        self,
        hidden_states: torch.Tensor,
        topk_ids: torch.Tensor,
        topk_weights: torch.Tensor,
        moe_lora_info: MoELoRABatchInfo,
    ) -> torch.Tensor:
        """
        计算 MoE LoRA 增量

        对于每个 token, 每个选中的 expert:
        delta += topk_weight * (lora_B[expert] @ act(lora_A[expert] @ x))
        """
        return self.lora_backend.run_moe_lora(
            hidden_states,
            topk_ids,
            topk_weights,
            moe_lora_info
        )
```

### 4.2 计算流程图

```
Input: hidden_states (num_tokens, hidden_size)
       topk_ids (num_tokens, top_k)
       topk_weights (num_tokens, top_k)
       lora_buffers: gate_up_A, gate_up_B, down_A, down_B

┌─────────────────────────────────────────────────────────────┐
│  For each token t, for each selected expert e in topk[t]:   │
│                                                              │
│  1. gate_up_lora_out = gate_up_B[e] @ (gate_up_A[e] @ x[t]) │
│                        Shape: (intermediate_size * 2,)       │
│                                                              │
│  2. gate_out = gate_up_lora_out[:intermediate_size]         │
│     up_out = gate_up_lora_out[intermediate_size:]           │
│     intermediate = silu(gate_out) * up_out                  │
│                        Shape: (intermediate_size,)          │
│                                                              │
│  3. down_lora_out = down_B[e] @ (down_A[e] @ intermediate)  │
│                        Shape: (hidden_size,)                │
│                                                              │
│  4. delta[t] += topk_weight[t, e] * down_lora_out * scaling │
└─────────────────────────────────────────────────────────────┘

Output: delta (num_tokens, hidden_size)
```

## 5. 权重加载设计

### 5.1 权重识别

```python
# 在 LoRAAdapter 中扩展
MOE_EXPERT_PATTERN = re.compile(
    r"model\.layers\.(\d+)\.mlp\.experts\.(\d+)\.(gate_proj|up_proj|down_proj)\.lora_([AB])\.weight"
)

def _parse_moe_weight_name(name: str) -> Optional[Tuple[int, int, str, str]]:
    """
    解析 MoE LoRA 权重名称

    返回: (layer_id, expert_id, proj_type, lora_type) 或 None
    """
    match = MOE_EXPERT_PATTERN.match(name)
    if match:
        layer_id = int(match.group(1))
        expert_id = int(match.group(2))
        proj_type = match.group(3)  # gate_proj, up_proj, down_proj
        lora_type = match.group(4)  # A or B
        return (layer_id, expert_id, proj_type, lora_type)
    return None
```

### 5.2 权重组织

```python
def load_moe_lora_weights(self, weights: Dict[str, torch.Tensor]):
    """加载并组织 MoE LoRA 权重"""

    # 按 layer 分组
    moe_weights_by_layer: Dict[int, Dict] = defaultdict(
        lambda: {
            "gate_proj": {"A": {}, "B": {}},
            "up_proj": {"A": {}, "B": {}},
            "down_proj": {"A": {}, "B": {}},
        }
    )

    for name, weight in weights.items():
        parsed = self._parse_moe_weight_name(name)
        if parsed:
            layer_id, expert_id, proj_type, lora_type = parsed
            moe_weights_by_layer[layer_id][proj_type][lora_type][expert_id] = weight

    # 归一化为 gate_up_proj 格式
    for layer_id, layer_weights in moe_weights_by_layer.items():
        moe_layer = self._normalize_moe_layer(layer_weights)
        self.moe_layers[layer_id] = moe_layer
```

### 5.3 权重归一化

```python
def _normalize_moe_layer(self, layer_weights: Dict) -> MoELoRALayer:
    """
    将 gate_proj + up_proj 归一化为 gate_up_proj
    类似于标准 LoRA 的 qkv_proj 归一化
    """
    gate_up_lora_a = {}
    gate_up_lora_b = {}
    down_lora_a = {}
    down_lora_b = {}

    # 获取所有 expert IDs
    expert_ids = set(layer_weights["gate_proj"]["A"].keys())

    for expert_id in expert_ids:
        # Merge gate + up -> gate_up (along output dimension for B)
        gate_a = layer_weights["gate_proj"]["A"][expert_id]
        up_a = layer_weights["up_proj"]["A"][expert_id]
        # A 矩阵: 相同 (都是 x -> intermediate)
        # 需要检查是否相同，或者分开存储

        gate_b = layer_weights["gate_proj"]["B"][expert_id]
        up_b = layer_weights["up_proj"]["B"][expert_id]
        # B 矩阵: concat along output dimension
        gate_up_b = torch.cat([gate_b, up_b], dim=0)

        gate_up_lora_a[expert_id] = gate_a  # 假设 gate_a == up_a
        gate_up_lora_b[expert_id] = gate_up_b

        # down_proj 直接使用
        down_lora_a[expert_id] = layer_weights["down_proj"]["A"][expert_id]
        down_lora_b[expert_id] = layer_weights["down_proj"]["B"][expert_id]

    return MoELoRALayer(
        gate_up_lora_a=gate_up_lora_a,
        gate_up_lora_b=gate_up_lora_b,
        down_lora_a=down_lora_a,
        down_lora_b=down_lora_b,
        num_experts=len(expert_ids),
        rank=gate_a.shape[0],
        scaling=self.config.lora_alpha / self.config.r,
    )
```

## 6. 内存池设计

### 6.1 MoELoRAMemoryPool 初始化

```python
class MoELoRAMemoryPool:
    def __init__(
        self,
        base_hf_config: AutoConfig,
        max_loras_per_batch: int,
        dtype: torch.dtype,
        tp_size: int,
        tp_rank: int,
        max_lora_rank: int,
        num_experts: int,
        eviction_policy: str,
    ):
        self.num_experts = num_experts
        self.max_loras_per_batch = max_loras_per_batch

        # 计算维度
        hidden_size = base_hf_config.hidden_size
        intermediate_size = base_hf_config.moe_intermediate_size

        # TP 分片 (与 FusedMoE 一致)
        if tp_size > 1:
            intermediate_size = intermediate_size // tp_size

        # 分配缓冲区
        self.gate_up_A_buffer = torch.zeros(
            (max_loras_per_batch, num_experts, max_lora_rank, hidden_size),
            dtype=dtype, device="cuda"
        )
        self.gate_up_B_buffer = torch.zeros(
            (max_loras_per_batch, num_experts, intermediate_size * 2, max_lora_rank),
            dtype=dtype, device="cuda"
        )
        self.down_A_buffer = torch.zeros(
            (max_loras_per_batch, num_experts, max_lora_rank, intermediate_size),
            dtype=dtype, device="cuda"
        )
        self.down_B_buffer = torch.zeros(
            (max_loras_per_batch, num_experts, hidden_size, max_lora_rank),
            dtype=dtype, device="cuda"
        )
```

### 6.2 权重复制策略

```python
def prepare_moe_lora_batch(
    self,
    forward_batch: ForwardBatch,
    lora_adapters: Dict[str, LoRAAdapter],
) -> MoELoRABatchInfo:
    """
    准备批处理的 MoE LoRA 信息

    策略:
    1. 对于批次中的每个 adapter，检查是否已在 GPU 缓冲区
    2. 如果不在，从 CPU 复制到 GPU
    3. 使用 eviction policy 管理缓冲区空间
    """
    for lora_id in forward_batch.unique_lora_ids():
        if lora_id not in self.uid_to_buffer_id:
            # 分配缓冲区槽位
            buffer_id = self._allocate_slot(lora_id)

            # 复制权重
            adapter = lora_adapters[lora_id]
            self._copy_weights_to_gpu(adapter, buffer_id)

    # 构建 batch info
    return self._build_batch_info(forward_batch)
```

## 7. 模型集成设计

### 7.1 DeepSeek-V2 集成

```python
# 在 deepseek_v2.py 中修改

class DeepseekV2MoE(nn.Module):
    def __init__(self, config, ...):
        ...
        self.experts = FusedMoE(...)
        # LoRA wrapper 将在 LoRA 初始化时添加
        self._lora_wrapper = None

    def set_lora_wrapper(self, wrapper: FusedMoEWithLoRA):
        """在 LoRA 初始化时调用"""
        self._lora_wrapper = wrapper

    def forward(self, hidden_states, moe_lora_info=None):
        if self._lora_wrapper is not None:
            return self._lora_wrapper(hidden_states, moe_lora_info)
        else:
            return self.experts(hidden_states)
```

### 7.2 LoRA 模式检测

```python
# 在模型中添加
def should_apply_moe_lora(module_name: str) -> bool:
    """检查是否应该对该模块应用 MoE LoRA"""
    moe_lora_pattern = re.compile(
        r"^model\.layers\.(\d+)\.mlp\.experts\.(\d+)\.(gate_up_proj|down_proj)$"
    )
    return moe_lora_pattern.match(module_name) is not None

# 添加到 DeepseekV2ForCausalLM
_lora_pattern_moe = re.compile(
    r"^model\.layers\.(\d+)\.mlp\.experts\.(\d+)\.(gate_up_proj|down_proj)$"
)
```

## 8. 错误处理设计

### 8.1 权重验证

```python
def validate_moe_lora_weights(adapter: LoRAAdapter, model_config: AutoConfig):
    """验证 MoE LoRA 权重与模型兼容"""

    errors = []

    for layer_id, moe_layer in adapter.moe_layers.items():
        # 检查 expert 数量
        if moe_layer.num_experts > model_config.n_routed_experts:
            errors.append(
                f"Layer {layer_id}: adapter has {moe_layer.num_experts} experts, "
                f"model has {model_config.n_routed_experts}"
            )

        # 检查维度
        expected_hidden = model_config.hidden_size
        for expert_id, weight in moe_layer.gate_up_lora_a.items():
            if weight.shape[1] != expected_hidden:
                errors.append(
                    f"Layer {layer_id}, Expert {expert_id}: "
                    f"gate_up_lora_a hidden_size mismatch"
                )

    if errors:
        raise ValueError("MoE LoRA validation failed:\n" + "\n".join(errors))
```

### 8.2 Fallback 机制

```python
def forward_with_fallback(self, hidden_states, moe_lora_info):
    """带 fallback 的 forward"""
    try:
        return self.forward(hidden_states, moe_lora_info)
    except Exception as e:
        logger.warning(f"MoE LoRA forward failed: {e}, falling back to base MoE")
        return self.fused_moe(hidden_states)
```

## 9. 测试设计

### 9.1 单元测试

```python
# test_moe_lora.py

def test_moe_lora_weight_loading():
    """测试 MoE LoRA 权重加载"""
    adapter = LoRAAdapter.from_pretrained(moe_lora_path)
    assert len(adapter.moe_layers) > 0
    assert adapter.moe_layers[1].num_experts == 64

def test_moe_lora_memory_pool():
    """测试 MoE LoRA 内存池"""
    pool = MoELoRAMemoryPool(...)
    pool.prepare_moe_lora_batch(batch, adapters)
    assert pool.gate_up_A_buffer is not None

def test_moe_lora_kernel():
    """测试 MoE LoRA Triton kernel"""
    output = moe_lora_forward(hidden_states, expert_ids, lora_weights)
    expected = reference_moe_lora(hidden_states, expert_ids, lora_weights)
    torch.testing.assert_close(output, expected)
```

### 9.2 集成测试

```python
def test_deepseek_v2_moe_lora_inference():
    """测试 DeepSeek-V2 + MoE LoRA 推理"""
    server = launch_server(
        model_path="DeepSeek-V2-Lite-Chat",
        lora_paths={"test": moe_lora_path},
    )

    response = client.chat.completions.create(
        model="DeepSeek:test",
        messages=[{"role": "user", "content": "你好"}],
    )

    # 验证输出不是乱码
    assert len(response.choices[0].message.content) > 0
    assert is_valid_chinese(response.choices[0].message.content)
```

## 10. 实现优先级

### Phase 1: 基础功能 (Week 1)
1. 权重识别和加载
2. MoELoRALayer 数据结构
3. 基础内存池

### Phase 2: 核心计算 (Week 2)
4. MoE LoRA Triton kernels
5. FusedMoEWithLoRA wrapper
6. 模型集成

### Phase 3: 完善和测试 (Week 3)
7. TP/EP 支持
8. 错误处理
9. 单元测试和集成测试
10. 文档更新
